{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRFX='b0327_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-03-17 02:32:24', '2020-03-24 17:09:45']\n",
      "['2020-02-06 00:00:00', '2020-02-13 00:00:00']\n"
     ]
    }
   ],
   "source": [
    "trntmstmp=1584412344\n",
    "valtmstmp=1585069785\n",
    "\n",
    "import datetime\n",
    "print([datetime.datetime.fromtimestamp(o).strftime('%Y-%m-%d %H:%M:%S') for o in (trntmstmp, valtmstmp)])\n",
    "\n",
    "grand_total=1.5e8\n",
    "MIN_TM_TRN=1580947200\n",
    "MIN_TM_TST=1581552000\n",
    "print([datetime.datetime.fromtimestamp(o).strftime('%Y-%m-%d %H:%M:%S') for o in (MIN_TM_TRN, MIN_TM_TST)])\n",
    "\n",
    "\n",
    "SEED=101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 27 14:28:58 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   45C    P0    39W / 300W |  15171MiB / 16130MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      7287      C   ...ubuntu/anaconda3/envs/rcss20/bin/python 15161MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import datetime\n",
    "def dtnow(): return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "SEED=101\n",
    "HOME='/data/git/recsys20'\n",
    "p_in=f'{HOME}/input'\n",
    "p_out = f'{HOME}/output/{PRFX}'\n",
    "from pathlib import Path\n",
    "Path(p_out).mkdir(exist_ok=True)\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import torch\n",
    "device=torch.device('cuda')\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "pretrained_weights='bert-base-multilingual-cased'\n",
    "bertmodel = BertModel.from_pretrained(pretrained_weights, output_hidden_states=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights, do_lower_case=False)\n",
    "\n",
    "import random\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Retweet': 'retwt',\n",
       " 'Reply': 'reply',\n",
       " 'Like': 'like',\n",
       " 'RTwCmnt': 'retwt_cmmnt'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=[\n",
    "'toks','hshtgs','twtid','media','links','domns','twttyp','lang','tm',\n",
    "'u1id','u1_fllwer_cnt','u1_fllwng_cnt','u1_vrfed','u1_create_tm',\n",
    "'u2id','u2_fllwer_cnt','u2_fllwng_cnt','u2_vrfed','u2_create_tm',\n",
    "'u1_fllw_u2','reply_tm','retwt_tm','retwt_cmmnt_tm','like_tm',]\n",
    "cols_cat = ['twttyp','lang']\n",
    "cols_val = cols[:-4]\n",
    "cols_tgt_tmstmp=[\n",
    "    'retwt_tm',\n",
    "    'reply_tm',\n",
    "    'like_tm',\n",
    "    'retwt_cmmnt_tm',\n",
    "]\n",
    "cols_tgt=[o.split('_tm')[0] for o in cols_tgt_tmstmp]\n",
    "tgts             = ['Retweet','Reply','Like','RTwCmnt',]\n",
    "assert cols_tgt == ['retwt',  'reply','like','retwt_cmmnt',]\n",
    "ntgts=len(tgts)\n",
    "\n",
    "\n",
    "tgt2col=dict(zip(tgts,cols_tgt))\n",
    "tgt2col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.57 s, sys: 742 ms, total: 8.31 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dftrn=pd.read_csv(f'{p_in}/trn_{trntmstmp}.tsv',sep='\\x01',header=None,encoding='utf-8',names=cols, \n",
    "    nrows=1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lens = dftrn.toks.apply(lambda x: len(x.split('\\t')))\n",
    "# lens.mean(), np.percentile(lens, 50), np.percentile(lens, 95), np.percentile(lens, 99)\n",
    "# (46.754583, 41.0, 106.0, 135.0)\n",
    "\n",
    "\n",
    "# tokenizer.pad_token, tokenizer.pad_token_id\n",
    "# ('[PAD]', 0)\n",
    "\n",
    "# tokenizer.sep_token, tokenizer.sep_token_id\n",
    "# ('[SEP]', 102)\n",
    "maxlen=128\n",
    "def mkids(x):\n",
    "    tokids=list(map(int, x.split('\\t')))\n",
    "    l=len(tokids) \n",
    "    if l<=maxlen: \n",
    "        return tokids + [0]*(maxlen-len(tokids))\n",
    "    else: \n",
    "        return tokids[:maxlen-1]+[102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_tensors(df, istrn=True):\n",
    "    tokids=dftrn.toks.apply(lambda x: mkids(x))\n",
    "    Xarr=np.array(list(tokids))\n",
    "    X=torch.tensor(Xarr,dtype=torch.long)\n",
    "    if not istrn: return X\n",
    "    ys=dftrn[cols_tgt_tmstmp].notna().values\n",
    "    ys=torch.tensor(ys,dtype=torch.float)\n",
    "    return X,ys\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X,ys = mk_tensors(dftrn)\n",
    "\n",
    "# BS=2\n",
    "# ds = TensorDataset(X,ys)\n",
    "# dl = DataLoader(ds, batch_size=BS, shuffle=True)\n",
    "# for step, batch in enumerate(dl):\n",
    "#     X_b,ys_b = (o.to(device) for o in batch)\n",
    "#     print(X_b.shape,ys_b.shape)\n",
    "#     break\n",
    "# torch.Size([2, 128]) torch.Size([2, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = TensorDataset(X,ys)\n",
    "# dl = DataLoader(ds, batch_size=BS, shuffle=True)\n",
    "# for step, batch in enumerate(dl):\n",
    "#     X_b,ys_b = (o.to(device) for o in batch)\n",
    "#     print(X_b.shape,ys_b.shape)\n",
    "#     break\n",
    "\n",
    "# # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "# bertmodel=bertmodel.eval()\n",
    "# bertmodel.to(device)\n",
    "# with torch.no_grad():\n",
    "#     last_hidden_state, pooler_output, hidden_states = bertmodel(X_b,)\n",
    "#     avg_pool = torch.mean(last_hidden_state,1)\n",
    "#     max_pool,_ = torch.max(last_hidden_state,1)\n",
    "# last_hidden_state.shape,pooler_output.shape, len(hidden_states), avg_pool.shape, max_pool.shape\n",
    "# (torch.Size([2, 128, 768]),\n",
    "#  torch.Size([2, 768]),\n",
    "#  13,\n",
    "#  torch.Size([2, 768]),\n",
    "#  torch.Size([2, 768]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see RobertaClassificationHead in transformers/modeling_roberta.py\n",
    "N_HIDDEN = 768 \n",
    "class TwtModel(nn.Module):\n",
    "    def __init__(self, bertmodel, num_labels=4):\n",
    "        super(TwtModel, self).__init__()\n",
    "        self.bertmodel = bertmodel        \n",
    "        dense_size = 64\n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "        nx = N_HIDDEN*2\n",
    "        self.dense = nn.Linear(nx, nx)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out_proj = nn.Linear(nx, num_labels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_msk = (x!=tokenizer.pad_token_id).float().to(device)\n",
    "        segments = torch.zeros(x.shape, dtype=torch.long).to(device)\n",
    "        bert_output=self.bertmodel(x,attention_mask=attn_msk,token_type_ids=segments)\n",
    "        last_hidden_state,pooler_output,hidden_states = bert_output\n",
    "        avg_pool = torch.mean(last_hidden_state,1)\n",
    "        max_pool,_ = torch.max(last_hidden_state,1)\n",
    "        x = torch.cat([avg_pool,max_pool],1) \n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS=104\n",
    "EPOCHS=4\n",
    "WD=0.01\n",
    "LR=1e-5\n",
    "SCHDLR_FUNC = get_cosine_schedule_with_warmup \n",
    "WARMUP_RATE = 0.05\n",
    "N_CYCLS = .5\n",
    "EPS = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X,ys = mk_tensors(dftrn)\n",
    "\n",
    "msk_vl = np.random.rand(len(X))<0.15\n",
    "tr = np.where(~msk_vl)[0]\n",
    "vl = np.where( msk_vl)[0]\n",
    "Xtr,Xvl = X[tr],X[vl]\n",
    "ys_tr, ys_vl = ys[tr], ys[vl]\n",
    "ds_tr = TensorDataset(Xtr,ys_tr)\n",
    "ds_vl = TensorDataset(Xvl,ys_vl)\n",
    "dl_tr = DataLoader(ds_tr, batch_size=BS,   pin_memory=True, shuffle=True)\n",
    "dl_vl = DataLoader(ds_vl, batch_size=BS, pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwtModel(bertmodel)\n",
    "model = model.to(device)\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': WD},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LR, eps=EPS)\n",
    "t_total = int(EPOCHS*len(dl_tr))\n",
    "scheduler = SCHDLR_FUNC(\n",
    "    optimizer, \n",
    "    num_warmup_steps=int(WARMUP_RATE*t_total), \n",
    "    num_training_steps=t_total,\n",
    "    num_cycles=N_CYCLS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dl):\n",
    "    prd = []\n",
    "    ys = []\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    for step, dat_b in enumerate(dl):\n",
    "        model.eval()\n",
    "        x_b,ys_b = (o.to(device) for o in dat_b)\n",
    "        with torch.no_grad(): \n",
    "            prd_b = model(x_b)\n",
    "            loss_ =  F.binary_cross_entropy_with_logits(prd_b,ys_b)\n",
    "\n",
    "        ys.append(ys_b.cpu().detach().numpy())\n",
    "        prd.append(prd_b.cpu().detach().numpy())\n",
    "        losses.append(loss_.item())\n",
    "\n",
    "    prd = np.concatenate(prd)\n",
    "    ys = np.concatenate(ys)\n",
    "    scrs = [roc_auc_score(ys[:,i], prd[:,i]) for i in range(4)]\n",
    "    scr  = roc_auc_score(ys, prd)\n",
    "    loss = np.mean(losses)\n",
    "    return prd, ys, scrs, scr, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-27 14:30:08 epoch 0 starts\n",
      "2020-03-27 14:30:09 step 0/8173 loss 0.6984\n",
      "2020-03-27 14:30:22 step 20/8173 loss 0.6925\n",
      "2020-03-27 14:30:35 step 40/8173 loss 0.6599\n",
      "2020-03-27 14:30:49 step 60/8173 loss 0.6356\n",
      "2020-03-27 14:31:02 step 80/8173 loss 0.5854\n",
      "2020-03-27 14:31:16 step 100/8173 loss 0.5214\n",
      "2020-03-27 14:31:29 step 120/8173 loss 0.4917\n",
      "2020-03-27 14:31:42 step 140/8173 loss 0.3996\n",
      "2020-03-27 14:31:56 step 160/8173 loss 0.3546\n",
      "2020-03-27 14:32:09 step 180/8173 loss 0.3138\n",
      "2020-03-27 14:32:23 step 200/8173 loss 0.3001\n",
      "2020-03-27 14:32:36 step 220/8173 loss 0.3027\n",
      "2020-03-27 14:32:50 step 240/8173 loss 0.3046\n",
      "2020-03-27 14:33:04 step 260/8173 loss 0.3181\n",
      "2020-03-27 14:33:17 step 280/8173 loss 0.2889\n",
      "2020-03-27 14:33:31 step 300/8173 loss 0.3422\n",
      "2020-03-27 14:33:44 step 320/8173 loss 0.3399\n",
      "2020-03-27 14:33:58 step 340/8173 loss 0.2907\n",
      "2020-03-27 14:34:12 step 360/8173 loss 0.2944\n",
      "2020-03-27 14:34:25 step 380/8173 loss 0.2950\n",
      "2020-03-27 14:34:39 step 400/8173 loss 0.3093\n",
      "2020-03-27 14:34:52 step 420/8173 loss 0.2782\n",
      "2020-03-27 14:35:06 step 440/8173 loss 0.3031\n",
      "2020-03-27 14:35:19 step 460/8173 loss 0.3177\n",
      "2020-03-27 14:35:33 step 480/8173 loss 0.3132\n",
      "2020-03-27 14:35:47 step 500/8173 loss 0.2760\n",
      "2020-03-27 14:36:00 step 520/8173 loss 0.2768\n",
      "2020-03-27 14:36:14 step 540/8173 loss 0.2705\n",
      "2020-03-27 14:36:27 step 560/8173 loss 0.3166\n",
      "2020-03-27 14:36:41 step 580/8173 loss 0.2860\n",
      "2020-03-27 14:36:54 step 600/8173 loss 0.2714\n",
      "2020-03-27 14:37:08 step 620/8173 loss 0.3172\n",
      "2020-03-27 14:37:22 step 640/8173 loss 0.2609\n",
      "2020-03-27 14:37:35 step 660/8173 loss 0.2650\n",
      "2020-03-27 14:37:49 step 680/8173 loss 0.2972\n",
      "2020-03-27 14:38:02 step 700/8173 loss 0.2642\n",
      "2020-03-27 14:38:16 step 720/8173 loss 0.3181\n",
      "2020-03-27 14:38:29 step 740/8173 loss 0.2902\n",
      "2020-03-27 14:38:43 step 760/8173 loss 0.2840\n",
      "2020-03-27 14:38:56 step 780/8173 loss 0.2771\n",
      "2020-03-27 14:39:10 step 800/8173 loss 0.2607\n",
      "2020-03-27 14:39:23 step 820/8173 loss 0.2980\n",
      "2020-03-27 14:39:37 step 840/8173 loss 0.2706\n",
      "2020-03-27 14:39:50 step 860/8173 loss 0.2809\n",
      "2020-03-27 14:40:04 step 880/8173 loss 0.2511\n",
      "2020-03-27 14:40:17 step 900/8173 loss 0.2598\n",
      "2020-03-27 14:40:31 step 920/8173 loss 0.3003\n",
      "2020-03-27 14:40:44 step 940/8173 loss 0.2722\n",
      "2020-03-27 14:40:57 step 960/8173 loss 0.2881\n",
      "2020-03-27 14:41:11 step 980/8173 loss 0.2631\n",
      "2020-03-27 14:41:24 step 1000/8173 loss 0.3061\n",
      "2020-03-27 14:41:38 step 1020/8173 loss 0.2485\n",
      "2020-03-27 14:41:51 step 1040/8173 loss 0.2402\n",
      "2020-03-27 14:42:05 step 1060/8173 loss 0.2344\n",
      "2020-03-27 14:42:18 step 1080/8173 loss 0.2892\n",
      "2020-03-27 14:42:31 step 1100/8173 loss 0.3102\n",
      "2020-03-27 14:42:45 step 1120/8173 loss 0.3493\n",
      "2020-03-27 14:42:58 step 1140/8173 loss 0.2698\n",
      "2020-03-27 14:43:12 step 1160/8173 loss 0.2656\n",
      "2020-03-27 14:43:25 step 1180/8173 loss 0.2981\n",
      "2020-03-27 14:43:39 step 1200/8173 loss 0.3041\n",
      "2020-03-27 14:43:52 step 1220/8173 loss 0.2330\n",
      "2020-03-27 14:44:06 step 1240/8173 loss 0.2615\n",
      "2020-03-27 14:44:19 step 1260/8173 loss 0.2866\n",
      "2020-03-27 14:44:32 step 1280/8173 loss 0.3445\n",
      "2020-03-27 14:44:46 step 1300/8173 loss 0.2784\n",
      "2020-03-27 14:44:59 step 1320/8173 loss 0.3103\n",
      "2020-03-27 14:45:13 step 1340/8173 loss 0.2860\n",
      "2020-03-27 14:45:26 step 1360/8173 loss 0.2708\n",
      "2020-03-27 14:45:40 step 1380/8173 loss 0.2878\n",
      "2020-03-27 14:45:53 step 1400/8173 loss 0.2847\n",
      "2020-03-27 14:46:06 step 1420/8173 loss 0.3719\n",
      "2020-03-27 14:46:20 step 1440/8173 loss 0.2692\n",
      "2020-03-27 14:46:33 step 1460/8173 loss 0.3064\n",
      "2020-03-27 14:46:47 step 1480/8173 loss 0.2884\n",
      "2020-03-27 14:47:00 step 1500/8173 loss 0.2774\n",
      "2020-03-27 14:47:14 step 1520/8173 loss 0.2603\n",
      "2020-03-27 14:47:27 step 1540/8173 loss 0.2452\n",
      "2020-03-27 14:47:40 step 1560/8173 loss 0.2775\n",
      "2020-03-27 14:47:54 step 1580/8173 loss 0.2648\n",
      "2020-03-27 14:48:07 step 1600/8173 loss 0.2918\n",
      "2020-03-27 14:48:21 step 1620/8173 loss 0.2919\n",
      "2020-03-27 14:48:34 step 1640/8173 loss 0.2913\n",
      "2020-03-27 14:48:48 step 1660/8173 loss 0.3035\n",
      "2020-03-27 14:49:01 step 1680/8173 loss 0.2294\n",
      "2020-03-27 14:49:15 step 1700/8173 loss 0.2992\n",
      "2020-03-27 14:49:28 step 1720/8173 loss 0.3062\n",
      "2020-03-27 14:49:42 step 1740/8173 loss 0.2965\n",
      "2020-03-27 14:49:55 step 1760/8173 loss 0.3190\n",
      "2020-03-27 14:50:09 step 1780/8173 loss 0.2296\n",
      "2020-03-27 14:50:22 step 1800/8173 loss 0.2434\n",
      "2020-03-27 14:50:36 step 1820/8173 loss 0.2386\n",
      "2020-03-27 14:50:49 step 1840/8173 loss 0.2990\n",
      "2020-03-27 14:51:03 step 1860/8173 loss 0.2749\n",
      "2020-03-27 14:51:16 step 1880/8173 loss 0.2881\n",
      "2020-03-27 14:51:29 step 1900/8173 loss 0.2702\n",
      "2020-03-27 14:51:43 step 1920/8173 loss 0.2818\n",
      "2020-03-27 14:51:56 step 1940/8173 loss 0.2852\n",
      "2020-03-27 14:52:10 step 1960/8173 loss 0.2976\n",
      "2020-03-27 14:52:23 step 1980/8173 loss 0.2834\n",
      "2020-03-27 14:52:37 step 2000/8173 loss 0.3479\n",
      "2020-03-27 14:52:50 step 2020/8173 loss 0.2596\n",
      "2020-03-27 14:53:04 step 2040/8173 loss 0.2360\n",
      "2020-03-27 14:53:17 step 2060/8173 loss 0.2434\n",
      "2020-03-27 14:53:31 step 2080/8173 loss 0.3017\n",
      "2020-03-27 14:53:44 step 2100/8173 loss 0.2586\n",
      "2020-03-27 14:53:58 step 2120/8173 loss 0.2734\n",
      "2020-03-27 14:54:11 step 2140/8173 loss 0.2710\n",
      "2020-03-27 14:54:25 step 2160/8173 loss 0.2985\n",
      "2020-03-27 14:54:38 step 2180/8173 loss 0.2753\n",
      "2020-03-27 14:54:52 step 2200/8173 loss 0.3032\n",
      "2020-03-27 14:55:05 step 2220/8173 loss 0.3109\n",
      "2020-03-27 14:55:19 step 2240/8173 loss 0.2401\n",
      "2020-03-27 14:55:32 step 2260/8173 loss 0.2787\n",
      "2020-03-27 14:55:46 step 2280/8173 loss 0.2745\n",
      "2020-03-27 14:55:59 step 2300/8173 loss 0.3160\n",
      "2020-03-27 14:56:13 step 2320/8173 loss 0.2444\n",
      "2020-03-27 14:56:26 step 2340/8173 loss 0.3385\n",
      "2020-03-27 14:56:39 step 2360/8173 loss 0.3665\n",
      "2020-03-27 14:56:53 step 2380/8173 loss 0.2590\n",
      "2020-03-27 14:57:06 step 2400/8173 loss 0.2511\n",
      "2020-03-27 14:57:20 step 2420/8173 loss 0.2909\n",
      "2020-03-27 14:57:33 step 2440/8173 loss 0.2786\n",
      "2020-03-27 14:57:47 step 2460/8173 loss 0.2434\n",
      "2020-03-27 14:58:00 step 2480/8173 loss 0.3207\n",
      "2020-03-27 14:58:14 step 2500/8173 loss 0.2628\n",
      "2020-03-27 14:58:27 step 2520/8173 loss 0.2933\n",
      "2020-03-27 14:58:40 step 2540/8173 loss 0.2817\n",
      "2020-03-27 14:58:54 step 2560/8173 loss 0.2970\n",
      "2020-03-27 14:59:07 step 2580/8173 loss 0.2973\n",
      "2020-03-27 14:59:21 step 2600/8173 loss 0.2885\n",
      "2020-03-27 14:59:34 step 2620/8173 loss 0.2623\n",
      "2020-03-27 14:59:48 step 2640/8173 loss 0.2331\n",
      "2020-03-27 15:00:01 step 2660/8173 loss 0.2778\n",
      "2020-03-27 15:00:15 step 2680/8173 loss 0.2443\n",
      "2020-03-27 15:00:28 step 2700/8173 loss 0.2858\n",
      "2020-03-27 15:00:42 step 2720/8173 loss 0.2710\n",
      "2020-03-27 15:00:55 step 2740/8173 loss 0.2988\n",
      "2020-03-27 15:01:09 step 2760/8173 loss 0.2386\n",
      "2020-03-27 15:01:22 step 2780/8173 loss 0.2308\n",
      "2020-03-27 15:01:36 step 2800/8173 loss 0.2918\n",
      "2020-03-27 15:01:49 step 2820/8173 loss 0.3148\n",
      "2020-03-27 15:02:03 step 2840/8173 loss 0.2643\n",
      "2020-03-27 15:02:16 step 2860/8173 loss 0.2860\n",
      "2020-03-27 15:02:30 step 2880/8173 loss 0.3392\n",
      "2020-03-27 15:02:43 step 2900/8173 loss 0.3071\n",
      "2020-03-27 15:02:56 step 2920/8173 loss 0.2242\n",
      "2020-03-27 15:03:10 step 2940/8173 loss 0.2850\n",
      "2020-03-27 15:03:23 step 2960/8173 loss 0.2362\n",
      "2020-03-27 15:03:37 step 2980/8173 loss 0.2953\n",
      "2020-03-27 15:03:50 step 3000/8173 loss 0.2425\n",
      "2020-03-27 15:04:04 step 3020/8173 loss 0.3048\n",
      "2020-03-27 15:04:17 step 3040/8173 loss 0.2683\n",
      "2020-03-27 15:04:31 step 3060/8173 loss 0.3258\n",
      "2020-03-27 15:04:44 step 3080/8173 loss 0.2791\n",
      "2020-03-27 15:04:58 step 3100/8173 loss 0.2913\n",
      "2020-03-27 15:05:11 step 3120/8173 loss 0.3067\n",
      "2020-03-27 15:05:25 step 3140/8173 loss 0.2713\n",
      "2020-03-27 15:05:38 step 3160/8173 loss 0.2698\n",
      "2020-03-27 15:05:52 step 3180/8173 loss 0.2582\n",
      "2020-03-27 15:06:05 step 3200/8173 loss 0.2845\n",
      "2020-03-27 15:06:19 step 3220/8173 loss 0.2869\n",
      "2020-03-27 15:06:32 step 3240/8173 loss 0.2850\n",
      "2020-03-27 15:06:45 step 3260/8173 loss 0.2556\n",
      "2020-03-27 15:06:59 step 3280/8173 loss 0.2673\n",
      "2020-03-27 15:07:12 step 3300/8173 loss 0.2608\n",
      "2020-03-27 15:07:26 step 3320/8173 loss 0.3002\n",
      "2020-03-27 15:07:39 step 3340/8173 loss 0.2885\n",
      "2020-03-27 15:07:53 step 3360/8173 loss 0.2901\n",
      "2020-03-27 15:08:06 step 3380/8173 loss 0.2930\n",
      "2020-03-27 15:08:20 step 3400/8173 loss 0.2509\n",
      "2020-03-27 15:08:33 step 3420/8173 loss 0.2998\n",
      "2020-03-27 15:08:47 step 3440/8173 loss 0.2635\n",
      "2020-03-27 15:09:01 step 3460/8173 loss 0.2942\n",
      "2020-03-27 15:09:14 step 3480/8173 loss 0.2564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-27 15:09:28 step 3500/8173 loss 0.2695\n",
      "2020-03-27 15:09:41 step 3520/8173 loss 0.3216\n",
      "2020-03-27 15:09:55 step 3540/8173 loss 0.3016\n",
      "2020-03-27 15:10:09 step 3560/8173 loss 0.2834\n",
      "2020-03-27 15:10:22 step 3580/8173 loss 0.2753\n",
      "2020-03-27 15:10:36 step 3600/8173 loss 0.3055\n",
      "2020-03-27 15:10:49 step 3620/8173 loss 0.2811\n",
      "2020-03-27 15:11:03 step 3640/8173 loss 0.2556\n",
      "2020-03-27 15:11:16 step 3660/8173 loss 0.2986\n",
      "2020-03-27 15:11:30 step 3680/8173 loss 0.2250\n",
      "2020-03-27 15:11:44 step 3700/8173 loss 0.2581\n",
      "2020-03-27 15:11:57 step 3720/8173 loss 0.2814\n",
      "2020-03-27 15:12:11 step 3740/8173 loss 0.2530\n",
      "2020-03-27 15:12:24 step 3760/8173 loss 0.3088\n",
      "2020-03-27 15:12:38 step 3780/8173 loss 0.2505\n",
      "2020-03-27 15:12:52 step 3800/8173 loss 0.2738\n",
      "2020-03-27 15:13:05 step 3820/8173 loss 0.2799\n",
      "2020-03-27 15:13:19 step 3840/8173 loss 0.2843\n",
      "2020-03-27 15:13:32 step 3860/8173 loss 0.3378\n",
      "2020-03-27 15:13:46 step 3880/8173 loss 0.2737\n",
      "2020-03-27 15:13:59 step 3900/8173 loss 0.2383\n",
      "2020-03-27 15:14:13 step 3920/8173 loss 0.2577\n",
      "2020-03-27 15:14:27 step 3940/8173 loss 0.2648\n",
      "2020-03-27 15:14:40 step 3960/8173 loss 0.2797\n",
      "2020-03-27 15:14:54 step 3980/8173 loss 0.2581\n",
      "2020-03-27 15:15:07 step 4000/8173 loss 0.3036\n",
      "2020-03-27 15:15:21 step 4020/8173 loss 0.2995\n",
      "2020-03-27 15:15:34 step 4040/8173 loss 0.2956\n",
      "2020-03-27 15:15:48 step 4060/8173 loss 0.2483\n",
      "2020-03-27 15:16:02 step 4080/8173 loss 0.2977\n",
      "2020-03-27 15:16:15 step 4100/8173 loss 0.2200\n",
      "2020-03-27 15:16:29 step 4120/8173 loss 0.2810\n",
      "2020-03-27 15:16:42 step 4140/8173 loss 0.3142\n",
      "2020-03-27 15:16:56 step 4160/8173 loss 0.2716\n",
      "2020-03-27 15:17:10 step 4180/8173 loss 0.2946\n",
      "2020-03-27 15:17:23 step 4200/8173 loss 0.2482\n",
      "2020-03-27 15:17:37 step 4220/8173 loss 0.2262\n",
      "2020-03-27 15:17:50 step 4240/8173 loss 0.2777\n",
      "2020-03-27 15:18:04 step 4260/8173 loss 0.2743\n",
      "2020-03-27 15:18:17 step 4280/8173 loss 0.2597\n",
      "2020-03-27 15:18:31 step 4300/8173 loss 0.2841\n",
      "2020-03-27 15:18:44 step 4320/8173 loss 0.2733\n",
      "2020-03-27 15:18:58 step 4340/8173 loss 0.2658\n",
      "2020-03-27 15:19:11 step 4360/8173 loss 0.2865\n",
      "2020-03-27 15:19:25 step 4380/8173 loss 0.2766\n",
      "2020-03-27 15:19:38 step 4400/8173 loss 0.2654\n",
      "2020-03-27 15:19:52 step 4420/8173 loss 0.2930\n",
      "2020-03-27 15:20:05 step 4440/8173 loss 0.2834\n",
      "2020-03-27 15:20:19 step 4460/8173 loss 0.2298\n",
      "2020-03-27 15:20:32 step 4480/8173 loss 0.2726\n",
      "2020-03-27 15:20:45 step 4500/8173 loss 0.2673\n",
      "2020-03-27 15:20:59 step 4520/8173 loss 0.2478\n",
      "2020-03-27 15:21:12 step 4540/8173 loss 0.2649\n",
      "2020-03-27 15:21:26 step 4560/8173 loss 0.3038\n",
      "2020-03-27 15:21:39 step 4580/8173 loss 0.2672\n",
      "2020-03-27 15:21:53 step 4600/8173 loss 0.2971\n",
      "2020-03-27 15:22:06 step 4620/8173 loss 0.2602\n",
      "2020-03-27 15:22:20 step 4640/8173 loss 0.2616\n",
      "2020-03-27 15:22:33 step 4660/8173 loss 0.2290\n",
      "2020-03-27 15:22:47 step 4680/8173 loss 0.2698\n",
      "2020-03-27 15:23:00 step 4700/8173 loss 0.2669\n",
      "2020-03-27 15:23:14 step 4720/8173 loss 0.2856\n",
      "2020-03-27 15:23:27 step 4740/8173 loss 0.2599\n",
      "2020-03-27 15:23:41 step 4760/8173 loss 0.2474\n",
      "2020-03-27 15:23:54 step 4780/8173 loss 0.3217\n",
      "2020-03-27 15:24:08 step 4800/8173 loss 0.2413\n",
      "2020-03-27 15:24:21 step 4820/8173 loss 0.2934\n",
      "2020-03-27 15:24:35 step 4840/8173 loss 0.2453\n",
      "2020-03-27 15:24:48 step 4860/8173 loss 0.2565\n",
      "2020-03-27 15:25:02 step 4880/8173 loss 0.2714\n",
      "2020-03-27 15:25:15 step 4900/8173 loss 0.3165\n",
      "2020-03-27 15:25:29 step 4920/8173 loss 0.2494\n",
      "2020-03-27 15:25:42 step 4940/8173 loss 0.2585\n",
      "2020-03-27 15:25:55 step 4960/8173 loss 0.2552\n",
      "2020-03-27 15:26:09 step 4980/8173 loss 0.2847\n",
      "2020-03-27 15:26:22 step 5000/8173 loss 0.3203\n",
      "2020-03-27 15:26:36 step 5020/8173 loss 0.2476\n",
      "2020-03-27 15:26:49 step 5040/8173 loss 0.2452\n",
      "2020-03-27 15:27:03 step 5060/8173 loss 0.2850\n",
      "2020-03-27 15:27:16 step 5080/8173 loss 0.2778\n",
      "2020-03-27 15:27:30 step 5100/8173 loss 0.2415\n",
      "2020-03-27 15:27:43 step 5120/8173 loss 0.2293\n",
      "2020-03-27 15:27:57 step 5140/8173 loss 0.2908\n",
      "2020-03-27 15:28:10 step 5160/8173 loss 0.2674\n",
      "2020-03-27 15:28:24 step 5180/8173 loss 0.2289\n",
      "2020-03-27 15:28:37 step 5200/8173 loss 0.2761\n",
      "2020-03-27 15:28:51 step 5220/8173 loss 0.3463\n",
      "2020-03-27 15:29:04 step 5240/8173 loss 0.2761\n",
      "2020-03-27 15:29:18 step 5260/8173 loss 0.2107\n",
      "2020-03-27 15:29:31 step 5280/8173 loss 0.2531\n",
      "2020-03-27 15:29:45 step 5300/8173 loss 0.2505\n",
      "2020-03-27 15:29:58 step 5320/8173 loss 0.2590\n",
      "2020-03-27 15:30:12 step 5340/8173 loss 0.3010\n",
      "2020-03-27 15:30:25 step 5360/8173 loss 0.2860\n",
      "2020-03-27 15:30:39 step 5380/8173 loss 0.2588\n",
      "2020-03-27 15:30:52 step 5400/8173 loss 0.2724\n",
      "2020-03-27 15:31:06 step 5420/8173 loss 0.3024\n",
      "2020-03-27 15:31:19 step 5440/8173 loss 0.2888\n",
      "2020-03-27 15:31:33 step 5460/8173 loss 0.2848\n",
      "2020-03-27 15:31:46 step 5480/8173 loss 0.3208\n",
      "2020-03-27 15:32:00 step 5500/8173 loss 0.2873\n",
      "2020-03-27 15:32:13 step 5520/8173 loss 0.2974\n",
      "2020-03-27 15:32:26 step 5540/8173 loss 0.2525\n",
      "2020-03-27 15:32:40 step 5560/8173 loss 0.2836\n",
      "2020-03-27 15:32:53 step 5580/8173 loss 0.3356\n",
      "2020-03-27 15:33:07 step 5600/8173 loss 0.2821\n",
      "2020-03-27 15:33:20 step 5620/8173 loss 0.2617\n",
      "2020-03-27 15:33:34 step 5640/8173 loss 0.2716\n",
      "2020-03-27 15:33:47 step 5660/8173 loss 0.2739\n",
      "2020-03-27 15:34:01 step 5680/8173 loss 0.2737\n",
      "2020-03-27 15:34:14 step 5700/8173 loss 0.2691\n",
      "2020-03-27 15:34:28 step 5720/8173 loss 0.2605\n",
      "2020-03-27 15:34:41 step 5740/8173 loss 0.2676\n",
      "2020-03-27 15:34:55 step 5760/8173 loss 0.2935\n",
      "2020-03-27 15:35:08 step 5780/8173 loss 0.2736\n",
      "2020-03-27 15:35:22 step 5800/8173 loss 0.2338\n",
      "2020-03-27 15:35:35 step 5820/8173 loss 0.2534\n",
      "2020-03-27 15:35:49 step 5840/8173 loss 0.2621\n",
      "2020-03-27 15:36:02 step 5860/8173 loss 0.2258\n",
      "2020-03-27 15:36:15 step 5880/8173 loss 0.2506\n",
      "2020-03-27 15:36:29 step 5900/8173 loss 0.2928\n",
      "2020-03-27 15:36:42 step 5920/8173 loss 0.2750\n",
      "2020-03-27 15:36:56 step 5940/8173 loss 0.2484\n",
      "2020-03-27 15:37:09 step 5960/8173 loss 0.2230\n",
      "2020-03-27 15:37:23 step 5980/8173 loss 0.2747\n",
      "2020-03-27 15:37:36 step 6000/8173 loss 0.2997\n",
      "2020-03-27 15:37:49 step 6020/8173 loss 0.2442\n",
      "2020-03-27 15:38:03 step 6040/8173 loss 0.2491\n",
      "2020-03-27 15:38:16 step 6060/8173 loss 0.2956\n",
      "2020-03-27 15:38:30 step 6080/8173 loss 0.2665\n",
      "2020-03-27 15:38:43 step 6100/8173 loss 0.2894\n",
      "2020-03-27 15:38:57 step 6120/8173 loss 0.3296\n",
      "2020-03-27 15:39:10 step 6140/8173 loss 0.2862\n",
      "2020-03-27 15:39:24 step 6160/8173 loss 0.2704\n",
      "2020-03-27 15:39:37 step 6180/8173 loss 0.2790\n",
      "2020-03-27 15:39:50 step 6200/8173 loss 0.2885\n",
      "2020-03-27 15:40:04 step 6220/8173 loss 0.3126\n",
      "2020-03-27 15:40:17 step 6240/8173 loss 0.2634\n",
      "2020-03-27 15:40:31 step 6260/8173 loss 0.2380\n",
      "2020-03-27 15:40:44 step 6280/8173 loss 0.2555\n",
      "2020-03-27 15:40:58 step 6300/8173 loss 0.2345\n",
      "2020-03-27 15:41:11 step 6320/8173 loss 0.2809\n",
      "2020-03-27 15:41:25 step 6340/8173 loss 0.2379\n",
      "2020-03-27 15:41:38 step 6360/8173 loss 0.2938\n",
      "2020-03-27 15:41:52 step 6380/8173 loss 0.2702\n",
      "2020-03-27 15:42:05 step 6400/8173 loss 0.2648\n",
      "2020-03-27 15:42:19 step 6420/8173 loss 0.2754\n",
      "2020-03-27 15:42:32 step 6440/8173 loss 0.2908\n",
      "2020-03-27 15:42:46 step 6460/8173 loss 0.2768\n",
      "2020-03-27 15:42:59 step 6480/8173 loss 0.3085\n",
      "2020-03-27 15:43:12 step 6500/8173 loss 0.2608\n",
      "2020-03-27 15:43:26 step 6520/8173 loss 0.2453\n",
      "2020-03-27 15:43:39 step 6540/8173 loss 0.3019\n",
      "2020-03-27 15:43:53 step 6560/8173 loss 0.2842\n"
     ]
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "set_seed(SEED)\n",
    "\n",
    "epc2loss_tr = []\n",
    "epc2loss_vl = []\n",
    "epc2scrs_tr = []\n",
    "epc2scrs_vl = []\n",
    "best_scr = float('-inf')\n",
    "save_p = f'{p_out}/mdl.p'\n",
    "lendl = len(dl_tr)\n",
    "for epoch in range(EPOCHS):\n",
    "    print(dtnow(), f'epoch {epoch} starts')\n",
    "    ys_tr_ep = []\n",
    "    prd_tr_ep = []\n",
    "    for step, dat_b in enumerate(dl_tr):\n",
    "        model.train()\n",
    "        x_b,ys_b = (o.to(device) for o in dat_b)\n",
    "        prd_b = model(x_b)\n",
    "        loss =  F.binary_cross_entropy_with_logits(prd_b,ys_b)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "\n",
    "        ys_tr_ep.append(ys_b.cpu().detach().numpy())\n",
    "        prd_tr_ep.append(prd_b.cpu().detach().numpy())\n",
    "        if step%20==0: print(dtnow(),f'step {step}/{lendl} loss {loss.item():.4f}')\n",
    "\n",
    "    prd_tr_ep = np.concatenate(prd_tr_ep)\n",
    "    ys_tr_ep = np.concatenate(ys_tr_ep)\n",
    "    scrs_tr = [roc_auc_score(ys_tr_ep[:,i], prd_tr_ep[:,i]) for i in range(4)]\n",
    "    scr_tr  = roc_auc_score(ys_tr_ep, prd_tr_ep)\n",
    "    loss_tr = loss.item()\n",
    "    \n",
    "    prd_vl_ep,ys_vl_ep,scrs_vl,scr_vl,loss_vl = evaluate(model, dl_vl)\n",
    "    print(f'lss_tr: {loss_tr: .4f}; scr_tr: {scr_tr: .4f}; {scrs_tr}')\n",
    "    print(f'lss_vl: {loss_vl: .4f}; scr_vl: {scr_vl: .4f}; {scrs_vl}')\n",
    "    if scr_vl>best_scr: \n",
    "        best_scr = scr_vl\n",
    "        best_prd_vl = prd_vl_ep\n",
    "        print(f'found better scr, saving model...')\n",
    "        torch.save(model.state_dict(),save_p)\n",
    "    \n",
    "    epc2loss_tr.append(loss_tr)\n",
    "    epc2loss_vl.append(loss_vl)\n",
    "    epc2scrs_tr.append(scrs_tr)\n",
    "    epc2scrs_vl.append(scrs_vl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step, len(dl_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcss20",
   "language": "python",
   "name": "rcss20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
