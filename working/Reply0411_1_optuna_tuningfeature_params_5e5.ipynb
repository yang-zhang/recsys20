{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://localhost:8081/notebooks/git/recsys20/working/0324_6_3_lgb_feat_engr-1e7.ipynb\n",
    "- http://localhost:8081/notebooks/git/recsys20/working/mdl0404_1__xgb_mean_encode.ipynb\n",
    "- http://localhost:8081/notebooks/git/recsys20/working/eda_0404_1.ipynb#v.s.-target\n",
    "- https://www.kaggle.com/discdiver/category-encoders-examples\n",
    "- https://www.kaggle.com/snakayama/lightgbm-using-optuna-optuna-lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TGT='Reply'\n",
    "PRFX='Reply0411_1'\n",
    "\n",
    "trntmstmp=1584412344\n",
    "tsttmstmp=1586133347\n",
    "\n",
    "tm_trn_min,tm_trn_max=(1580947200, 1581551999)\n",
    "tm_tst_min,tm_tst_max=(1581552000, 1582156799)\n",
    "\n",
    "SEED=101\n",
    "\n",
    "valsz = int(5e5)#int(1e5)\n",
    "trnsz = int(5e5)#int(5e5)\n",
    "\n",
    "train_total=148075238\n",
    "test_total=int(1.4e7)\n",
    "print(f'train_total {train_total:,}, test_total {test_total:,}')\n",
    "\n",
    "\n",
    "import datetime\n",
    "def showtm(tm): return datetime.datetime.fromtimestamp(tm).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print([showtm(tm) for tm in (trntmstmp, tsttmstmp)])\n",
    "print([showtm(tm) for tm in (tm_trn_min,tm_trn_max)])\n",
    "print([showtm(tm) for tm in (tm_tst_min,tm_tst_max)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import Counter,defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc, precision_recall_curve\n",
    "import pickle\n",
    "# import xgboost as xgb\n",
    "# import lightgbm as lgb\n",
    "import category_encoders as ce\n",
    "import gc\n",
    "\n",
    "import optuna\n",
    "# optuna.logging.CRITICAL, optuna.logging.FATAL\n",
    "# optuna.logging.ERROR\n",
    "# optuna.logging.WARNING, optuna.logging.WARN\n",
    "# optuna.logging.INFO\n",
    "# optuna.logging.DEBUG\n",
    "# optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "import optuna.integration.lightgbm as lgb\n",
    "optuna.logging.disable_default_handler()\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "\n",
    "import datetime\n",
    "def dtnow(): return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def iou(s1,s2):\n",
    "    return len(s1&s2) / len(s1|s2)\n",
    "\n",
    "HOME='/data/git/recsys20'\n",
    "p_in=f'{HOME}/input'\n",
    "p_out=f'{HOME}/output/{PRFX}'\n",
    "Path(p_out).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc, log_loss\n",
    "\n",
    "def compute_prauc(pred, gt):\n",
    "    prec, recall, thresh = precision_recall_curve(gt, pred)\n",
    "    prauc = auc(recall, prec)\n",
    "    return prauc\n",
    "\n",
    "def calculate_ctr(gt):\n",
    "    positive = len([x for x in gt if x == 1])\n",
    "    ctr = positive/float(len(gt))\n",
    "    return ctr\n",
    "\n",
    "def compute_rce(pred, gt):\n",
    "    cross_entropy = log_loss(gt, pred)\n",
    "    data_ctr = calculate_ctr(gt)\n",
    "    strawman_cross_entropy = log_loss(gt, [data_ctr for _ in range(len(gt))])\n",
    "    return (1.0 - cross_entropy/strawman_cross_entropy)*100.0\n",
    "\n",
    "# https://towardsdatascience.com/how-to-calibrate-undersampled-model-scores-8f3319c1ea5b\n",
    "# How to use the function?\n",
    "# Letâ€™s say your goal is to generate a model that shows the credit default probabilities and your original \n",
    "# training data has 50,000 rows with only 500 of them labeled as target class. When you sample your non-target \n",
    "# instances randomly and reduce the total row count to 10,000, while conserving 500 target rows, our calibration\n",
    "# function becomes:\n",
    "# calibration(model_results, 50000, 500, 10000, 500)\n",
    "# Here model_results is your model probability output array. After you train your model and put the results in it, your function is ready to use. \n",
    "def calibration(data, train_pop, target_pop, sampled_train_pop, sampled_target_pop):\n",
    "    calibrated_data = ((data * (target_pop / train_pop) / (sampled_target_pop / sampled_train_pop)) /\n",
    "    (((1 - data) * (1 - target_pop / train_pop) / (1 - sampled_target_pop / sampled_train_pop)) +\n",
    "     (data * (target_pop / train_pop) / (sampled_target_pop / sampled_train_pop))))\n",
    "    return calibrated_data\n",
    "\n",
    "\n",
    "cols=['toks','hshtgs','twtid','media','links','domns','twttyp','lang','tm','u1id','u1_fllwer_cnt','u1_fllwing_cnt','u1_vrfed','u1_create_tm','u2id','u2_fllwer_cnt','u2_fllwng_cnt','u2_vrfed','u2_create_tm','u1_fllw_u2','reply_tm','retwt_tm','retwt_cmmnt_tm','like_tm',]\n",
    "\n",
    "cols_val = cols[:-4]\n",
    "cols_tgt_tmstmp=[ 'retwt_tm', 'reply_tm', 'like_tm', 'retwt_cmmnt_tm',]\n",
    "cols_tgt=[o.split('_tm')[0] for o in cols_tgt_tmstmp]\n",
    "tgts             = ['Retweet','Reply','Like','RTwCmnt',]\n",
    "assert cols_tgt == ['retwt',  'reply','like','retwt_cmmnt',]\n",
    "tgt2col = dict(zip(tgts, cols_tgt))\n",
    "TCOL=tgt2col[TGT]\n",
    "print('TCOL', TCOL)\n",
    "print(tgt2col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prp_tgt(df):\n",
    "    df[cols_tgt]=df[cols_tgt_tmstmp].notna()\n",
    "    df.drop(columns=cols_tgt_tmstmp, inplace=True)\n",
    "    display(df[cols_tgt].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dfval = pd.read_csv(f'{p_in}/trn_{trntmstmp}.tsv',sep='\\x01',header=None,names=cols,nrows=valsz)\n",
    "print(dfval.shape)\n",
    "prp_tgt(dfval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## maybe oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(f'{p_in}/trn_{trntmstmp}.tsv',sep='\\x01',header=None,names=cols,nrows=5)\n",
    "# dfval = pd.read_csv(f'{p_in}/trn_{trntmstmp}.tsv',sep='\\x01',header=None,names=cols,nrows=2)\n",
    "# dftrn = pd.read_csv(f'{p_in}/trn_{trntmstmp}.tsv',sep='\\x01',header=None,names=cols,skiprows=2,nrows=3)\n",
    "# display(df.twtid)\n",
    "# display(dfval.twtid)\n",
    "# display(dftrn.twtid)\n",
    "\n",
    "# 0    D4D1EBDE74F74C5DA529959AF979625C\n",
    "# 1    BFB529DAB6D384EB83E899A72AB3830D\n",
    "# 2    519078C7834E9642508F72A6C2D0F3B7\n",
    "# 3    52AAE9E33EFAC8C478C57B31A9E31ED1\n",
    "# 4    89C1298C55EB3D68E2784F0BFB69E6F8\n",
    "# Name: twtid, dtype: object\n",
    "# 0    D4D1EBDE74F74C5DA529959AF979625C\n",
    "# 1    BFB529DAB6D384EB83E899A72AB3830D\n",
    "# Name: twtid, dtype: object\n",
    "# 0    519078C7834E9642508F72A6C2D0F3B7\n",
    "# 1    52AAE9E33EFAC8C478C57B31A9E31ED1\n",
    "# 2    89C1298C55EB3D68E2784F0BFB69E6F8\n",
    "# Name: twtid, dtype: object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if TGT in ('Retweet','Like'): trnpop=trnsz\n",
    "if TGT=='Reply': trnpop=trnsz*3.7\n",
    "if TGT=='RTwCmnt': trnpop=trnsz*11.5\n",
    "assert trnpop<train_total\n",
    "\n",
    "dftrn = pd.read_csv(f'{p_in}/trn_{trntmstmp}.tsv',sep='\\x01',header=None,names=cols,\n",
    "                     skiprows=valsz, nrows=trnpop)\n",
    "print(dftrn.shape)\n",
    "prp_tgt(dftrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_pop=len(dftrn)\n",
    "sampled_train_pop=trnsz\n",
    "idx_pos = np.where(dftrn[TCOL])[0]\n",
    "target_pop=sampled_target_pop=len(idx_pos)\n",
    "if TGT in ('Reply','RTwCmnt'):\n",
    "    idx_neg0 = np.where(~dftrn[TCOL])[0]\n",
    "    idx_neg = np.random.choice(idx_neg0, trnsz-len(idx_pos), replace=False)\n",
    "    idx = np.concatenate([idx_pos,idx_neg])\n",
    "    dftrn = dftrn.iloc[idx].copy()\n",
    "print(dftrn.shape, dftrn[TCOL].mean())\n",
    "print(train_pop, target_pop, sampled_train_pop, sampled_target_pop)\n",
    "\n",
    "display(dftrn.head())\n",
    "display(dftrn.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prp_df(df, tm_max):\n",
    "    '''\n",
    "    tm_max = tm_trn_max for train tm_tst_max for test\n",
    "    '''\n",
    "    lendf = len(df)\n",
    "    print(dtnow(), 'start')\n",
    "    set_u1 = set(df.u1id)\n",
    "    set_u2 = set(df.u2id)\n",
    "    df['u1inu2']=df.u1id.isin(set_u2)\n",
    "    df['u2inu1']=df.u2id.isin(set_u1)\n",
    "\n",
    "    #counts\n",
    "    df['len_toks'] = df.toks.apply(lambda x: len(x.split('\\t')))\n",
    "    df.drop(columns=['toks'], inplace=True)    \n",
    "    for m in ['Photo', 'Video', 'GIF']:\n",
    "        df[f'n_media_{m}'] = df['media'].fillna('').apply(lambda x: x.split('\\t').count(m))\n",
    "    tgnms = ['hshtgs','media', 'links', 'domns',]\n",
    "    for tgnm in tgnms:\n",
    "        df[f'has_{tgnm}']=df[tgnm].notna()\n",
    "        df[f'lst_{tgnm}'] = df[tgnm].fillna('').apply(lambda x: x.split('\\t') if len(x) else [])\n",
    "        df[f'n_{tgnm}'] = df[f'lst_{tgnm}'].apply(len)  \n",
    "    df.drop(columns=['has_links','n_links'], inplace=True) #duplicates has_domns,n_domns \n",
    "    \n",
    "    #time\n",
    "    dt = pd.to_datetime(df.tm, unit='s')\n",
    "    df['dayofweek'] = dt.dt.dayofweek\n",
    "    df['hour'] = dt.dt.hour\n",
    "    \n",
    "    df['tmdlta_u2u1']  = df.u2_create_tm - df.u1_create_tm\n",
    "    df['tmdlta_twtu1'] = df.tm - df.u1_create_tm\n",
    "    df['tmdlta_twtu2'] = df.tm - df.u2_create_tm\n",
    "\n",
    "    df['twt_age']   = tm_max-df.tm\n",
    "    df['u1_age']    = tm_max-df.u1_create_tm\n",
    "    df['u2_age']    = tm_max-df.u2_create_tm\n",
    "    df.drop(columns=['tm', 'u1_create_tm', 'u2_create_tm'], inplace=True)\n",
    "\n",
    "    #time derived\n",
    "    df['u1_fllwer_cnt_by_age'] = df.u1_fllwer_cnt / df.u1_age\n",
    "    df['u1_fllwng_cnt_by_age'] = df.u2_fllwng_cnt / df.u2_age\n",
    "    \n",
    "    #interaction\n",
    "    df['u1u2']=df.u1id+'_'+df.u2id\n",
    "    df['langhour']=df.lang+'_'+df.hour.astype(str)\n",
    "    \n",
    "\n",
    "    #freq of feature values\n",
    "    print(dtnow(), 'freq of columns using CountEncoder')\n",
    "    encoder = ce.CountEncoder()\n",
    "    encoded = encoder.fit_transform(\n",
    "    df[['twtid', 'twttyp', 'lang', 'u1id', 'u1_fllwing_cnt', 'u1_vrfed', 'u2id', 'u2_fllwer_cnt', 'u2_vrfed', 'u1_fllw_u2', \n",
    "       'n_media_Photo', 'n_media_Video', 'n_media_GIF', \n",
    "       'has_hshtgs', 'n_hshtgs', 'has_media', 'n_media','has_domns', 'n_domns', \n",
    "       'dayofweek', 'hour', 'u1u2', 'langhour']].astype(object))\n",
    "    encoded = encoded.astype(int)/lendf\n",
    "    encoded.columns = [f'frq_{col}' for col in encoded.columns]\n",
    "    df = pd.concat([df,encoded],1)\n",
    "    \n",
    "    #freq of tgnm values\n",
    "    print(dtnow(), 'freq of tags')\n",
    "    for tgnm in tgnms:\n",
    "        vs = [j for i in df[f'lst_{tgnm}'] for j in i]\n",
    "        cnt = Counter(vs)\n",
    "        frq = {k:v/lendf for k,v in cnt.items()}\n",
    "        df[f'sumfrq_{tgnm}']=df[f'lst_{tgnm}'].apply(lambda x: sum([frq.get(o,0) for o in x]))\n",
    "        df[f'maxfrq_{tgnm}']=df[f'lst_{tgnm}'].apply(lambda x: max([frq.get(o,0) for o in x]) if len(x) else 0)\n",
    "    df.drop(columns=tgnms+[f'lst_{tgnm}' for tgnm in tgnms],inplace=True)\n",
    "\n",
    "    print(dtnow(), 'done')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dftrn=prp_df(dftrn, tm_max=tm_trn_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dfval=prp_df(dfval, tm_max=tm_trn_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tr vl split and target encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_vl=np.random.rand(len(dftrn))<0.15\n",
    "idxvl=np.where( msk_vl)[0]\n",
    "idxtr=np.where(~msk_vl)[0]\n",
    "\n",
    "dftr = dftrn.iloc[idxtr].copy()\n",
    "dfvl = dftrn.iloc[idxvl].copy()\n",
    "del dftrn\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "col2tgtenc=['twtid', 'twttyp', 'lang', 'u1id', 'u1_fllwing_cnt', 'u1_vrfed', 'u2id', 'u2_fllwer_cnt', 'u2_vrfed', 'u1_fllw_u2', \n",
    "   'n_media_Photo', 'n_media_Video', 'n_media_GIF', \n",
    "   'has_hshtgs', 'n_hshtgs', 'has_media', 'n_media','has_domns', 'n_domns', \n",
    "   'dayofweek', 'hour', 'u1u2', 'langhour']\n",
    "tgt_encoder = ce.TargetEncoder()\n",
    "encoded_tr = tgt_encoder.fit_transform(dftr[col2tgtenc].astype(object), dftr[TCOL])\n",
    "encoded_vl = tgt_encoder.transform(dfvl[col2tgtenc].astype(object))\n",
    "encoded_val = tgt_encoder.transform(dfval[col2tgtenc].astype(object))\n",
    "\n",
    "tgtenc_columns = [f'tgtenc_{col}' for col in encoded_tr.columns]\n",
    "encoded_tr.columns = tgtenc_columns\n",
    "encoded_vl.columns = tgtenc_columns\n",
    "encoded_val.columns = tgtenc_columns\n",
    "\n",
    "dftr = pd.concat([dftr, encoded_tr], 1)\n",
    "dfvl = pd.concat([dfvl, encoded_vl], 1)\n",
    "dfval = pd.concat([dfval, encoded_val], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_category=['twttyp', 'lang', 'langhour']\n",
    "for df in dftr,dfvl,dfval:\n",
    "    df.drop(columns=['twtid','u1id','u2id','u1u2'], inplace=True)\n",
    "    df[cols_category]=df[cols_category].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_feat = [col for col in dftr.columns if col not in cols_tgt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cols_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cols_feat))\n",
    "cols_feat=['twttyp',\n",
    " 'lang',\n",
    " 'u1_fllwer_cnt',\n",
    " 'u1_fllwing_cnt',\n",
    " 'u1_vrfed',\n",
    " 'u2_fllwer_cnt',\n",
    " 'u2_fllwng_cnt',\n",
    " 'u2_vrfed',\n",
    " 'u1_fllw_u2',\n",
    " 'u1inu2',\n",
    " 'u2inu1',\n",
    " 'len_toks',\n",
    " 'n_media_Photo',\n",
    " 'n_media_Video',\n",
    " 'n_media_GIF',\n",
    "#  'has_hshtgs',\n",
    " 'n_hshtgs',\n",
    "#  'has_media',\n",
    " 'n_media',\n",
    "#  'has_domns',\n",
    " 'n_domns',\n",
    " 'dayofweek',\n",
    " 'hour',\n",
    " 'tmdlta_u2u1',\n",
    " 'tmdlta_twtu1',\n",
    " 'tmdlta_twtu2',\n",
    " 'twt_age',\n",
    " 'u1_age',\n",
    " 'u2_age',\n",
    " 'u1_fllwer_cnt_by_age',\n",
    " 'u1_fllwng_cnt_by_age',\n",
    " 'langhour',\n",
    " 'frq_twtid',\n",
    "#  'frq_twttyp',\n",
    "#  'frq_lang',\n",
    " 'frq_u1id',\n",
    "#  'frq_u1_fllwing_cnt',\n",
    "#  'frq_u1_vrfed',\n",
    " 'frq_u2id',\n",
    "#  'frq_u2_fllwer_cnt',\n",
    "#  'frq_u2_vrfed',\n",
    "#  'frq_u1_fllw_u2',\n",
    "#  'frq_n_media_Photo',\n",
    "#  'frq_n_media_Video',\n",
    "#  'frq_n_media_GIF',\n",
    "#  'frq_has_hshtgs',\n",
    "#  'frq_n_hshtgs',\n",
    "#  'frq_has_media',\n",
    "#  'frq_n_media',\n",
    "#  'frq_has_domns',\n",
    "#  'frq_n_domns',\n",
    "#  'frq_dayofweek',\n",
    "#  'frq_hour',\n",
    " 'frq_u1u2',\n",
    "#  'frq_langhour',\n",
    " 'sumfrq_hshtgs',\n",
    "#  'maxfrq_hshtgs',\n",
    "#  'sumfrq_media',\n",
    "#  'maxfrq_media',\n",
    "#  'sumfrq_links',\n",
    "#  'maxfrq_links',\n",
    " 'sumfrq_domns',\n",
    "#  'maxfrq_domns',\n",
    "#  'tgtenc_twtid',\n",
    "#  'tgtenc_twttyp',\n",
    " 'tgtenc_lang',\n",
    "#  'tgtenc_u1id',\n",
    "#  'tgtenc_u1_fllwing_cnt',\n",
    "#  'tgtenc_u1_vrfed',\n",
    "#  'tgtenc_u2id',\n",
    "#  'tgtenc_u2_fllwer_cnt',\n",
    "#  'tgtenc_u2_vrfed',\n",
    "#  'tgtenc_u1_fllw_u2',\n",
    "#  'tgtenc_n_media_Photo',\n",
    "#  'tgtenc_n_media_Video',\n",
    "#  'tgtenc_n_media_GIF',\n",
    "#  'tgtenc_has_hshtgs',\n",
    "#  'tgtenc_n_hshtgs',\n",
    "#  'tgtenc_has_media',\n",
    "#  'tgtenc_n_media',\n",
    "#  'tgtenc_has_domns',\n",
    "#  'tgtenc_n_domns',\n",
    "#  'tgtenc_dayofweek',\n",
    "#  'tgtenc_hour',\n",
    "#  'tgtenc_u1u2',\n",
    "#  'tgtenc_langhour'\n",
    "          ]\n",
    "print(len(cols_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results['PRFX'] = PRFX\n",
    "results['TGT'] = TGT\n",
    "results['cols_feat'] = cols_feat\n",
    "results['tgt_encoder'] = tgt_encoder\n",
    "results['col2tgtenc'] = col2tgtenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'verbosity': 0,    \n",
    "}\n",
    "results['params'] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dtr = lgb.Dataset(dftr[cols_feat], label=dftr[TCOL])\n",
    "dvl = lgb.Dataset(dfvl[cols_feat], label=dfvl[TCOL])\n",
    "best_params, tuning_history = {}, []\n",
    "evalres = {}\n",
    "evallist = [(dtr, 'train'), (dvl, 'eval')]\n",
    "bst = lgb.train(params=params, \n",
    "                train_set=dtr, \n",
    "                num_boost_round=50000,\n",
    "                valid_sets=[dtr, dvl],\n",
    "                valid_names=['tr','vl'],\n",
    "                best_params=best_params,\n",
    "                tuning_history=tuning_history,\n",
    "                verbose_eval=100,\n",
    "                early_stopping_rounds=100,\n",
    "                evals_result=evalres,\n",
    "               )\n",
    "results['bst'] = bst\n",
    "results['evalres'] = evalres\n",
    "results['best_params']=best_params\n",
    "results['tuning_history']=tuning_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tr vl trajec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(evalres['tr']['binary_logloss'])\n",
    "plt.plot(evalres['vl']['binary_logloss'])\n",
    "plt.title(f\"logloss; best_iteration {bst.best_iteration}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = lgb.plot_importance(bst, height=0.8, max_num_features=50, figsize=(10,15))\n",
    "ax.grid(False, axis=\"y\")\n",
    "ax.set_title(f'Estimated feature importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([bst.feature_name(), bst.feature_importance()]).T.sort_values(1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col2imp = dict(zip(bst.feature_name(), bst.feature_importance()))\n",
    "sorted(col2imp.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[o[0] for o in sorted(col2imp.items(), key=lambda x: -x[1]) if o[1]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prdtr = bst.predict(dftr[cols_feat],num_iteration=bst.best_iteration)\n",
    "prdvl = bst.predict(dfvl[cols_feat],num_iteration=bst.best_iteration)\n",
    "prdval0 = bst.predict(dfval[cols_feat],num_iteration=bst.best_iteration)\n",
    "prdval = calibration(prdval0, train_pop, target_pop, sampled_train_pop, sampled_target_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ytr,yvl,yval=[df[TCOL].values for df in (dftr,dfvl,dfval)]\n",
    "\n",
    "auc_tr=compute_prauc(prdtr, ytr)\n",
    "rce_tr=compute_rce(prdtr, ytr)\n",
    "auc_vl=compute_prauc(prdvl, yvl)\n",
    "rce_vl=compute_rce(prdvl, yvl)\n",
    "auc_val=compute_prauc(prdval, yval)\n",
    "rce_val=compute_rce(prdval, yval)\n",
    "\n",
    "results['scrs'] = {}\n",
    "results['scrs']['auc_tr']=auc_tr\n",
    "results['scrs']['rce_tr']=rce_tr\n",
    "results['scrs']['auc_vl']=auc_vl\n",
    "results['scrs']['rce_vl']=rce_vl\n",
    "results['scrs']['auc_val']=auc_val\n",
    "results['scrs']['rce_val']=rce_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'auc_tr:  {auc_tr:.4f}')\n",
    "print(f'auc_vl:  {auc_vl:.4f}')\n",
    "print(f'auc_val: {auc_val:.4f}')\n",
    "print()\n",
    "print(f'rce_tr:  {rce_tr:.4f}')\n",
    "print(f'rce_vl:  {rce_vl:.4f}')\n",
    "print(f'rce_val: {rce_val:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'-{auc_val:.4f}-{rce_val:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(results, open(f'{p_out}/results_{PRFX}.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pickle.load( open(f'{p_out}/results_{PRFX}.p', 'rb'))\n",
    "bst = results['bst']\n",
    "col2tgtenc = results['col2tgtenc']\n",
    "cols_feat = results['cols_feat']\n",
    "tgt_encoder = results['tgt_encoder']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dftst=pd.read_csv(\n",
    "    f'{p_in}/val_{tsttmstmp}.tsv',\n",
    "    sep='\\x01', header=None, names=cols_val, \n",
    "    nrows=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dftst=prp_df(dftst, tm_max=tm_tst_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tst = tgt_encoder.transform(dftst[col2tgtenc].astype(object))\n",
    "tgtenc_columns = [f'tgtenc_{col}' for col in encoded_tst.columns]\n",
    "encoded_tst.columns = tgtenc_columns\n",
    "dftst = pd.concat([dftst, encoded_tst], 1)\n",
    "dftst.drop(columns=['u1id','u1u2'], inplace=True)\n",
    "cols_category=['twttyp', 'lang', 'langhour']\n",
    "dftst[cols_category]=dftst[cols_category].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prdtst0 = bst.predict(dftst[cols_feat],num_iteration=bst.best_iteration)\n",
    "prdtst = calibration(prdtst0, train_pop, target_pop, sampled_train_pop, sampled_target_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsub_ids = dftst[['twtid','u2id',]]\n",
    "dfsub = dfsub_ids.copy()\n",
    "dfsub['scr'] = prdtst\n",
    "dfsub.to_csv(f'{p_out}/{TGT}__{PRFX}.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcss20",
   "language": "python",
   "name": "rcss20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
