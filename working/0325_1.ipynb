{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/learn/feature-engineering\n",
    "user count feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-03-17 02:32:24', '2020-03-26 16:11:23']\n",
      "['2020-02-06 00:00:00', '2020-02-13 00:00:00']\n",
      "n_chnks_vl 1\n",
      "n_chnks_tr 10\n"
     ]
    }
   ],
   "source": [
    "PRFX='0325_1'\n",
    "trntmstmp=1584412344\n",
    "valtmstmp=1585239083\n",
    "import datetime\n",
    "print([datetime.datetime.fromtimestamp(o).strftime('%Y-%m-%d %H:%M:%S') for o in (trntmstmp, valtmstmp)])\n",
    "\n",
    "grand_total=1.5e8\n",
    "MIN_TM_TRN=1580947200\n",
    "MIN_TM_TST=1581552000\n",
    "print([datetime.datetime.fromtimestamp(o).strftime('%Y-%m-%d %H:%M:%S') for o in (MIN_TM_TRN, MIN_TM_TST)])\n",
    "\n",
    "\n",
    "CHNKSZ=int(1e6)\n",
    "VLSZ=int(1e6)\n",
    "n_chnks_vl=int(VLSZ/CHNKSZ)\n",
    "print('n_chnks_vl', n_chnks_vl)\n",
    "TRSZ=int(1e7)\n",
    "n_chnks_tr=int(TRSZ/CHNKSZ)\n",
    "print('n_chnks_tr', n_chnks_tr)\n",
    "\n",
    "POST_RATE_WANTED=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Retweet': 'retwt',\n",
       " 'Reply': 'reply',\n",
       " 'Like': 'like',\n",
       " 'RTwCmnt': 'retwt_cmmnt'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc, precision_recall_curve\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import category_encoders as ce\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "from functools import reduce\n",
    "import datetime\n",
    "import time\n",
    "# tm_start=time.time()\n",
    "# time.sleep(2)\n",
    "# time.time()-tm_start\n",
    "\n",
    "def dtnow(): return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "SEED=101\n",
    "HOME='/data/git/recsys20'\n",
    "p_in=f'{HOME}/input'\n",
    "p_out=f'{HOME}/output/{PRFX}'\n",
    "Path(p_out).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc, log_loss\n",
    "\n",
    "def compute_prauc(pred, gt):\n",
    "    prec, recall, thresh = precision_recall_curve(gt, pred)\n",
    "    prauc = auc(recall, prec)\n",
    "    return prauc\n",
    "\n",
    "def calculate_ctr(gt):\n",
    "    positive = len([x for x in gt if x == 1])\n",
    "    ctr = positive/float(len(gt))\n",
    "    return ctr\n",
    "\n",
    "def compute_rce(pred, gt):\n",
    "    cross_entropy = log_loss(gt, pred)\n",
    "    data_ctr = calculate_ctr(gt)\n",
    "    strawman_cross_entropy = log_loss(gt, [data_ctr for _ in range(len(gt))])\n",
    "    return (1.0 - cross_entropy/strawman_cross_entropy)*100.0\n",
    "\n",
    "# https://towardsdatascience.com/how-to-calibrate-undersampled-model-scores-8f3319c1ea5b\n",
    "# How to use the function?\n",
    "# Letâ€™s say your goal is to generate a model that shows the credit default probabilities and your original \n",
    "# training data has 50,000 rows with only 500 of them labeled as target class. When you sample your non-target \n",
    "# instances randomly and reduce the total row count to 10,000, while conserving 500 target rows, our calibration\n",
    "# function becomes:\n",
    "# calibration(model_results, 50000, 500, 10000, 500)\n",
    "# Here model_results is your model probability output array. After you train your model and put the results in it, your function is ready to use. \n",
    "\n",
    "def calibration(data, train_pop, target_pop, sampled_train_pop, sampled_target_pop):\n",
    "    calibrated_data = \\\n",
    "    ((data * (target_pop / train_pop) / (sampled_target_pop / sampled_train_pop)) /\n",
    "    ((\n",
    "        (1 - data) * (1 - target_pop / train_pop) / (1 - sampled_target_pop / sampled_train_pop)\n",
    "     ) +\n",
    "     (\n",
    "        data * (target_pop / train_pop) / (sampled_target_pop / sampled_train_pop)\n",
    "     )))\n",
    "\n",
    "    return calibrated_data\n",
    "\n",
    "cols=[\n",
    "'toks',\n",
    "'hshtgs',\n",
    "'twtid',\n",
    "'media',\n",
    "'links',\n",
    "'domns',\n",
    "'twttyp',\n",
    "'lang',\n",
    "'tm',\n",
    "\n",
    "'u1id',\n",
    "'u1_fllwer_cnt',\n",
    "'u1_fllwng_cnt',\n",
    "'u1_vrfed',\n",
    "'u1_create_tm',\n",
    "\n",
    "'u2id',\n",
    "'u2_fllwer_cnt',\n",
    "'u2_fllwng_cnt',\n",
    "'u2_vrfed',\n",
    "'u2_create_tm',\n",
    "\n",
    "'u1_fllw_u2',\n",
    "'reply_tm',\n",
    "'retwt_tm',\n",
    "'retwt_cmmnt_tm',\n",
    "'like_tm',\n",
    "]\n",
    "cols_cat = ['twttyp','lang']\n",
    "cols_val = cols[:-4]\n",
    "cols_tgt_tmstmp=[\n",
    "    'retwt_tm',\n",
    "    'reply_tm',\n",
    "    'like_tm',\n",
    "    'retwt_cmmnt_tm',\n",
    "]\n",
    "cols_tgt=[o.split('_tm')[0] for o in cols_tgt_tmstmp]\n",
    "tgts             = ['Retweet','Reply','Like','RTwCmnt',]\n",
    "assert cols_tgt == ['retwt',  'reply','like','retwt_cmmnt',]\n",
    "ntgts=len(tgts)\n",
    "\n",
    "\n",
    "tgt2col=dict(zip(tgts,cols_tgt))\n",
    "tgt2col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prp_df(df, istrn=True):\n",
    "    tm_min = MIN_TM_TRN if istrn else MIN_TM_TST\n",
    "    df['len_toks'] = df.toks.apply(len)\n",
    "    for media in ['Photo', 'Video', 'GIF']:\n",
    "        df[f'has_media_{media}'] = df.media.fillna('').apply(lambda x: media in x)\n",
    "    for col in ['hshtgs', 'links', 'domns',]:\n",
    "        df[f'num_{col}'] = df[col].fillna('').apply(lambda x: len(x.split('\\t')) if len(x) else 0)\n",
    "\n",
    "    df['twt_age'] = df.tm - tm_min\n",
    "    df['u1_age']  = df.tm - df.u1_create_tm\n",
    "    df['u2_age']  = df.tm - df.u2_create_tm\n",
    "\n",
    "    tm_dt=pd.to_datetime(df.tm, unit='s')\n",
    "    df['tm_dayofweek']=tm_dt.dt.dayofweek\n",
    "    df['tm_hour']=tm_dt.dt.hour\n",
    "\n",
    "    df['tmdlta_u2u1']  = df.u2_create_tm - df.u1_create_tm\n",
    "\n",
    "    df['u1_fllwer_cnt_by_age'] = df.u1_fllwer_cnt / df.u1_age\n",
    "    df['u1_fllwng_cnt_by_age'] = df.u2_fllwng_cnt / df.u2_age\n",
    "\n",
    "    for col in cols_cat:\n",
    "        df[col]=df[col].astype('category')\n",
    "\n",
    "    df['u1u2']=df.u1id+'_'+df.u2id\n",
    "    ce_features = ['twttyp', 'lang', 'u1id', 'u2id', 'u1u2', 'twtid']\n",
    "    cnt_encd = ce.CountEncoder()\n",
    "    cnt_encded = cnt_encd.fit_transform(df[ce_features])\n",
    "    cnt_encded = cnt_encded.astype(int)\n",
    "    cnt_encded.columns = [f'{col}_cnt' for col in ce_features]\n",
    "\n",
    "    df = pd.concat([df,cnt_encded],1)\n",
    "    if istrn: \n",
    "        df[cols_tgt]=df[cols_tgt_tmstmp].notna().astype('int8')\n",
    "        df.drop(inplace=True, columns=['toks', 'hshtgs', 'media', 'links', 'domns',  \n",
    "                                       'tm', 'u1_create_tm','u2_create_tm', 'u1id', 'u1u2', 'u2id', 'twtid', ]+cols_tgt_tmstmp, )\n",
    "    else:\n",
    "        df.drop(inplace=True, columns=['toks', 'hshtgs', 'media', 'links', 'domns', \n",
    "                                       'tm', 'u1_create_tm','u2_create_tm', 'u1id', 'u1u2'])   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000000.0, 150.0, 15.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grand_total, grand_total/CHNKSZ, grand_total/TRSZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 14:32:22 chunk 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/rcss20/lib/python3.7/site-packages/category_encoders/count.py:255: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  X.loc[:, self.cols] = X.fillna(value=pd.np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfvalid.shape: (1000000, 34)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "twttyp                  category\n",
       "lang                    category\n",
       "u1_fllwer_cnt              int64\n",
       "u1_fllwng_cnt              int64\n",
       "u1_vrfed                    bool\n",
       "u2_fllwer_cnt              int64\n",
       "u2_fllwng_cnt              int64\n",
       "u2_vrfed                    bool\n",
       "u1_fllw_u2                  bool\n",
       "len_toks                   int64\n",
       "has_media_Photo             bool\n",
       "has_media_Video             bool\n",
       "has_media_GIF               bool\n",
       "num_hshtgs                 int64\n",
       "num_links                  int64\n",
       "num_domns                  int64\n",
       "twt_age                    int64\n",
       "u1_age                     int64\n",
       "u2_age                     int64\n",
       "tm_dayofweek               int64\n",
       "tm_hour                    int64\n",
       "tmdlta_u2u1                int64\n",
       "u1_fllwer_cnt_by_age     float64\n",
       "u1_fllwng_cnt_by_age     float64\n",
       "twttyp_cnt                 int64\n",
       "lang_cnt                   int64\n",
       "u1id_cnt                   int64\n",
       "u2id_cnt                   int64\n",
       "u1u2_cnt                   int64\n",
       "twtid_cnt                  int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chnks_trn = pd.read_csv(f'{p_in}/trn_{trntmstmp}.tsv',sep='\\x01',\n",
    "                    header=None,names=cols, \n",
    "                    chunksize=CHNKSZ)\n",
    "# first chunk as validate data\n",
    "for ichnk,df in enumerate(chnks_trn):\n",
    "    if ichnk==n_chnks_vl: break\n",
    "    print(dtnow(), 'chunk', ichnk)\n",
    "#     print([datetime.datetime.fromtimestamp(o).strftime('%Y-%m-%d %H:%M:%S') \n",
    "#            for o in (df.tm.min(), df.tm.max())])\n",
    "    dfvalid = prp_df(df)\n",
    "print('dfvalid.shape:',dfvalid.shape)\n",
    "\n",
    "cols_feat=[o for o in dfvalid.columns if o not in cols_tgt]\n",
    "\n",
    "display(dfvalid[cols_feat].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trnval data func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdftrvl(tgt):\n",
    "    print(tgt)\n",
    "    tgtcol=tgt2col[tgt]\n",
    "    chnks_trn = pd.read_csv(f'{p_in}/trn_{trntmstmp}.tsv',sep='\\x01',\n",
    "                        header=None,names=cols, \n",
    "                            chunksize=CHNKSZ)\n",
    "    len_df_wanted = TRSZ\n",
    "    # retwt          0.113031\n",
    "    # reply          0.027488\n",
    "    # like           0.439499\n",
    "    # retwt_cmmnt    0.007742\n",
    "    pos_rate_wanted = POST_RATE_WANTED\n",
    "    n_pos_wanted = int(len_df_wanted*pos_rate_wanted)\n",
    "    print('n_pos_wanted', n_pos_wanted)\n",
    "    np.random.seed(SEED)\n",
    "    lst_df = []\n",
    "    n_pos_ttl = 0\n",
    "    for ichnk,df in enumerate(chnks_trn):\n",
    "        #skip first chunks (it was validate data)\n",
    "        if ichnk<n_chnks_vl: continue\n",
    "        df = prp_df(df)\n",
    "        n_pos_ttl+= df[tgtcol].sum()\n",
    "        lst_df.append(df)\n",
    "        if (ichnk>=n_chnks_vl+n_chnks_tr) and (n_pos_ttl>=n_pos_wanted): break\n",
    "        print(dtnow(), 'chunk', ichnk, 'n_pos_ttl', n_pos_ttl)\n",
    "\n",
    "    df = pd.concat(lst_df)\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "    # https://stackoverflow.com/questions/28556942/pandas-remove-rows-at-random-without-shuffling-dataset\n",
    "    idx_neg=np.where(df[tgtcol]==0)[0]\n",
    "    n_neg = len(idx_neg)\n",
    "    n_pos = len(df)-len(idx_neg)\n",
    "    n_neg2keep = len_df_wanted-n_pos\n",
    "    n_neg2rmv = n_neg-n_neg2keep\n",
    "    if n_neg2rmv>0:\n",
    "        idx_neg2rmv = np.random.choice(idx_neg, n_neg2rmv, replace=False)\n",
    "        dftrvl = df.drop(idx_neg2rmv)\n",
    "    else:\n",
    "        dftrvl = df\n",
    "    dftrvl = dftrvl.sample(len_df_wanted, replace=False)\n",
    "    for col in cols_cat:\n",
    "        dftrvl[col]=dftrvl[col].astype('category')\n",
    "    \n",
    "#     display(dftrvl.dtypes)\n",
    "    print('dftrvl.shape:',dftrvl.shape,'dftrvl[tgtcol].mean():',dftrvl[tgtcol].mean())\n",
    "    \n",
    "    pops={\n",
    "        'train_pop':len(df),\n",
    "        'target_pop':n_pos,\n",
    "        'sampled_train_pop':len_df_wanted,\n",
    "        'sampled_target_pop':n_pos,\n",
    "    }\n",
    "    print(pops)\n",
    "    return dftrvl, pops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params,dtr,dvl):\n",
    "    print(params)\n",
    "    evals_result = {}\n",
    "    evallist = [(dtr, 'train'), (dvl, 'eval')]\n",
    "    bst = lgb.train(params=params, \n",
    "                    train_set=dtr, \n",
    "                    num_boost_round=100000,\n",
    "                    valid_sets=[dtr, dvl],\n",
    "                    verbose_eval=1000,\n",
    "                    early_stopping_rounds=100,\n",
    "                    evals_result=evals_result,\n",
    "                   )\n",
    "    return bst,evals_result\n",
    "\n",
    "def valid(bst,dftr,dfvl):\n",
    "    prdtr = bst.predict(dftr[cols_feat],num_iteration=bst.best_iteration)\n",
    "    prdvl = bst.predict(dfvl[cols_feat],num_iteration=bst.best_iteration)\n",
    "    return prdtr,prdvl\n",
    "\n",
    "def do_tgt(tgt):\n",
    "    params=tgt2params[tgt]\n",
    "    tgtcol=tgt2col[tgt]\n",
    "    dftrvl, pops=getdftrvl(tgt)\n",
    "    split=int(len(dftrvl)*0.85)\n",
    "    dftr,dfvl=dftrvl[:split],dftrvl[split:]\n",
    "    dtr = lgb.Dataset(dftr[cols_feat], label=dftr[tgtcol])\n",
    "    dvl = lgb.Dataset(dfvl[cols_feat], label=dfvl[tgtcol])\n",
    "    bst,evlres=train(params,dtr,dvl)\n",
    "    prdtr,prdvl=valid(bst,dftr,dfvl)\n",
    "    \n",
    "    tgt2bst[tgt]=bst\n",
    "    tgt2evlres[tgt]=evlres\n",
    "    tgt2ytr[tgt]=dftr[tgtcol]\n",
    "    tgt2yvl[tgt]=dfvl[tgtcol]\n",
    "    tgt2pops[tgt]=pops\n",
    "    tgt2prdtr[tgt]=prdtr\n",
    "    tgt2prdvl[tgt]=prdvl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 14:32:45 Retweet ********************************************************************************\n",
      "Retweet\n",
      "n_pos_wanted 2000000\n",
      "2020-03-26 14:33:12 chunk 1 n_pos_ttl 112823\n",
      "2020-03-26 14:33:32 chunk 2 n_pos_ttl 226081\n",
      "2020-03-26 14:33:52 chunk 3 n_pos_ttl 338645\n",
      "2020-03-26 14:34:13 chunk 4 n_pos_ttl 452019\n",
      "2020-03-26 14:34:33 chunk 5 n_pos_ttl 565298\n",
      "2020-03-26 14:34:53 chunk 6 n_pos_ttl 678591\n",
      "2020-03-26 14:35:12 chunk 7 n_pos_ttl 791588\n",
      "2020-03-26 14:35:32 chunk 8 n_pos_ttl 904490\n",
      "2020-03-26 14:35:53 chunk 9 n_pos_ttl 1017793\n",
      "2020-03-26 14:36:13 chunk 10 n_pos_ttl 1131194\n",
      "2020-03-26 14:36:33 chunk 11 n_pos_ttl 1243970\n",
      "2020-03-26 14:36:53 chunk 12 n_pos_ttl 1356713\n",
      "2020-03-26 14:37:13 chunk 13 n_pos_ttl 1470005\n",
      "2020-03-26 14:37:33 chunk 14 n_pos_ttl 1582955\n",
      "2020-03-26 14:37:53 chunk 15 n_pos_ttl 1695443\n",
      "2020-03-26 14:38:13 chunk 16 n_pos_ttl 1809038\n",
      "2020-03-26 14:38:33 chunk 17 n_pos_ttl 1921414\n",
      "dftrvl.shape: (10000000, 34) dftrvl[tgtcol].mean(): 0.2033516\n",
      "{'train_pop': 18000000, 'target_pop': 2033516, 'sampled_train_pop': 10000000, 'sampled_target_pop': 2033516}\n",
      "{'objective': 'binary', 'metric': 'binary_logloss', 'verbosity': 0, 'boosting_type': 'gbdt'}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\ttraining's binary_logloss: 0.417223\tvalid_1's binary_logloss: 0.420376\n",
      "[2000]\ttraining's binary_logloss: 0.412242\tvalid_1's binary_logloss: 0.417977\n",
      "[3000]\ttraining's binary_logloss: 0.408359\tvalid_1's binary_logloss: 0.416695\n",
      "[4000]\ttraining's binary_logloss: 0.405096\tvalid_1's binary_logloss: 0.415934\n",
      "[5000]\ttraining's binary_logloss: 0.402029\tvalid_1's binary_logloss: 0.415304\n",
      "[6000]\ttraining's binary_logloss: 0.399295\tvalid_1's binary_logloss: 0.414939\n",
      "[7000]\ttraining's binary_logloss: 0.396593\tvalid_1's binary_logloss: 0.414608\n",
      "[8000]\ttraining's binary_logloss: 0.394084\tvalid_1's binary_logloss: 0.414403\n",
      "[9000]\ttraining's binary_logloss: 0.391532\tvalid_1's binary_logloss: 0.414134\n",
      "[10000]\ttraining's binary_logloss: 0.389198\tvalid_1's binary_logloss: 0.413965\n",
      "[11000]\ttraining's binary_logloss: 0.386831\tvalid_1's binary_logloss: 0.413818\n",
      "[12000]\ttraining's binary_logloss: 0.384602\tvalid_1's binary_logloss: 0.413699\n",
      "[13000]\ttraining's binary_logloss: 0.382404\tvalid_1's binary_logloss: 0.413607\n",
      "[14000]\ttraining's binary_logloss: 0.380242\tvalid_1's binary_logloss: 0.413531\n",
      "Early stopping, best iteration is:\n",
      "[13904]\ttraining's binary_logloss: 0.38044\tvalid_1's binary_logloss: 0.413524\n",
      "ran for 109 mins\n",
      "2020-03-26 16:22:42 Reply ********************************************************************************\n",
      "Reply\n",
      "n_pos_wanted 2000000\n",
      "2020-03-26 16:23:17 chunk 1 n_pos_ttl 27350\n",
      "2020-03-26 16:23:40 chunk 2 n_pos_ttl 54969\n",
      "2020-03-26 16:24:07 chunk 3 n_pos_ttl 82293\n",
      "2020-03-26 16:24:29 chunk 4 n_pos_ttl 109752\n",
      "2020-03-26 16:24:52 chunk 5 n_pos_ttl 136944\n",
      "2020-03-26 16:25:14 chunk 6 n_pos_ttl 164386\n",
      "2020-03-26 16:25:36 chunk 7 n_pos_ttl 191985\n",
      "2020-03-26 16:25:58 chunk 8 n_pos_ttl 219508\n",
      "2020-03-26 16:26:21 chunk 9 n_pos_ttl 247153\n",
      "2020-03-26 16:26:43 chunk 10 n_pos_ttl 274660\n",
      "2020-03-26 16:27:07 chunk 11 n_pos_ttl 302205\n",
      "2020-03-26 16:27:29 chunk 12 n_pos_ttl 329498\n",
      "2020-03-26 16:27:54 chunk 13 n_pos_ttl 356902\n",
      "2020-03-26 16:28:17 chunk 14 n_pos_ttl 384130\n",
      "2020-03-26 16:28:41 chunk 15 n_pos_ttl 411416\n",
      "2020-03-26 16:29:03 chunk 16 n_pos_ttl 438846\n",
      "2020-03-26 16:29:27 chunk 17 n_pos_ttl 466316\n",
      "2020-03-26 16:29:55 chunk 18 n_pos_ttl 493405\n",
      "2020-03-26 16:30:19 chunk 19 n_pos_ttl 520852\n",
      "2020-03-26 16:30:43 chunk 20 n_pos_ttl 548377\n",
      "2020-03-26 16:31:07 chunk 21 n_pos_ttl 575841\n",
      "2020-03-26 16:31:30 chunk 22 n_pos_ttl 603221\n",
      "2020-03-26 16:31:58 chunk 23 n_pos_ttl 630595\n",
      "2020-03-26 16:32:21 chunk 24 n_pos_ttl 658164\n",
      "2020-03-26 16:32:45 chunk 25 n_pos_ttl 685611\n",
      "2020-03-26 16:33:08 chunk 26 n_pos_ttl 713086\n",
      "2020-03-26 16:33:32 chunk 27 n_pos_ttl 740510\n",
      "2020-03-26 16:33:55 chunk 28 n_pos_ttl 768071\n",
      "2020-03-26 16:34:19 chunk 29 n_pos_ttl 795679\n",
      "2020-03-26 16:34:44 chunk 30 n_pos_ttl 823095\n",
      "2020-03-26 16:35:08 chunk 31 n_pos_ttl 850505\n",
      "2020-03-26 16:35:35 chunk 32 n_pos_ttl 877956\n",
      "2020-03-26 16:35:59 chunk 33 n_pos_ttl 905217\n",
      "2020-03-26 16:36:22 chunk 34 n_pos_ttl 932631\n",
      "2020-03-26 16:36:45 chunk 35 n_pos_ttl 960110\n",
      "2020-03-26 16:37:09 chunk 36 n_pos_ttl 987480\n",
      "2020-03-26 16:37:33 chunk 37 n_pos_ttl 1015047\n",
      "2020-03-26 16:37:57 chunk 38 n_pos_ttl 1042673\n",
      "2020-03-26 16:38:22 chunk 39 n_pos_ttl 1070191\n",
      "2020-03-26 16:38:46 chunk 40 n_pos_ttl 1097609\n",
      "2020-03-26 16:39:12 chunk 41 n_pos_ttl 1125171\n",
      "2020-03-26 16:39:35 chunk 42 n_pos_ttl 1152525\n",
      "2020-03-26 16:39:58 chunk 43 n_pos_ttl 1180240\n",
      "2020-03-26 16:40:23 chunk 44 n_pos_ttl 1208033\n",
      "2020-03-26 16:40:48 chunk 45 n_pos_ttl 1235508\n",
      "2020-03-26 16:41:12 chunk 46 n_pos_ttl 1262501\n",
      "2020-03-26 16:41:35 chunk 47 n_pos_ttl 1289974\n",
      "2020-03-26 16:42:02 chunk 48 n_pos_ttl 1317550\n",
      "2020-03-26 16:42:26 chunk 49 n_pos_ttl 1344969\n",
      "2020-03-26 16:42:50 chunk 50 n_pos_ttl 1372396\n",
      "2020-03-26 16:43:17 chunk 51 n_pos_ttl 1399978\n",
      "2020-03-26 16:43:42 chunk 52 n_pos_ttl 1427884\n",
      "2020-03-26 16:44:05 chunk 53 n_pos_ttl 1455523\n",
      "2020-03-26 16:44:31 chunk 54 n_pos_ttl 1483028\n",
      "2020-03-26 16:44:55 chunk 55 n_pos_ttl 1510598\n",
      "2020-03-26 16:45:19 chunk 56 n_pos_ttl 1538018\n",
      "2020-03-26 16:45:45 chunk 57 n_pos_ttl 1565240\n",
      "2020-03-26 16:46:09 chunk 58 n_pos_ttl 1592740\n",
      "2020-03-26 16:46:33 chunk 59 n_pos_ttl 1620169\n",
      "2020-03-26 16:46:58 chunk 60 n_pos_ttl 1647662\n",
      "2020-03-26 16:47:21 chunk 61 n_pos_ttl 1675141\n",
      "2020-03-26 16:47:45 chunk 62 n_pos_ttl 1702709\n",
      "2020-03-26 16:48:09 chunk 63 n_pos_ttl 1729957\n",
      "2020-03-26 16:48:34 chunk 64 n_pos_ttl 1757305\n",
      "2020-03-26 16:48:57 chunk 65 n_pos_ttl 1784810\n",
      "2020-03-26 16:49:23 chunk 66 n_pos_ttl 1812057\n",
      "2020-03-26 16:49:49 chunk 67 n_pos_ttl 1839288\n",
      "2020-03-26 16:50:12 chunk 68 n_pos_ttl 1866749\n",
      "2020-03-26 16:50:36 chunk 69 n_pos_ttl 1894160\n",
      "2020-03-26 16:51:00 chunk 70 n_pos_ttl 1921591\n",
      "2020-03-26 16:51:26 chunk 71 n_pos_ttl 1949194\n",
      "2020-03-26 16:51:50 chunk 72 n_pos_ttl 1976672\n",
      "dftrvl.shape: (10000000, 34) dftrvl[tgtcol].mean(): 0.20045\n",
      "{'train_pop': 73000000, 'target_pop': 2004500, 'sampled_train_pop': 10000000, 'sampled_target_pop': 2004500}\n",
      "{'objective': 'binary', 'metric': 'binary_logloss', 'verbosity': 0, 'boosting_type': 'gbdt'}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\ttraining's binary_logloss: 0.403903\tvalid_1's binary_logloss: 0.40637\n",
      "[2000]\ttraining's binary_logloss: 0.399081\tvalid_1's binary_logloss: 0.404212\n",
      "[3000]\ttraining's binary_logloss: 0.395419\tvalid_1's binary_logloss: 0.403199\n",
      "[4000]\ttraining's binary_logloss: 0.39225\tvalid_1's binary_logloss: 0.402581\n",
      "[5000]\ttraining's binary_logloss: 0.389282\tvalid_1's binary_logloss: 0.402121\n",
      "[6000]\ttraining's binary_logloss: 0.386538\tvalid_1's binary_logloss: 0.40183\n",
      "[7000]\ttraining's binary_logloss: 0.383849\tvalid_1's binary_logloss: 0.401585\n",
      "[8000]\ttraining's binary_logloss: 0.381325\tvalid_1's binary_logloss: 0.401383\n",
      "Early stopping, best iteration is:\n",
      "[8369]\ttraining's binary_logloss: 0.380435\tvalid_1's binary_logloss: 0.401343\n",
      "ran for 92 mins\n",
      "2020-03-26 17:55:02 Like ********************************************************************************\n",
      "Like\n",
      "n_pos_wanted 2000000\n",
      "2020-03-26 17:55:31 chunk 1 n_pos_ttl 439225\n",
      "2020-03-26 17:55:53 chunk 2 n_pos_ttl 878765\n",
      "2020-03-26 17:56:13 chunk 3 n_pos_ttl 1320039\n",
      "2020-03-26 17:56:37 chunk 4 n_pos_ttl 1758693\n",
      "2020-03-26 17:57:02 chunk 5 n_pos_ttl 2197741\n",
      "2020-03-26 17:57:25 chunk 6 n_pos_ttl 2636766\n",
      "2020-03-26 17:57:47 chunk 7 n_pos_ttl 3075026\n",
      "2020-03-26 17:58:11 chunk 8 n_pos_ttl 3514243\n",
      "2020-03-26 17:58:37 chunk 9 n_pos_ttl 3954267\n",
      "2020-03-26 17:59:00 chunk 10 n_pos_ttl 4394655\n",
      "dftrvl.shape: (10000000, 34) dftrvl[tgtcol].mean(): 0.4833535\n",
      "{'train_pop': 11000000, 'target_pop': 4833535, 'sampled_train_pop': 10000000, 'sampled_target_pop': 4833535}\n",
      "{'objective': 'binary', 'metric': 'binary_logloss', 'verbosity': 0, 'boosting_type': 'gbdt'}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\ttraining's binary_logloss: 0.554836\tvalid_1's binary_logloss: 0.556716\n",
      "[2000]\ttraining's binary_logloss: 0.547957\tvalid_1's binary_logloss: 0.55218\n",
      "[3000]\ttraining's binary_logloss: 0.543067\tvalid_1's binary_logloss: 0.549598\n",
      "[4000]\ttraining's binary_logloss: 0.539108\tvalid_1's binary_logloss: 0.547955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000]\ttraining's binary_logloss: 0.535521\tvalid_1's binary_logloss: 0.546663\n",
      "[6000]\ttraining's binary_logloss: 0.532333\tvalid_1's binary_logloss: 0.545719\n",
      "[7000]\ttraining's binary_logloss: 0.529375\tvalid_1's binary_logloss: 0.545041\n",
      "[8000]\ttraining's binary_logloss: 0.526382\tvalid_1's binary_logloss: 0.544195\n",
      "[9000]\ttraining's binary_logloss: 0.523655\tvalid_1's binary_logloss: 0.54363\n",
      "[10000]\ttraining's binary_logloss: 0.520954\tvalid_1's binary_logloss: 0.543012\n",
      "[11000]\ttraining's binary_logloss: 0.518419\tvalid_1's binary_logloss: 0.542527\n",
      "[12000]\ttraining's binary_logloss: 0.516147\tvalid_1's binary_logloss: 0.542317\n",
      "[13000]\ttraining's binary_logloss: 0.513807\tvalid_1's binary_logloss: 0.542026\n",
      "Early stopping, best iteration is:\n",
      "[13863]\ttraining's binary_logloss: 0.511857\tvalid_1's binary_logloss: 0.541757\n",
      "ran for 87 mins\n",
      "2020-03-26 19:22:16 RTwCmnt ********************************************************************************\n",
      "RTwCmnt\n",
      "n_pos_wanted 2000000\n",
      "2020-03-26 19:22:43 chunk 1 n_pos_ttl 7677\n",
      "2020-03-26 19:23:02 chunk 2 n_pos_ttl 15441\n",
      "2020-03-26 19:23:21 chunk 3 n_pos_ttl 23094\n",
      "2020-03-26 19:23:40 chunk 4 n_pos_ttl 30958\n",
      "2020-03-26 19:23:59 chunk 5 n_pos_ttl 38699\n",
      "2020-03-26 19:24:18 chunk 6 n_pos_ttl 46608\n",
      "2020-03-26 19:24:37 chunk 7 n_pos_ttl 54324\n",
      "2020-03-26 19:24:57 chunk 8 n_pos_ttl 62055\n",
      "2020-03-26 19:25:16 chunk 9 n_pos_ttl 69848\n",
      "2020-03-26 19:25:35 chunk 10 n_pos_ttl 77756\n",
      "2020-03-26 19:25:54 chunk 11 n_pos_ttl 85591\n",
      "2020-03-26 19:26:13 chunk 12 n_pos_ttl 93417\n",
      "2020-03-26 19:26:33 chunk 13 n_pos_ttl 101203\n",
      "2020-03-26 19:26:53 chunk 14 n_pos_ttl 108942\n",
      "2020-03-26 19:27:12 chunk 15 n_pos_ttl 116728\n",
      "2020-03-26 19:27:32 chunk 16 n_pos_ttl 124577\n",
      "2020-03-26 19:27:51 chunk 17 n_pos_ttl 132475\n",
      "2020-03-26 19:28:11 chunk 18 n_pos_ttl 140285\n",
      "2020-03-26 19:28:30 chunk 19 n_pos_ttl 148045\n",
      "2020-03-26 19:28:50 chunk 20 n_pos_ttl 155820\n",
      "2020-03-26 19:29:09 chunk 21 n_pos_ttl 163547\n",
      "2020-03-26 19:29:29 chunk 22 n_pos_ttl 171280\n",
      "2020-03-26 19:29:48 chunk 23 n_pos_ttl 179032\n",
      "2020-03-26 19:30:08 chunk 24 n_pos_ttl 186813\n",
      "2020-03-26 19:30:28 chunk 25 n_pos_ttl 194561\n",
      "2020-03-26 19:30:47 chunk 26 n_pos_ttl 202288\n",
      "2020-03-26 19:31:07 chunk 27 n_pos_ttl 210083\n",
      "2020-03-26 19:31:30 chunk 28 n_pos_ttl 217867\n",
      "2020-03-26 19:31:54 chunk 29 n_pos_ttl 225616\n",
      "2020-03-26 19:32:15 chunk 30 n_pos_ttl 233423\n",
      "2020-03-26 19:32:35 chunk 31 n_pos_ttl 241234\n",
      "2020-03-26 19:32:57 chunk 32 n_pos_ttl 249165\n",
      "2020-03-26 19:33:19 chunk 33 n_pos_ttl 257025\n",
      "2020-03-26 19:33:39 chunk 34 n_pos_ttl 264687\n",
      "2020-03-26 19:33:59 chunk 35 n_pos_ttl 272561\n",
      "2020-03-26 19:34:20 chunk 36 n_pos_ttl 280540\n",
      "2020-03-26 19:34:40 chunk 37 n_pos_ttl 288202\n",
      "2020-03-26 19:35:00 chunk 38 n_pos_ttl 295967\n",
      "2020-03-26 19:35:20 chunk 39 n_pos_ttl 303842\n",
      "2020-03-26 19:35:40 chunk 40 n_pos_ttl 311445\n",
      "2020-03-26 19:36:02 chunk 41 n_pos_ttl 319215\n",
      "2020-03-26 19:36:22 chunk 42 n_pos_ttl 327034\n",
      "2020-03-26 19:36:42 chunk 43 n_pos_ttl 334897\n",
      "2020-03-26 19:37:02 chunk 44 n_pos_ttl 342594\n",
      "2020-03-26 19:37:23 chunk 45 n_pos_ttl 350499\n",
      "2020-03-26 19:37:42 chunk 46 n_pos_ttl 358305\n",
      "2020-03-26 19:38:03 chunk 47 n_pos_ttl 366029\n",
      "2020-03-26 19:38:23 chunk 48 n_pos_ttl 373979\n",
      "2020-03-26 19:38:42 chunk 49 n_pos_ttl 381676\n",
      "2020-03-26 19:39:03 chunk 50 n_pos_ttl 389490\n",
      "2020-03-26 19:39:23 chunk 51 n_pos_ttl 397345\n",
      "2020-03-26 19:39:43 chunk 52 n_pos_ttl 405052\n",
      "2020-03-26 19:40:03 chunk 53 n_pos_ttl 412979\n",
      "2020-03-26 19:40:23 chunk 54 n_pos_ttl 420850\n",
      "2020-03-26 19:40:43 chunk 55 n_pos_ttl 428918\n",
      "2020-03-26 19:41:03 chunk 56 n_pos_ttl 436646\n",
      "2020-03-26 19:41:23 chunk 57 n_pos_ttl 444340\n",
      "2020-03-26 19:41:44 chunk 58 n_pos_ttl 452171\n",
      "2020-03-26 19:42:05 chunk 59 n_pos_ttl 459895\n",
      "2020-03-26 19:42:26 chunk 60 n_pos_ttl 467776\n",
      "2020-03-26 19:42:46 chunk 61 n_pos_ttl 475644\n",
      "2020-03-26 19:43:08 chunk 62 n_pos_ttl 483558\n",
      "2020-03-26 19:43:28 chunk 63 n_pos_ttl 491239\n",
      "2020-03-26 19:43:48 chunk 64 n_pos_ttl 499054\n",
      "2020-03-26 19:44:08 chunk 65 n_pos_ttl 506839\n",
      "2020-03-26 19:44:28 chunk 66 n_pos_ttl 514633\n",
      "2020-03-26 19:44:48 chunk 67 n_pos_ttl 522402\n",
      "2020-03-26 19:45:08 chunk 68 n_pos_ttl 529969\n",
      "2020-03-26 19:45:29 chunk 69 n_pos_ttl 537838\n",
      "2020-03-26 19:45:49 chunk 70 n_pos_ttl 545568\n",
      "2020-03-26 19:46:08 chunk 71 n_pos_ttl 553233\n",
      "2020-03-26 19:46:28 chunk 72 n_pos_ttl 561021\n",
      "2020-03-26 19:46:49 chunk 73 n_pos_ttl 568804\n",
      "2020-03-26 19:47:16 chunk 74 n_pos_ttl 576668\n",
      "2020-03-26 19:47:45 chunk 75 n_pos_ttl 584525\n",
      "2020-03-26 19:48:14 chunk 76 n_pos_ttl 592291\n",
      "2020-03-26 19:48:41 chunk 77 n_pos_ttl 599913\n",
      "2020-03-26 19:49:07 chunk 78 n_pos_ttl 607652\n",
      "2020-03-26 19:49:33 chunk 79 n_pos_ttl 615477\n",
      "2020-03-26 19:50:00 chunk 80 n_pos_ttl 623250\n",
      "2020-03-26 19:50:25 chunk 81 n_pos_ttl 631088\n",
      "2020-03-26 19:50:51 chunk 82 n_pos_ttl 638903\n",
      "2020-03-26 19:51:17 chunk 83 n_pos_ttl 646757\n",
      "2020-03-26 19:51:44 chunk 84 n_pos_ttl 654551\n",
      "2020-03-26 19:52:10 chunk 85 n_pos_ttl 662301\n",
      "2020-03-26 19:52:36 chunk 86 n_pos_ttl 670285\n",
      "2020-03-26 19:53:01 chunk 87 n_pos_ttl 678061\n",
      "2020-03-26 19:53:27 chunk 88 n_pos_ttl 685779\n",
      "2020-03-26 19:53:54 chunk 89 n_pos_ttl 693568\n",
      "2020-03-26 19:54:20 chunk 90 n_pos_ttl 701500\n",
      "2020-03-26 19:54:46 chunk 91 n_pos_ttl 709347\n",
      "2020-03-26 19:55:12 chunk 92 n_pos_ttl 717097\n",
      "2020-03-26 19:55:38 chunk 93 n_pos_ttl 724854\n",
      "2020-03-26 19:56:05 chunk 94 n_pos_ttl 732585\n",
      "2020-03-26 19:56:32 chunk 95 n_pos_ttl 740416\n",
      "2020-03-26 19:56:59 chunk 96 n_pos_ttl 748044\n",
      "2020-03-26 19:57:26 chunk 97 n_pos_ttl 755787\n",
      "2020-03-26 19:57:53 chunk 98 n_pos_ttl 763607\n",
      "2020-03-26 19:58:19 chunk 99 n_pos_ttl 771449\n",
      "2020-03-26 19:58:45 chunk 100 n_pos_ttl 779307\n",
      "2020-03-26 19:59:12 chunk 101 n_pos_ttl 786987\n",
      "2020-03-26 19:59:38 chunk 102 n_pos_ttl 794889\n",
      "2020-03-26 20:00:05 chunk 103 n_pos_ttl 802667\n",
      "2020-03-26 20:00:33 chunk 104 n_pos_ttl 810472\n",
      "2020-03-26 20:01:02 chunk 105 n_pos_ttl 818135\n",
      "2020-03-26 20:01:28 chunk 106 n_pos_ttl 825937\n",
      "2020-03-26 20:01:55 chunk 107 n_pos_ttl 833590\n",
      "2020-03-26 20:02:21 chunk 108 n_pos_ttl 841291\n",
      "2020-03-26 20:02:47 chunk 109 n_pos_ttl 848864\n",
      "2020-03-26 20:03:12 chunk 110 n_pos_ttl 856781\n",
      "2020-03-26 20:03:39 chunk 111 n_pos_ttl 864583\n",
      "2020-03-26 20:04:04 chunk 112 n_pos_ttl 872272\n",
      "2020-03-26 20:04:28 chunk 113 n_pos_ttl 880054\n",
      "2020-03-26 20:04:52 chunk 114 n_pos_ttl 887829\n",
      "2020-03-26 20:05:16 chunk 115 n_pos_ttl 895682\n",
      "2020-03-26 20:05:41 chunk 116 n_pos_ttl 903505\n",
      "2020-03-26 20:06:05 chunk 117 n_pos_ttl 911349\n",
      "2020-03-26 20:06:29 chunk 118 n_pos_ttl 919254\n",
      "2020-03-26 20:06:53 chunk 119 n_pos_ttl 927181\n",
      "2020-03-26 20:07:18 chunk 120 n_pos_ttl 934888\n",
      "2020-03-26 20:07:42 chunk 121 n_pos_ttl 942748\n",
      "2020-03-26 20:08:08 chunk 122 n_pos_ttl 950413\n",
      "2020-03-26 20:08:34 chunk 123 n_pos_ttl 958148\n",
      "2020-03-26 20:08:58 chunk 124 n_pos_ttl 965800\n",
      "2020-03-26 20:09:24 chunk 125 n_pos_ttl 973489\n",
      "2020-03-26 20:09:49 chunk 126 n_pos_ttl 981038\n",
      "2020-03-26 20:10:13 chunk 127 n_pos_ttl 988869\n",
      "2020-03-26 20:10:38 chunk 128 n_pos_ttl 996774\n",
      "2020-03-26 20:11:02 chunk 129 n_pos_ttl 1004623\n",
      "2020-03-26 20:11:27 chunk 130 n_pos_ttl 1012372\n"
     ]
    }
   ],
   "source": [
    "params_shared = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"verbosity\": 0,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "}\n",
    "tgt2params = {k:params_shared for k in tgts}\n",
    "\n",
    "tgt2bst={}\n",
    "tgt2evlres={}\n",
    "tgt2tuning_history={}\n",
    "tgt2ytr={}\n",
    "tgt2yvl={}\n",
    "tgt2prdtr={}\n",
    "tgt2prdvl={}\n",
    "tgt2pops={}\n",
    "for tgt in tgts:\n",
    "    print(dtnow(), tgt, '*'*80)\n",
    "    tmstart=time.time()\n",
    "    do_tgt(tgt)\n",
    "    print('ran for',int((time.time()-tmstart)/60),'mins')\n",
    "    \n",
    "pickle.dump(tgt2bst, open(f\"{p_out}/tgt2bst.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tr vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tgt in tgt2evlres:\n",
    "    evlres=tgt2evlres[tgt]\n",
    "    plt.plot(evlres['training']['binary_logloss'])\n",
    "    plt.plot(evlres['valid_1']['binary_logloss'])\n",
    "    plt.title(f\"{tgt} logloss {len(evlres['valid_1']['binary_logloss'])} rounds\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feat_importance(tgt):\n",
    "    bst=tgt2bst[tgt]\n",
    "    ax = lgb.plot_importance(bst, height=0.8, max_num_features=20)\n",
    "    ax.grid(False, axis=\"y\")\n",
    "    ax.set_title(f'{tgt} Estimated feature importance')\n",
    "    plt.show()\n",
    "#     feat2importance=bst.get_fscore()\n",
    "#     print(tgt)\n",
    "#     display(pd.DataFrame([feat2importance.keys(), \n",
    "#                           feat2importance.values()]).T.sort_values(1, ascending=False))\n",
    "\n",
    "for tgt in tgt2bst:\n",
    "    show_feat_importance(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt2auc_tr={}\n",
    "tgt2rce_tr={}\n",
    "tgt2auc_vl={}\n",
    "tgt2rce_vl={}\n",
    "for tgt in tgt2bst:\n",
    "    print(tgt)\n",
    "    prdtr_i, prdvl_i = tgt2prdtr[tgt], tgt2prdvl[tgt]\n",
    "    ytr_i, yvl_i = tgt2ytr[tgt], tgt2yvl[tgt]\n",
    "    scr_auc_tr=compute_prauc(prdtr_i, ytr_i)\n",
    "    scr_rce_tr=compute_rce(prdtr_i, ytr_i)\n",
    "    scr_auc_vl=compute_prauc(prdvl_i, yvl_i)\n",
    "    scr_rce_vl=compute_rce(prdvl_i, yvl_i)\n",
    "\n",
    "    tgt2auc_tr[tgt]=scr_auc_tr\n",
    "    tgt2rce_tr[tgt]=scr_rce_tr\n",
    "    tgt2auc_vl[tgt]=scr_auc_vl\n",
    "    tgt2rce_vl[tgt]=scr_rce_vl\n",
    "    \n",
    "    print('tr prauc:', f'{scr_auc_tr:.4f}','tr rce:', f'{scr_rce_tr:.4f}', )\n",
    "    print('vl prauc:', f'{scr_auc_vl:.4f}','vl rce:', f'{scr_rce_vl:.4f}', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsttr=[]\n",
    "lstvl=[]\n",
    "for tgt in ['Retweet','Reply','Like','RTwCmnt',]:\n",
    "    if tgt not in tgt2bst: continue\n",
    "    lsttr+=[(f'PRAUC {tgt}',tgt2auc_tr[tgt]),\n",
    "          (f'RCE {tgt}',tgt2rce_tr[tgt])]\n",
    "    lstvl+=[(f'PRAUC {tgt}',tgt2auc_vl[tgt]),\n",
    "          (f'RCE {tgt}',tgt2rce_vl[tgt])]\n",
    "\n",
    "dfscrtr=pd.DataFrame(lsttr)\n",
    "dfscrtr.columns=['metric','scr']\n",
    "dfscrvl=pd.DataFrame(lstvl)\n",
    "dfscrvl.columns=['metric','scr']\n",
    "dfscr = pd.merge(dfscrtr, dfscrvl, on='metric', suffixes=('tr','vl'))\n",
    "dfscr.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tgt=tgts[1]\n",
    "# tgtcol=tgt2col[tgt]\n",
    "# bst=tgt2bst[tgt]\n",
    "\n",
    "# dvalid=xgb.DMatrix(dfvalid[cols_feat], label=dfvalid[tgtcol], feature_names=cols_feat)\n",
    "\n",
    "# prdvalid = bst.predict(dvalid, ntree_limit=bst.best_ntree_limit)\n",
    "\n",
    "# pops=tgt2pops[tgt]\n",
    "\n",
    "# prdvalid[:10]\n",
    "# # array([0.11734424, 0.09971393, 0.05619054, 0.03059793, 0.07979691,\n",
    "# #        0.01358252, 0.05293725, 0.27954698, 0.05738379, 0.01741553],\n",
    "# #       dtype=float32)\n",
    "\n",
    "\n",
    "# pops\n",
    "# # {'train_pop': 4000000,\n",
    "# #  'target_pop': 109752,\n",
    "# #  'sampled_train_pop': 1000000,\n",
    "# #  'sampled_target_pop': 109752}\n",
    "\n",
    "# prdvalid_calib = calibration(prdvalid, **pops)\n",
    "\n",
    "# prdvalid_calib[:10]\n",
    "# # array([0.02952491, 0.02471944, 0.01344113, 0.00717127, 0.01945818,\n",
    "# #        0.00314114, 0.0126298 , 0.08155248, 0.01373977, 0.00403964],\n",
    "# #       dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt2pops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_post_valid(tgt):\n",
    "    tgtcol=tgt2col[tgt]\n",
    "    bst=tgt2bst[tgt]\n",
    "    pops=tgt2pops[tgt]\n",
    "    prdvalid = bst.predict(dfvalid[cols_feat],num_iteration=bst.best_iteration)\n",
    "    prdvalid_calib = calibration(prdvalid, **pops)\n",
    "    return prdvalid,prdvalid_calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt2yvalid={tgt:dfvalid[tgt2col[tgt]] for tgt in tgts}\n",
    "tgt2prdvalid={}\n",
    "tgt2prdvalid_calib={}\n",
    "for tgt in tgts:\n",
    "    print(dtnow(), tgt)\n",
    "    tgt2prdvalid[tgt],tgt2prdvalid_calib[tgt]=do_post_valid(tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt2auc_valid={}\n",
    "tgt2rce_valid={}\n",
    "tgt2auc_valid_calib={}\n",
    "tgt2rce_valid_calib={}\n",
    "for tgt in tgts:\n",
    "    print(dtnow(), tgt)\n",
    "    prdvalid, prdvalid_calib = tgt2prdvalid[tgt], tgt2prdvalid_calib[tgt]\n",
    "    yvalid = tgt2yvalid[tgt]\n",
    "    scr_auc_valid=compute_prauc(prdvalid, yvalid)\n",
    "    scr_rce_valid=compute_rce(prdvalid, yvalid)\n",
    "    scr_auc_valid_calib=compute_prauc(prdvalid_calib, yvalid)\n",
    "    scr_rce_valid_calib=compute_rce(prdvalid_calib, yvalid)\n",
    "\n",
    "    tgt2auc_valid[tgt]=scr_auc_valid\n",
    "    tgt2rce_valid[tgt]=scr_rce_valid\n",
    "    tgt2auc_valid_calib[tgt]=scr_auc_valid_calib\n",
    "    tgt2rce_valid_calib[tgt]=scr_rce_valid_calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tgt in tgts:\n",
    "    print(tgt)\n",
    "    print('tr          prauc:', f'{tgt2auc_tr[tgt]:.4f}','tr rce:', f'{tgt2rce_tr[tgt]:.4f}', )\n",
    "    print('vl          prauc:', f'{tgt2auc_vl[tgt]:.4f}','tr rce:', f'{tgt2rce_vl[tgt]:.4f}', )\n",
    "    print('valid       prauc:', f'{tgt2auc_valid[tgt]:.4f}','tr rce:', f'{tgt2rce_valid[tgt]:.4f}', )\n",
    "    print('valid_calib prauc:', f'{tgt2auc_valid_calib[tgt]:.4f}','tr rce:', f'{tgt2rce_valid_calib[tgt]:.4f}', )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsttr=[]\n",
    "lstvl=[]\n",
    "lstvalid=[]\n",
    "lstvalid_calib=[]\n",
    "for tgt in ['Retweet','Reply','Like','RTwCmnt',]:\n",
    "    if tgt not in tgt2bst: continue\n",
    "    lsttr+=[(f'PRAUC {tgt}',tgt2auc_tr[tgt]),\n",
    "          (f'RCE {tgt}',tgt2rce_tr[tgt])]\n",
    "    lstvl+=[(f'PRAUC {tgt}',tgt2auc_vl[tgt]),\n",
    "          (f'RCE {tgt}',tgt2rce_vl[tgt])]\n",
    "    lstvalid+=[(f'PRAUC {tgt}',tgt2auc_valid[tgt]),\n",
    "          (f'RCE {tgt}',tgt2rce_valid[tgt])]\n",
    "    lstvalid_calib+=[(f'PRAUC {tgt}',tgt2auc_valid_calib[tgt]),\n",
    "          (f'RCE {tgt}',tgt2rce_valid_calib[tgt])]\n",
    "\n",
    "dfscrtr=pd.DataFrame(lsttr)\n",
    "dfscrtr.columns=['metric','scr']\n",
    "dfscrvl=pd.DataFrame(lstvl)\n",
    "dfscrvl.columns=['metric','scr']\n",
    "dfscrvalid=pd.DataFrame(lstvalid)\n",
    "dfscrvalid.columns=['metric','scr']\n",
    "dfscrvalid_calib=pd.DataFrame(lstvalid_calib)\n",
    "dfscrvalid_calib.columns=['metric','scr']\n",
    "\n",
    "dfscr = reduce(lambda df1,df2: pd.merge(df1,df2,on='metric'), \n",
    "            [dfscrtr,dfscrvl,dfscrvalid,dfscrvalid_calib])\n",
    "\n",
    "dfscr.columns=['scr','tr','vl','valid','valid_calib']\n",
    "dfscr.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls -lhS $p_in | grep val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dftst=pd.read_csv(\n",
    "    f'{p_in}/val_{valtmstmp}.tsv',\n",
    "#     f'{p_in}/val_259A6F6DFD672CB1F883CBEC01B99F2D_1584405047.tsv',\n",
    "    sep='\\x01', header=None, names=cols_val,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dftst = prp_df(dftst, istrn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tgt2prdtst={}\n",
    "for tgt in tgts:\n",
    "    print(dtnow(), tgt)\n",
    "    bst = tgt2bst[tgt]\n",
    "    pops=tgt2pops[tgt]\n",
    "    prdtst = bst.predict(dftst[cols_feat], num_iteration=bst.best_iteration)\n",
    "    prdtst_calib = calibration(prdtst, **pops)\n",
    "    tgt2prdtst[tgt] = prdtst_calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsub_ids = dftst[['twtid','u2id',]]\n",
    "\n",
    "tgt2dfsub = {}\n",
    "for tgt,prdtst in tgt2prdtst.items():\n",
    "    dfsub = dfsub_ids.copy()\n",
    "    dfsub['scr'] = prdtst\n",
    "    tgt2dfsub[tgt]=dfsub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i,tgt in enumerate(['Retweet','Reply','RTwCmnt','Like',]):\n",
    "    dfsub = tgt2dfsub[tgt]\n",
    "    print(dtnow(), tgt)\n",
    "    dfsub.to_csv(f'{p_out}/{i}_{tgt}__{valtmstmp}__{PRFX}.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcss20",
   "language": "python",
   "name": "rcss20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
