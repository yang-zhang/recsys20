{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:8889/notebooks/git/google-quest-challenge/working/QstBertBase0113_1.ipynb\n",
    "\n",
    "no freezing of bert model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRFX='0326_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-03-17 02:32:24', '2020-03-24 17:09:45']\n",
      "['2020-02-06 00:00:00', '2020-02-13 00:00:00']\n"
     ]
    }
   ],
   "source": [
    "trntmstmp=1584412344\n",
    "valtmstmp=1585069785\n",
    "\n",
    "import datetime\n",
    "print([datetime.datetime.fromtimestamp(o).strftime('%Y-%m-%d %H:%M:%S') for o in (trntmstmp, valtmstmp)])\n",
    "\n",
    "grand_total=1.5e8\n",
    "MIN_TM_TRN=1580947200\n",
    "MIN_TM_TST=1581552000\n",
    "print([datetime.datetime.fromtimestamp(o).strftime('%Y-%m-%d %H:%M:%S') for o in (MIN_TM_TRN, MIN_TM_TST)])\n",
    "\n",
    "\n",
    "SEED=101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 27 01:55:23 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   78C    P0   148W / 149W |   8168MiB / 11441MiB |    100%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      6331      C   ...ubuntu/anaconda3/envs/rcss20/bin/python  8157MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import datetime\n",
    "def dtnow(): return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "SEED=101\n",
    "HOME='/data/git/recsys20'\n",
    "p_in=f'{HOME}/input'\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import torch\n",
    "device=torch.device('cpu')\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "pretrained_weights='bert-base-multilingual-cased'\n",
    "bertmodel = BertModel.from_pretrained(pretrained_weights, output_hidden_states=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights, do_lower_case=False)\n",
    "\n",
    "import random\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Retweet': 'retwt',\n",
       " 'Reply': 'reply',\n",
       " 'Like': 'like',\n",
       " 'RTwCmnt': 'retwt_cmmnt'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=[\n",
    "'toks',\n",
    "'hshtgs',\n",
    "'twtid',\n",
    "'media',\n",
    "'links',\n",
    "'domns',\n",
    "'twttyp',\n",
    "'lang',\n",
    "'tm',\n",
    "\n",
    "'u1id',\n",
    "'u1_fllwer_cnt',\n",
    "'u1_fllwng_cnt',\n",
    "'u1_vrfed',\n",
    "'u1_create_tm',\n",
    "\n",
    "'u2id',\n",
    "'u2_fllwer_cnt',\n",
    "'u2_fllwng_cnt',\n",
    "'u2_vrfed',\n",
    "'u2_create_tm',\n",
    "\n",
    "'u1_fllw_u2',\n",
    "'reply_tm',\n",
    "'retwt_tm',\n",
    "'retwt_cmmnt_tm',\n",
    "'like_tm',\n",
    "]\n",
    "cols_cat = ['twttyp','lang']\n",
    "cols_val = cols[:-4]\n",
    "cols_tgt_tmstmp=[\n",
    "    'retwt_tm',\n",
    "    'reply_tm',\n",
    "    'like_tm',\n",
    "    'retwt_cmmnt_tm',\n",
    "]\n",
    "cols_tgt=[o.split('_tm')[0] for o in cols_tgt_tmstmp]\n",
    "tgts             = ['Retweet','Reply','Like','RTwCmnt',]\n",
    "assert cols_tgt == ['retwt',  'reply','like','retwt_cmmnt',]\n",
    "ntgts=len(tgts)\n",
    "\n",
    "\n",
    "tgt2col=dict(zip(tgts,cols_tgt))\n",
    "tgt2col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.83 ms, sys: 812 Âµs, total: 9.64 ms\n",
      "Wall time: 8.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dftrn=pd.read_csv(\n",
    "    f'{p_in}/trn_{trntmstmp}.tsv',\n",
    "    sep='\\x01', header=None, \n",
    "    encoding='utf-8', \n",
    "    names=cols, \n",
    "    nrows=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lens = dftrn.toks.apply(lambda x: len(x.split('\\t')))\n",
    "\n",
    "# lens.mean(), np.percentile(lens, 50), np.percentile(lens, 95), np.percentile(lens, 99)\n",
    "# (46.754583, 41.0, 106.0, 135.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token, tokenizer.pad_token_id\n",
    "# ('[PAD]', 0)\n",
    "\n",
    "# tokenizer.sep_token, tokenizer.sep_token_id\n",
    "# ('[SEP]', 102)\n",
    "maxlen=128\n",
    "def mkids(x):\n",
    "    tokids=list(map(int, x.split('\\t')))\n",
    "    l=len(tokids) \n",
    "    if l<=maxlen: \n",
    "        return tokids + [0]*(maxlen-len(tokids))\n",
    "    else: \n",
    "        return tokids[:maxlen-1]+[102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_tensors(df, istrn=True):\n",
    "    tokids=dftrn.toks.apply(lambda x: mkids(x))\n",
    "    Xarr=np.array(list(tokids))\n",
    "    X=torch.tensor(Xarr,dtype=torch.long)\n",
    "    if not istrn: return X\n",
    "    ys=dftrn[cols_tgt_tmstmp].notna().values\n",
    "    ys=torch.tensor(ys,dtype=torch.float)\n",
    "    return X,ys\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X,ys = mk_tensors(dftrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128]) torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "BS=2\n",
    "ds = TensorDataset(X,ys)\n",
    "dl = DataLoader(ds, batch_size=BS, shuffle=True)\n",
    "for step, batch in enumerate(dl):\n",
    "    X_b,ys_b = (o.to(device) for o in batch)\n",
    "    print(X_b.shape,ys_b.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=2#4\n",
    "WD=0.01\n",
    "LR=1e-5\n",
    "SCHDLR_FUNC = get_cosine_schedule_with_warmup \n",
    "WARMUP_RATE = 0.05\n",
    "N_CYCLS = .5\n",
    "EPS = 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = TensorDataset(X,ys)\n",
    "# dl = DataLoader(ds, batch_size=BS, shuffle=True)\n",
    "# for step, batch in enumerate(dl):\n",
    "#     X_b,ys_b = (o.to(device) for o in batch)\n",
    "#     print(X_b.shape,ys_b.shape)\n",
    "#     break\n",
    "\n",
    "# # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "# bertmodel=bertmodel.eval()\n",
    "# bertmodel.to(device)\n",
    "# with torch.no_grad():\n",
    "#     last_hidden_state, pooler_output, hidden_states = bertmodel(X_b,)\n",
    "#     avg_pool = torch.mean(last_hidden_state,1)\n",
    "#     max_pool,_ = torch.max(last_hidden_state,1)\n",
    "# last_hidden_state.shape,pooler_output.shape, len(hidden_states), avg_pool.shape, max_pool.shape\n",
    "# (torch.Size([2, 128, 768]),\n",
    "#  torch.Size([2, 768]),\n",
    "#  13,\n",
    "#  torch.Size([2, 768]),\n",
    "#  torch.Size([2, 768]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see RobertaClassificationHead in transformers/modeling_roberta.py\n",
    "N_HIDDEN = 768 \n",
    "class TwtModel(nn.Module):\n",
    "    def __init__(self, bertmodel, num_labels=4):\n",
    "        super(TwtModel, self).__init__()\n",
    "        self.bertmodel = bertmodel        \n",
    "        dense_size = 64\n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "        nx = N_HIDDEN*2\n",
    "        self.dense = nn.Linear(nx, nx)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out_proj = nn.Linear(nx, num_labels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_msk = (x!=tokenizer.pad_token_id).float().to(device)\n",
    "        segments = torch.zeros(x.shape, dtype=torch.long).to(device)\n",
    "        bert_output=self.bertmodel(x,attention_mask=attn_msk,token_type_ids=segments)\n",
    "        last_hidden_state,pooler_output,hidden_states = bert_output\n",
    "        avg_pool = torch.mean(last_hidden_state,1)\n",
    "        max_pool,_ = torch.max(last_hidden_state,1)\n",
    "        x = torch.cat([avg_pool,max_pool],1) \n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_vl = np.random.rand(len(X))<0.15\n",
    "tr = np.where(~msk_vl)[0]\n",
    "vl = np.where( msk_vl)[0]\n",
    "Xtr,Xvl = X[tr],X[vl]\n",
    "ys_tr, ys_vl = ys[tr], ys[vl]\n",
    "ds_tr = TensorDataset(Xtr,ys_tr)\n",
    "ds_vl = TensorDataset(Xvl,ys_vl)\n",
    "dl_tr = DataLoader(ds_tr, batch_size=BS,   pin_memory=True, shuffle=True)\n",
    "dl_vl = DataLoader(ds_vl, batch_size=BS, pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwtModel(bertmodel)\n",
    "model = model.to(device)\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': WD},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LR, eps=EPS)\n",
    "t_total = int(EPOCHS*len(dl))\n",
    "scheduler = SCHDLR_FUNC(\n",
    "    optimizer, \n",
    "    num_warmup_steps=int(WARMUP_RATE*t_total), \n",
    "    num_training_steps=t_total,\n",
    "    num_cycles=N_CYCLS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dl):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    for step, dat_b in enumerate(dl):\n",
    "        model.eval()\n",
    "        dat_b = (o.to(device) for o in dat_b)\n",
    "        (*xs_b,y_b)=dat_b\n",
    "        if len(y_b)<2: continue #ignore last batch if size is 1\n",
    "        yb_pred = model(xs_b)\n",
    "        y_true.append(y_b.cpu().detach().numpy())\n",
    "        y_pred.append(yb_pred.cpu().detach().numpy())\n",
    "        loss_ = F.binary_cross_entropy_with_logits(yb_pred,y_b).item()\n",
    "        losses.append(loss_)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    scrs = spearmanrs(y_true, y_pred)\n",
    "    loss = np.mean(losses)\n",
    "    return y_pred, y_true, loss, scrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-4e5522655783>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mepc2scrs_vl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbest_scr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msave_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{p_out}/fld_{fld}.p'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'epoch {epoch} starts'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p_out' is not defined"
     ]
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "set_seed(SEED)\n",
    "\n",
    "epc2loss_tr = []\n",
    "epc2loss_vl = []\n",
    "epc2scrs_tr = []\n",
    "epc2scrs_vl = []\n",
    "best_scr = float('-inf')\n",
    "save_p = f'{p_out}/fld_{fld}.p'\n",
    "for epoch in range(EPOCHS):\n",
    "    print(dtnow(), f'epoch {epoch} starts')\n",
    "    y_tr_epoch = []\n",
    "    y_pred_tr_epoch = []\n",
    "    for step, dat_b in enumerate(dl):\n",
    "        model.train()\n",
    "        x_b,ys_b = (o.to(device) for o in batch)\n",
    "        yb_pred = model(x_b)\n",
    "        y_tr_epoch.append(y_bs.cpu().detach().numpy())\n",
    "        y_pred_tr_epoch.append(yb_pred.cpu().detach().numpy())\n",
    "        loss =  F.binary_cross_entropy_with_logits(yb_pred,ys_b)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        model.zero_grad()\n",
    "\n",
    "    y_pred_tr_epoch = np.concatenate(y_pred_tr_epoch)\n",
    "    y_tr_epoch = np.concatenate(y_tr_epoch)\n",
    "    \n",
    "    break\n",
    "    \n",
    "#     scrs_tr = spearmanrs(y_tr_epoch, y_pred_tr_epoch)\n",
    "#     scr_tr = np.mean(scrs_tr)\n",
    "#     pred_vl, y_true_vl, lss_vl, scrs_vl = evaluate(model, X_vl,Xcat_vl,Xhst_vl,Xfeat_vl,y_vl)\n",
    "#     scr_vl = np.mean(scrs_vl)\n",
    "#     print(f'lss_tr: {lss_tr: .4f}; scr_tr: {scr_tr: .4f}; ')\n",
    "#     print(f'lss_vl: {lss_vl: .4f}; scr_vl: {scr_vl: .4f}; ')\n",
    "#     if scr_vl>best_scr: \n",
    "#         best_scr = scr_vl\n",
    "#         best_pred_vl = pred_vl\n",
    "#         print(f'found better scr, saving model...')\n",
    "#         torch.save(model.state_dict(),save_p)\n",
    "\n",
    "#     epc2loss_tr.append(lss_tr)\n",
    "#     epc2loss_vl.append(lss_vl)\n",
    "#     epc2scrs_tr.append(scrs_tr)\n",
    "#     epc2scrs_vl.append(scrs_vl)\n",
    "\n",
    "# fld2loss_tr.append(epc2loss_tr)\n",
    "# fld2loss_vl.append(epc2loss_vl)\n",
    "# fld2scrs_tr.append(epc2scrs_tr)\n",
    "# fld2scrs_vl.append(epc2scrs_vl)\n",
    "# preds_vl += list(best_pred_vl)\n",
    "# ys_true_vl += list(y_true_vl)\n",
    "\n",
    "# del model\n",
    "# gc.collect()\n",
    "\n",
    "# pickle.dump(vls, open(f'{p_out}/vls.p', 'wb'))\n",
    "# pickle.dump(preds_vl, open(f'{p_out}/preds_vl.p', 'wb'))\n",
    "# pickle.dump(ys_true_vl, open(f'{p_out}/ys_true_vl.p', 'wb'))\n",
    "\n",
    "# results = {    \n",
    "# 'fld2loss_tr': np.array(fld2loss_tr),\n",
    "# 'fld2loss_vl': np.array(fld2loss_vl),\n",
    "# 'fld2scrs_tr': np.array(fld2scrs_tr),\n",
    "# 'fld2scrs_vl': np.array(fld2scrs_vl),\n",
    "# }\n",
    "# return results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(dtnow(), f'epoch {epoch} starts')\n",
    "    y_tr_epoch = []\n",
    "    y_pred_tr_epoch = []\n",
    "    for step, batch in enumerate(dl):\n",
    "        model.train()\n",
    "        x_b,ys_b = (o.to(device) for o in batch)\n",
    "        yb_pred = model(x_b)\n",
    "\n",
    "        loss =  F.binary_cross_entropy_with_logits(yb_pred,ys_b)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        model.zero_grad()\n",
    "        if step%100==0: print(step, loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcss20",
   "language": "python",
   "name": "rcss20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
