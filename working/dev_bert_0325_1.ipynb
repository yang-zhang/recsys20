{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:8889/notebooks/git/google-quest-challenge/working/QstBertBase0113_1.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-03-17 02:32:24', '2020-03-24 17:09:45']\n",
      "['2020-02-06 00:00:00', '2020-02-13 00:00:00']\n"
     ]
    }
   ],
   "source": [
    "trntmstmp=1584412344\n",
    "valtmstmp=1585069785\n",
    "import datetime\n",
    "print([datetime.datetime.fromtimestamp(o).strftime('%Y-%m-%d %H:%M:%S') for o in (trntmstmp, valtmstmp)])\n",
    "\n",
    "grand_total=1.5e8\n",
    "MIN_TM_TRN=1580947200\n",
    "MIN_TM_TST=1581552000\n",
    "print([datetime.datetime.fromtimestamp(o).strftime('%Y-%m-%d %H:%M:%S') for o in (MIN_TM_TRN, MIN_TM_TST)])\n",
    "\n",
    "\n",
    "CHNKSZ=1e6\n",
    "POST_RATE_WANTED=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import datetime\n",
    "def dtnow(): return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "SEED=101\n",
    "HOME='/data/git/recsys20'\n",
    "p_in=f'{HOME}/input'\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import torch\n",
    "device=torch.device('cpu')\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "pretrained_weights='bert-base-multilingual-cased'\n",
    "bertmodel = BertModel.from_pretrained(pretrained_weights, output_hidden_states=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights, do_lower_case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Retweet': 'retwt',\n",
       " 'Reply': 'reply',\n",
       " 'Like': 'like',\n",
       " 'RTwCmnt': 'retwt_cmmnt'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=[\n",
    "'toks',\n",
    "'hshtgs',\n",
    "'twtid',\n",
    "'media',\n",
    "'links',\n",
    "'domns',\n",
    "'twttyp',\n",
    "'lang',\n",
    "'tm',\n",
    "\n",
    "'u1id',\n",
    "'u1_fllwer_cnt',\n",
    "'u1_fllwng_cnt',\n",
    "'u1_vrfed',\n",
    "'u1_create_tm',\n",
    "\n",
    "'u2id',\n",
    "'u2_fllwer_cnt',\n",
    "'u2_fllwng_cnt',\n",
    "'u2_vrfed',\n",
    "'u2_create_tm',\n",
    "\n",
    "'u1_fllw_u2',\n",
    "'reply_tm',\n",
    "'retwt_tm',\n",
    "'retwt_cmmnt_tm',\n",
    "'like_tm',\n",
    "]\n",
    "cols_cat = ['twttyp','lang']\n",
    "cols_val = cols[:-4]\n",
    "cols_tgt_tmstmp=[\n",
    "    'retwt_tm',\n",
    "    'reply_tm',\n",
    "    'like_tm',\n",
    "    'retwt_cmmnt_tm',\n",
    "]\n",
    "cols_tgt=[o.split('_tm')[0] for o in cols_tgt_tmstmp]\n",
    "tgts             = ['Retweet','Reply','Like','RTwCmnt',]\n",
    "assert cols_tgt == ['retwt',  'reply','like','retwt_cmmnt',]\n",
    "ntgts=len(tgts)\n",
    "\n",
    "\n",
    "tgt2col=dict(zip(tgts,cols_tgt))\n",
    "tgt2col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 75.1 ms, sys: 19.9 ms, total: 95 ms\n",
      "Wall time: 94.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dftrn=pd.read_csv(\n",
    "    f'{p_in}/trn_{trntmstmp}.tsv',\n",
    "    sep='\\x01', header=None, \n",
    "    encoding='utf-8', \n",
    "    names=cols, \n",
    "    nrows=1e4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token, tokenizer.pad_token_id\n",
    "# ('[PAD]', 0)\n",
    "\n",
    "# tokenizer.sep_token, tokenizer.sep_token_id\n",
    "# ('[SEP]', 102)\n",
    "maxlen=512\n",
    "def mkids(x):\n",
    "    tokids=list(map(int, x.split('\\t')))\n",
    "    l=len(tokids) \n",
    "    if l<=maxlen: \n",
    "        return tokids + [0]*(maxlen-len(tokids))\n",
    "    else: \n",
    "        return tokids[:maxlen-1]+[102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_tensors(df, istrn=True):\n",
    "    tokids=dftrn.toks.apply(lambda x: mkids(x))\n",
    "    Xarr=np.array(list(tokids))\n",
    "    X=torch.tensor(Xarr,dtype=torch.long)\n",
    "    if not istrn: return X\n",
    "    ys=dftrn[cols_tgt_tmstmp].notna().values\n",
    "    ys=torch.tensor(ys,dtype=torch.float)\n",
    "    return X,ys\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X,ys = mk_tensors(dftrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512]) torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "BS=8\n",
    "ds = TensorDataset(X,ys)\n",
    "dl = DataLoader(ds, batch_size=BS, shuffle=True)\n",
    "for step, batch in enumerate(dl):\n",
    "    X_b,ys_b = (o.to(device) for o in batch)\n",
    "    print(X_b.shape,ys_b.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=1\n",
    "WD=0.01\n",
    "LR=1e-5\n",
    "SCHDLR_FUNC = get_cosine_schedule_with_warmup \n",
    "WARMUP_RATE = 0.05\n",
    "N_CYCLS = .5\n",
    "EPS = 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512]) torch.Size([16, 4])\n"
     ]
    }
   ],
   "source": [
    "BS=16\n",
    "ds = TensorDataset(X,ys)\n",
    "dl = DataLoader(ds, batch_size=BS, shuffle=True)\n",
    "for step, batch in enumerate(dl):\n",
    "    X_b,ys_b = (o.to(device) for o in batch)\n",
    "    print(X_b.shape,ys_b.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 512, 768]),\n",
       " torch.Size([16, 768]),\n",
       " 13,\n",
       " torch.Size([16, 768]),\n",
       " torch.Size([16, 768]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "bertmodel=bertmodel.eval()\n",
    "with torch.no_grad():\n",
    "    last_hidden_state, pooler_output, hidden_states = bertmodel(X_b,)\n",
    "    avg_pool = torch.mean(last_hidden_state,1)\n",
    "    max_pool,_ = torch.max(last_hidden_state,1)\n",
    "last_hidden_state.shape,pooler_output.shape, len(hidden_states), avg_pool.shape, max_pool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see RobertaClassificationHead in transformers/modeling_roberta.py\n",
    "N_HIDDEN = 768 \n",
    "class TwtModel(nn.Module):\n",
    "    def __init__(self, bertmodel, num_labels=4):\n",
    "        super(TwtModel, self).__init__()\n",
    "        self.bertmodel = bertmodel\n",
    "        for param in self.bertmodel.parameters():\n",
    "            param.requires_grad=False\n",
    "        \n",
    "        dense_size = 64\n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "        nx = N_HIDDEN*2\n",
    "        self.dense = nn.Linear(nx, nx)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out_proj = nn.Linear(nx, num_labels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_msk = (x!=tokenizer.pad_token_id).float()\n",
    "        segments = torch.zeros(x.shape, dtype=torch.long)\n",
    "        last_hidden_state, pooler_output, hidden_states = self.bertmodel(x, \n",
    "                                                                         attention_mask=attn_msk,\n",
    "                                         token_type_ids=segments)\n",
    "        avg_pool = torch.mean(last_hidden_state,1)\n",
    "        max_pool,_ = torch.max(last_hidden_state,1)\n",
    "        x = torch.cat([avg_pool,max_pool],1) \n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=TwtModel(bertmodel)\n",
    "model = model.to(device)\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': WD},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LR, eps=EPS)\n",
    "t_total = int(EPOCHS*len(dl))\n",
    "scheduler = SCHDLR_FUNC(\n",
    "    optimizer, \n",
    "    num_warmup_steps=int(WARMUP_RATE*t_total), \n",
    "    num_training_steps=t_total,\n",
    "    num_cycles=N_CYCLS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 03:06:00 epoch 0 starts\n",
      "0.724166750907898\n",
      "0.7611744403839111\n",
      "0.7165363430976868\n",
      "0.7270771861076355\n",
      "0.7215524315834045\n",
      "0.7184446454048157\n",
      "0.7295160889625549\n",
      "0.703510582447052\n",
      "0.7019727230072021\n",
      "0.6832834482192993\n",
      "0.6831183433532715\n",
      "0.6756281852722168\n",
      "0.6698161959648132\n",
      "0.6478674411773682\n",
      "0.664527416229248\n",
      "0.6329403519630432\n",
      "0.6270857453346252\n",
      "0.5989986062049866\n",
      "0.6107409596443176\n",
      "0.6088334321975708\n",
      "0.58661949634552\n",
      "0.577872633934021\n",
      "0.5494076609611511\n",
      "0.5302914977073669\n",
      "0.5388901829719543\n",
      "0.5078228116035461\n",
      "0.4997655153274536\n",
      "0.5121878981590271\n",
      "0.5350878238677979\n",
      "0.4489264488220215\n",
      "0.46939677000045776\n",
      "0.4414101541042328\n",
      "0.4342990219593048\n",
      "0.4353935420513153\n",
      "0.4303920865058899\n",
      "0.38269665837287903\n",
      "0.3928782641887665\n",
      "0.34978458285331726\n",
      "0.37044528126716614\n",
      "0.38987016677856445\n",
      "0.39276182651519775\n",
      "0.34186574816703796\n",
      "0.4189632833003998\n",
      "0.3607929050922394\n",
      "0.3236846327781677\n",
      "0.29945361614227295\n",
      "0.30355846881866455\n",
      "0.33683884143829346\n",
      "0.32145896553993225\n",
      "0.3413238823413849\n",
      "0.2978539764881134\n",
      "0.37600576877593994\n",
      "0.2800609767436981\n",
      "0.3464999198913574\n",
      "0.34463223814964294\n",
      "0.2712650001049042\n",
      "0.3217778503894806\n",
      "0.44884032011032104\n",
      "0.33846041560173035\n",
      "0.3284517228603363\n",
      "0.3064930737018585\n",
      "0.35860323905944824\n",
      "0.3549776077270508\n",
      "0.24135607481002808\n",
      "0.31938183307647705\n",
      "0.3530348837375641\n",
      "0.2566617727279663\n",
      "0.26940178871154785\n",
      "0.31208765506744385\n",
      "0.40950775146484375\n",
      "0.2656025290489197\n",
      "0.3392544984817505\n",
      "0.55324786901474\n",
      "0.43377646803855896\n",
      "0.3458641767501831\n",
      "0.30547618865966797\n",
      "0.29897409677505493\n",
      "0.3734862506389618\n",
      "0.3857952058315277\n",
      "0.3957146108150482\n",
      "0.3905937671661377\n",
      "0.3328113257884979\n",
      "0.38125330209732056\n",
      "0.24480809271335602\n",
      "0.2678593397140503\n",
      "0.2597678005695343\n",
      "0.3220789134502411\n",
      "0.3056071996688843\n",
      "0.3337421715259552\n",
      "0.2589271068572998\n",
      "0.2678138315677643\n",
      "0.2939012944698334\n",
      "0.39353498816490173\n",
      "0.2883693277835846\n",
      "0.259353905916214\n",
      "0.28895068168640137\n",
      "0.24251312017440796\n",
      "0.2863514721393585\n",
      "0.3571084141731262\n",
      "0.3817984163761139\n",
      "0.27246108651161194\n",
      "0.2816948890686035\n",
      "0.3019174039363861\n",
      "0.33443132042884827\n",
      "0.28140777349472046\n",
      "0.3119497299194336\n",
      "0.3743263781070709\n",
      "0.2863257825374603\n",
      "0.3284718990325928\n",
      "0.35354360938072205\n",
      "0.3022627830505371\n",
      "0.2641884386539459\n",
      "0.25898393988609314\n",
      "0.27742257714271545\n",
      "0.32041630148887634\n",
      "0.2892152667045593\n",
      "0.23763316869735718\n",
      "0.2472166270017624\n",
      "0.3398261070251465\n",
      "0.24437947571277618\n",
      "0.36215126514434814\n",
      "0.3566072881221771\n",
      "0.2611343264579773\n",
      "0.33026716113090515\n",
      "0.378680557012558\n",
      "0.24666500091552734\n",
      "0.477760374546051\n",
      "0.3409937620162964\n",
      "0.2702217102050781\n",
      "0.34906309843063354\n",
      "0.3725551664829254\n",
      "0.31107500195503235\n",
      "0.3455403745174408\n",
      "0.3549162745475769\n",
      "0.327688068151474\n",
      "0.36986595392227173\n",
      "0.23612329363822937\n",
      "0.3942452371120453\n",
      "0.24711671471595764\n",
      "0.32610368728637695\n",
      "0.25655224919319153\n",
      "0.3098468780517578\n",
      "0.4271359145641327\n",
      "0.33479082584381104\n",
      "0.3632344901561737\n",
      "0.35529208183288574\n",
      "0.25198617577552795\n",
      "0.2550043761730194\n",
      "0.3892265856266022\n",
      "0.39668217301368713\n",
      "0.30263251066207886\n",
      "0.46162480115890503\n",
      "0.4094449281692505\n",
      "0.3393103778362274\n",
      "0.29223769903182983\n",
      "0.35333654284477234\n",
      "0.26816919445991516\n",
      "0.3109006881713867\n",
      "0.316836416721344\n",
      "0.28906238079071045\n",
      "0.22866913676261902\n",
      "0.27183184027671814\n",
      "0.2890632748603821\n",
      "0.4098582863807678\n",
      "0.3463624119758606\n",
      "0.26165470480918884\n",
      "0.33559951186180115\n",
      "0.2511303424835205\n",
      "0.28260287642478943\n",
      "0.2841576635837555\n",
      "0.22164976596832275\n",
      "0.3269486725330353\n",
      "0.29536116123199463\n",
      "0.3265784978866577\n",
      "0.2552548050880432\n",
      "0.2562546730041504\n",
      "0.40214526653289795\n",
      "0.28302109241485596\n",
      "0.2526358962059021\n",
      "0.4141731262207031\n",
      "0.36634767055511475\n",
      "0.3095996379852295\n",
      "0.27999749779701233\n",
      "0.25112131237983704\n",
      "0.24493446946144104\n",
      "0.3555662930011749\n",
      "0.27774444222450256\n",
      "0.26840129494667053\n",
      "0.3357274532318115\n",
      "0.3206629455089569\n",
      "0.3563550114631653\n",
      "0.2907569408416748\n",
      "0.31573063135147095\n",
      "0.279653936624527\n",
      "0.3311803638935089\n",
      "0.3069882392883301\n",
      "0.20929427444934845\n",
      "0.2481059432029724\n",
      "0.3551809787750244\n",
      "0.20593217015266418\n",
      "0.3139621615409851\n",
      "0.3090476095676422\n",
      "0.3228774666786194\n",
      "0.21201767027378082\n",
      "0.31903791427612305\n",
      "0.2800891101360321\n",
      "0.3337162435054779\n",
      "0.33452337980270386\n",
      "0.2947509288787842\n",
      "0.3033585846424103\n",
      "0.4274837076663971\n",
      "0.3317488133907318\n",
      "0.42978137731552124\n",
      "0.3430554270744324\n",
      "0.526252269744873\n",
      "0.2426239550113678\n",
      "0.25815245509147644\n",
      "0.31890738010406494\n",
      "0.33194631338119507\n",
      "0.3768445551395416\n",
      "0.40714231133461\n",
      "0.27193161845207214\n",
      "0.44326290488243103\n",
      "0.32120224833488464\n",
      "0.334451287984848\n",
      "0.33219629526138306\n",
      "0.3611172139644623\n",
      "0.3213522732257843\n",
      "0.4919740557670593\n",
      "0.2733514606952667\n",
      "0.3277978301048279\n",
      "0.37470537424087524\n",
      "0.31655964255332947\n",
      "0.27929049730300903\n",
      "0.3714837431907654\n",
      "0.26936712861061096\n",
      "0.3136245012283325\n",
      "0.2636934220790863\n",
      "0.2889194190502167\n",
      "0.280909925699234\n",
      "0.2837657332420349\n",
      "0.38996952772140503\n",
      "0.23030909895896912\n",
      "0.3386131823062897\n",
      "0.42030537128448486\n",
      "0.29981303215026855\n",
      "0.3315174877643585\n",
      "0.28972870111465454\n",
      "0.20836304128170013\n",
      "0.3578447699546814\n",
      "0.32897353172302246\n",
      "0.484695166349411\n",
      "0.21617810428142548\n",
      "0.22559912502765656\n",
      "0.3104531168937683\n",
      "0.3559287190437317\n",
      "0.2507951557636261\n",
      "0.2611737847328186\n",
      "0.21773597598075867\n",
      "0.415783166885376\n",
      "0.2637794017791748\n",
      "0.3235681354999542\n",
      "0.2900328040122986\n",
      "0.27259430289268494\n",
      "0.38489481806755066\n",
      "0.3245428502559662\n",
      "0.2669089138507843\n",
      "0.27674487233161926\n",
      "0.29042166471481323\n",
      "0.2577959895133972\n",
      "0.2785155475139618\n",
      "0.24707180261611938\n",
      "0.41254451870918274\n",
      "0.23613578081130981\n",
      "0.21396072208881378\n",
      "0.21759065985679626\n",
      "0.3665676712989807\n",
      "0.2870877683162689\n",
      "0.23971496522426605\n",
      "0.24948230385780334\n",
      "0.21146275103092194\n",
      "0.34652554988861084\n",
      "0.32917818427085876\n",
      "0.24955245852470398\n",
      "0.269162654876709\n",
      "0.30650049448013306\n",
      "0.49248865246772766\n",
      "0.24673831462860107\n",
      "0.24666936695575714\n",
      "0.3487420678138733\n",
      "0.3224751353263855\n",
      "0.3165375292301178\n",
      "0.2547103464603424\n",
      "0.2222350686788559\n",
      "0.2831238806247711\n",
      "0.2798863351345062\n",
      "0.20714716613292694\n",
      "0.23532328009605408\n",
      "0.4338425099849701\n",
      "0.2368914633989334\n",
      "0.40672510862350464\n",
      "0.2083294540643692\n",
      "0.2753160893917084\n",
      "0.2564448416233063\n",
      "0.30794817209243774\n",
      "0.4352797269821167\n",
      "0.22911429405212402\n",
      "0.36321771144866943\n",
      "0.4410879909992218\n",
      "0.4188559353351593\n",
      "0.30171290040016174\n",
      "0.33096039295196533\n",
      "0.23097796738147736\n",
      "0.26061394810676575\n",
      "0.25251007080078125\n",
      "0.2661879062652588\n",
      "0.3061679005622864\n",
      "0.22368958592414856\n",
      "0.27669888734817505\n",
      "0.23544113337993622\n",
      "0.31894800066947937\n",
      "0.2903694212436676\n",
      "0.303597092628479\n",
      "0.26052919030189514\n",
      "0.367767333984375\n",
      "0.30958786606788635\n",
      "0.20818230509757996\n",
      "0.32507216930389404\n",
      "0.27846378087997437\n",
      "0.23412014544010162\n",
      "0.32605260610580444\n",
      "0.5022276639938354\n",
      "0.2871795892715454\n",
      "0.30137258768081665\n",
      "0.2368783950805664\n",
      "0.30603641271591187\n",
      "0.2447585165500641\n",
      "0.22417476773262024\n",
      "0.2777230739593506\n",
      "0.21994923055171967\n",
      "0.2609577178955078\n",
      "0.25092947483062744\n",
      "0.2467624545097351\n",
      "0.2835999131202698\n",
      "0.3238072693347931\n",
      "0.28778207302093506\n",
      "0.21198774874210358\n",
      "0.3265456557273865\n",
      "0.40840616822242737\n",
      "0.24271512031555176\n",
      "0.306908518075943\n",
      "0.39857393503189087\n",
      "0.2917322814464569\n",
      "0.2522059679031372\n",
      "0.4715147614479065\n",
      "0.30564334988594055\n",
      "0.25462979078292847\n",
      "0.20713752508163452\n",
      "0.36790695786476135\n",
      "0.22968840599060059\n",
      "0.49907001852989197\n",
      "0.21507662534713745\n",
      "0.26321008801460266\n",
      "0.2673950493335724\n",
      "0.3480524718761444\n",
      "0.4082699418067932\n",
      "0.26150691509246826\n",
      "0.24402326345443726\n",
      "0.2849983870983124\n",
      "0.2825946509838104\n",
      "0.27999749779701233\n",
      "0.29050910472869873\n",
      "0.2957943081855774\n",
      "0.2861221730709076\n",
      "0.33397623896598816\n",
      "0.24785582721233368\n",
      "0.2509920001029968\n",
      "0.3322896659374237\n",
      "0.3806980848312378\n",
      "0.4030808210372925\n",
      "0.40416112542152405\n",
      "0.2014906108379364\n",
      "0.2974625825881958\n",
      "0.4016835689544678\n",
      "0.3448091745376587\n",
      "0.41154301166534424\n",
      "0.3582688271999359\n",
      "0.3349034786224365\n",
      "0.3998703956604004\n",
      "0.2820986211299896\n",
      "0.21724370121955872\n",
      "0.2736019790172577\n",
      "0.23291173577308655\n",
      "0.3708314001560211\n",
      "0.45222532749176025\n",
      "0.29600536823272705\n",
      "0.45134440064430237\n",
      "0.28482410311698914\n",
      "0.2076667845249176\n",
      "0.31465280055999756\n",
      "0.24360594153404236\n",
      "0.3154435455799103\n",
      "0.29200759530067444\n",
      "0.20591109991073608\n",
      "0.4175901412963867\n",
      "0.2747006118297577\n",
      "0.2539280652999878\n",
      "0.33148616552352905\n",
      "0.259846955537796\n",
      "0.39840635657310486\n",
      "0.31049326062202454\n",
      "0.3039183020591736\n",
      "0.2555226683616638\n",
      "0.2532534897327423\n",
      "0.243136465549469\n",
      "0.29142168164253235\n",
      "0.2596096694469452\n",
      "0.36695995926856995\n",
      "0.2025158405303955\n",
      "0.29869014024734497\n",
      "0.2547156810760498\n",
      "0.3300711512565613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35026487708091736\n",
      "0.3562433123588562\n",
      "0.3695516884326935\n",
      "0.23794475197792053\n",
      "0.2594481110572815\n",
      "0.2334095686674118\n",
      "0.25155967473983765\n",
      "0.32329070568084717\n",
      "0.29399463534355164\n",
      "0.3138405978679657\n",
      "0.21448245644569397\n",
      "0.25654351711273193\n",
      "0.30383944511413574\n",
      "0.2033945471048355\n",
      "0.37577003240585327\n",
      "0.2122085839509964\n",
      "0.2366200089454651\n",
      "0.29897525906562805\n",
      "0.23352020978927612\n",
      "0.25522392988204956\n",
      "0.33408334851264954\n",
      "0.3212050795555115\n",
      "0.29248306155204773\n",
      "0.4375634789466858\n",
      "0.2571515142917633\n",
      "0.3343764543533325\n",
      "0.3281973898410797\n",
      "0.24249199032783508\n",
      "0.3124220669269562\n",
      "0.3425847291946411\n",
      "0.3493194878101349\n",
      "0.3449494540691376\n",
      "0.4042837619781494\n",
      "0.2834582030773163\n",
      "0.4252440333366394\n",
      "0.38038337230682373\n",
      "0.3264071047306061\n",
      "0.24662239849567413\n",
      "0.3277108073234558\n",
      "0.33027446269989014\n",
      "0.2710770070552826\n",
      "0.355884313583374\n",
      "0.29588741064071655\n",
      "0.3233395218849182\n",
      "0.24451042711734772\n",
      "0.3657400608062744\n",
      "0.2413473278284073\n",
      "0.2256113737821579\n",
      "0.30429700016975403\n",
      "0.36991217732429504\n",
      "0.2797745168209076\n",
      "0.2542039155960083\n",
      "0.31444576382637024\n",
      "0.3442075550556183\n",
      "0.2567453384399414\n",
      "0.24122878909111023\n",
      "0.268899530172348\n",
      "0.37278783321380615\n",
      "0.38121768832206726\n",
      "0.3093489706516266\n",
      "0.27020788192749023\n",
      "0.2006688416004181\n",
      "0.2418724149465561\n",
      "0.23429541289806366\n",
      "0.21669523417949677\n",
      "0.30082568526268005\n",
      "0.22044581174850464\n",
      "0.2789464592933655\n",
      "0.3525981307029724\n",
      "0.321881502866745\n",
      "0.21816526353359222\n",
      "0.23940688371658325\n",
      "0.21955382823944092\n",
      "0.3185573220252991\n",
      "0.3676804304122925\n",
      "0.2692580819129944\n",
      "0.2941022515296936\n",
      "0.30756399035453796\n",
      "0.2187390774488449\n",
      "0.3037357032299042\n",
      "0.34143251180648804\n",
      "0.3478623330593109\n",
      "0.3114853799343109\n",
      "0.3247957229614258\n",
      "0.3742419183254242\n",
      "0.2522364556789398\n",
      "0.32857680320739746\n",
      "0.2782544493675232\n",
      "0.21968232095241547\n",
      "0.3130510747432709\n",
      "0.24175436794757843\n",
      "0.42974576354026794\n",
      "0.2745259702205658\n",
      "0.27884507179260254\n",
      "0.310882568359375\n",
      "0.3386296033859253\n",
      "0.30511584877967834\n",
      "0.2865975499153137\n",
      "0.29509520530700684\n",
      "0.20546017587184906\n",
      "0.3456714451313019\n",
      "0.24774032831192017\n",
      "0.24711968004703522\n",
      "0.261382520198822\n",
      "0.31667041778564453\n",
      "0.29606080055236816\n",
      "0.47927960753440857\n",
      "0.38223281502723694\n",
      "0.3130769431591034\n",
      "0.23686940968036652\n",
      "0.2703898847103119\n",
      "0.37751075625419617\n",
      "0.342487096786499\n",
      "0.21912555396556854\n",
      "0.34018105268478394\n",
      "0.22559954226016998\n",
      "0.3235199451446533\n",
      "0.3399827182292938\n",
      "0.36114856600761414\n",
      "0.2490643560886383\n",
      "0.3165735602378845\n",
      "0.30954575538635254\n",
      "0.2532009184360504\n",
      "0.2552585303783417\n",
      "0.26373809576034546\n",
      "0.249217689037323\n",
      "0.3731808066368103\n",
      "0.25018760561943054\n",
      "0.2940366566181183\n",
      "0.20740686357021332\n",
      "0.35357457399368286\n",
      "0.26345619559288025\n",
      "0.2459234893321991\n",
      "0.24403244256973267\n",
      "0.38510867953300476\n",
      "0.3264657258987427\n",
      "0.28646817803382874\n",
      "0.2867724299430847\n",
      "0.34336331486701965\n",
      "0.24653805792331696\n",
      "0.36914294958114624\n",
      "0.31283628940582275\n",
      "0.2947322726249695\n",
      "0.3536127507686615\n",
      "0.23063409328460693\n",
      "0.37870585918426514\n",
      "0.2528977692127228\n",
      "0.42193469405174255\n",
      "0.2795402705669403\n",
      "0.23927365243434906\n",
      "0.3262660503387451\n",
      "0.22722817957401276\n",
      "0.33010557293891907\n",
      "0.20103082060813904\n",
      "0.3285393714904785\n",
      "0.23138268291950226\n",
      "0.2886813282966614\n",
      "0.4316287040710449\n",
      "0.3111651837825775\n",
      "0.32843780517578125\n",
      "0.4016694724559784\n",
      "0.27770373225212097\n",
      "0.25748440623283386\n",
      "0.239314466714859\n",
      "0.3308734893798828\n",
      "0.3116627335548401\n",
      "0.5123583674430847\n",
      "0.2593730390071869\n",
      "0.21067120134830475\n",
      "0.4550415873527527\n",
      "0.3783509135246277\n",
      "0.47569119930267334\n",
      "0.23531624674797058\n",
      "0.359678715467453\n",
      "0.3435254693031311\n",
      "0.2184790074825287\n",
      "0.31152546405792236\n",
      "0.19930362701416016\n",
      "0.3672111928462982\n",
      "0.23681019246578217\n",
      "0.38537728786468506\n",
      "0.2578093409538269\n",
      "0.2490241527557373\n",
      "0.38915199041366577\n",
      "0.29777154326438904\n",
      "0.21283167600631714\n",
      "0.2773797810077667\n",
      "0.30700522661209106\n",
      "0.24961404502391815\n",
      "0.21162690222263336\n",
      "0.34871381521224976\n",
      "0.2841046452522278\n",
      "0.32590875029563904\n",
      "0.42020976543426514\n",
      "0.24427837133407593\n",
      "0.32790103554725647\n",
      "0.3559023439884186\n",
      "0.2358471155166626\n",
      "0.23804393410682678\n",
      "0.39496147632598877\n",
      "0.2523583769798279\n",
      "0.33031976222991943\n",
      "0.2796463072299957\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(dtnow(), f'epoch {epoch} starts')\n",
    "    y_tr_epoch = []\n",
    "    y_pred_tr_epoch = []\n",
    "    for step, batch in enumerate(dl):\n",
    "        model.train()\n",
    "        x_b,ys_b = (o.to(device) for o in batch)\n",
    "        yb_pred = model(x_b)\n",
    "\n",
    "        loss =  F.binary_cross_entropy_with_logits(yb_pred,ys_b)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        model.zero_grad()\n",
    "        print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcss20",
   "language": "python",
   "name": "rcss20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
