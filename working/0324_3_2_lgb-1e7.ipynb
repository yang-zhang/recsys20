{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://medium.com/optuna/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258\n",
    "- https://github.com/optuna/optuna/blob/master/examples/lightgbm_tuner_simple.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2020-03-17 02:32:24', '2020-03-24 17:09:45']\n",
      "['2020-02-06 00:00:00', '2020-02-13 00:00:00']\n"
     ]
    }
   ],
   "source": [
    "PRFX='0324_3_2'\n",
    "trntmstmp=1584412344\n",
    "valtmstmp=1585069785\n",
    "import datetime\n",
    "print([datetime.datetime.fromtimestamp(o).strftime('%Y-%m-%d %H:%M:%S') for o in (trntmstmp, valtmstmp)])\n",
    "\n",
    "grand_total=1.5e8\n",
    "MIN_TM_TRN=1580947200\n",
    "MIN_TM_TST=1581552000\n",
    "print([datetime.datetime.fromtimestamp(o).strftime('%Y-%m-%d %H:%M:%S') for o in (MIN_TM_TRN, MIN_TM_TST)])\n",
    "\n",
    "\n",
    "CHNKSZ=1e7\n",
    "POST_RATE_WANTED=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Retweet': 'retwt',\n",
       " 'Reply': 'reply',\n",
       " 'Like': 'like',\n",
       " 'RTwCmnt': 'retwt_cmmnt'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc, precision_recall_curve\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "from functools import reduce\n",
    "import datetime\n",
    "def dtnow(): return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "SEED=101\n",
    "HOME='/data/git/recsys20'\n",
    "p_in=f'{HOME}/input'\n",
    "p_out=f'{HOME}/output/{PRFX}'\n",
    "Path(p_out).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc, log_loss\n",
    "\n",
    "def compute_prauc(pred, gt):\n",
    "    prec, recall, thresh = precision_recall_curve(gt, pred)\n",
    "    prauc = auc(recall, prec)\n",
    "    return prauc\n",
    "\n",
    "def calculate_ctr(gt):\n",
    "    positive = len([x for x in gt if x == 1])\n",
    "    ctr = positive/float(len(gt))\n",
    "    return ctr\n",
    "\n",
    "def compute_rce(pred, gt):\n",
    "    cross_entropy = log_loss(gt, pred)\n",
    "    data_ctr = calculate_ctr(gt)\n",
    "    strawman_cross_entropy = log_loss(gt, [data_ctr for _ in range(len(gt))])\n",
    "    return (1.0 - cross_entropy/strawman_cross_entropy)*100.0\n",
    "\n",
    "# https://towardsdatascience.com/how-to-calibrate-undersampled-model-scores-8f3319c1ea5b\n",
    "# How to use the function?\n",
    "# Letâ€™s say your goal is to generate a model that shows the credit default probabilities and your original \n",
    "# training data has 50,000 rows with only 500 of them labeled as target class. When you sample your non-target \n",
    "# instances randomly and reduce the total row count to 10,000, while conserving 500 target rows, our calibration\n",
    "# function becomes:\n",
    "# calibration(model_results, 50000, 500, 10000, 500)\n",
    "# Here model_results is your model probability output array. After you train your model and put the results in it, your function is ready to use. \n",
    "\n",
    "def calibration(data, train_pop, target_pop, sampled_train_pop, sampled_target_pop):\n",
    "    calibrated_data = \\\n",
    "    ((data * (target_pop / train_pop) / (sampled_target_pop / sampled_train_pop)) /\n",
    "    ((\n",
    "        (1 - data) * (1 - target_pop / train_pop) / (1 - sampled_target_pop / sampled_train_pop)\n",
    "     ) +\n",
    "     (\n",
    "        data * (target_pop / train_pop) / (sampled_target_pop / sampled_train_pop)\n",
    "     )))\n",
    "\n",
    "    return calibrated_data\n",
    "\n",
    "cols=[\n",
    "'toks',\n",
    "'hshtgs',\n",
    "'twtid',\n",
    "'media',\n",
    "'links',\n",
    "'domns',\n",
    "'twttyp',\n",
    "'lang',\n",
    "'tm',\n",
    "\n",
    "'u1id',\n",
    "'u1_fllwer_cnt',\n",
    "'u1_fllwng_cnt',\n",
    "'u1_vrfed',\n",
    "'u1_create_tm',\n",
    "\n",
    "'u2id',\n",
    "'u2_fllwer_cnt',\n",
    "'u2_fllwng_cnt',\n",
    "'u2_vrfed',\n",
    "'u2_create_tm',\n",
    "\n",
    "'u1_fllw_u2',\n",
    "'reply_tm',\n",
    "'retwt_tm',\n",
    "'retwt_cmmnt_tm',\n",
    "'like_tm',\n",
    "]\n",
    "cols_cat = ['twttyp','lang']\n",
    "cols_val = cols[:-4]\n",
    "cols_tgt_tmstmp=[\n",
    "    'retwt_tm',\n",
    "    'reply_tm',\n",
    "    'like_tm',\n",
    "    'retwt_cmmnt_tm',\n",
    "]\n",
    "cols_tgt=[o.split('_tm')[0] for o in cols_tgt_tmstmp]\n",
    "tgts             = ['Retweet','Reply','Like','RTwCmnt',]\n",
    "assert cols_tgt == ['retwt',  'reply','like','retwt_cmmnt',]\n",
    "ntgts=len(tgts)\n",
    "\n",
    "\n",
    "tgt2col=dict(zip(tgts,cols_tgt))\n",
    "tgt2col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chnks_trn = pd.read_csv(f'{p_in}/trn_{trntmstmp}.tsv',sep='\\x01',\n",
    "#                     header=None,names=cols, \n",
    "#                         chunksize=CHNKSZ)\n",
    "# # first chunk as validate data\n",
    "# for ichnk,df in enumerate(chnks_trn):\n",
    "#     df\n",
    "#     break\n",
    "\n",
    "# istrn=True\n",
    "# tm_min = MIN_TM_TRN if istrn else MIN_TM_TST\n",
    "# df['len_toks'] = df.toks.apply(len)\n",
    "# for media in ['Photo', 'Video', 'GIF']:\n",
    "#     df[f'has_media_{media}'] = df.media.fillna('').apply(lambda x: media in x)\n",
    "# for col in ['hshtgs', 'links', 'domns',]:\n",
    "#     df[f'num_{col}'] = df[col].fillna('').apply(lambda x: len(x.split('\\t')) if len(x) else 0)\n",
    "\n",
    "# df['twt_age'] = df.tm - tm_min\n",
    "# df['u1_age']  = df.tm - df.u1_create_tm\n",
    "# df['u2_age']  = df.tm - df.u2_create_tm\n",
    "\n",
    "# tm_dt=pd.to_datetime(df.tm, unit='s')\n",
    "# df['tm_dayofweek']=tm_dt.dt.dayofweek\n",
    "# df['tm_hour']=tm_dt.dt.hour\n",
    "\n",
    "# df['tmdlta_u2u1']  = df.u2_create_tm - df.u1_create_tm\n",
    "\n",
    "# df['u1_fllwer_cnt_by_age'] = df.u1_fllwer_cnt / df.u1_age\n",
    "# df['u1_fllwng_cnt_by_age'] = df.u2_fllwng_cnt / df.u2_age\n",
    "\n",
    "# for col in ['twttyp','lang']:\n",
    "#     df[col]=df[col].astype('category')\n",
    "\n",
    "# if istrn: \n",
    "#     df[cols_tgt]=df[cols_tgt_tmstmp].notna().astype('int8')\n",
    "#     df.drop(inplace=True, columns=['toks', 'hshtgs', 'media', 'links', 'domns',  \n",
    "#                                    'tm', 'u1_create_tm','u2_create_tm', 'u1id', 'u2id', 'twtid', ]+cols_tgt_tmstmp, )\n",
    "# else:\n",
    "#     df.drop(inplace=True, columns=['toks', 'hshtgs', 'media', 'links', 'domns', \n",
    "#                                    'tm', 'u1_create_tm','u2_create_tm', 'u1id', ])   \n",
    "\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prp_df(df, istrn=True):\n",
    "    tm_min = MIN_TM_TRN if istrn else MIN_TM_TST\n",
    "    df['len_toks'] = df.toks.apply(len)\n",
    "    for media in ['Photo', 'Video', 'GIF']:\n",
    "        df[f'has_media_{media}'] = df.media.fillna('').apply(lambda x: media in x)\n",
    "    for col in ['hshtgs', 'links', 'domns',]:\n",
    "        df[f'num_{col}'] = df[col].fillna('').apply(lambda x: len(x.split('\\t')) if len(x) else 0)\n",
    "\n",
    "    df['twt_age'] = df.tm - tm_min\n",
    "    df['u1_age']  = df.tm - df.u1_create_tm\n",
    "    df['u2_age']  = df.tm - df.u2_create_tm\n",
    "\n",
    "    tm_dt=pd.to_datetime(df.tm, unit='s')\n",
    "    df['tm_dayofweek']=tm_dt.dt.dayofweek\n",
    "    df['tm_hour']=tm_dt.dt.hour\n",
    "\n",
    "    df['tmdlta_u2u1']  = df.u2_create_tm - df.u1_create_tm\n",
    "\n",
    "    df['u1_fllwer_cnt_by_age'] = df.u1_fllwer_cnt / df.u1_age\n",
    "    df['u1_fllwng_cnt_by_age'] = df.u2_fllwng_cnt / df.u2_age\n",
    "\n",
    "    for col in cols_cat:\n",
    "        df[col]=df[col].astype('category')\n",
    "\n",
    "    if istrn: \n",
    "        df[cols_tgt]=df[cols_tgt_tmstmp].notna().astype('int8')\n",
    "        df.drop(inplace=True, columns=['toks', 'hshtgs', 'media', 'links', 'domns',  \n",
    "                                       'tm', 'u1_create_tm','u2_create_tm', 'u1id', 'u2id', 'twtid', ]+cols_tgt_tmstmp, )\n",
    "    else:\n",
    "        df.drop(inplace=True, columns=['toks', 'hshtgs', 'media', 'links', 'domns', \n",
    "                                       'tm', 'u1_create_tm','u2_create_tm', 'u1id', ])   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000000.0, 15.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grand_total, grand_total/CHNKSZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-25 01:14:50 chunk 0\n",
      "dfvalid.shape: (10000000, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "twttyp                  category\n",
       "lang                    category\n",
       "u1_fllwer_cnt              int64\n",
       "u1_fllwng_cnt              int64\n",
       "u1_vrfed                    bool\n",
       "u2_fllwer_cnt              int64\n",
       "u2_fllwng_cnt              int64\n",
       "u2_vrfed                    bool\n",
       "u1_fllw_u2                  bool\n",
       "len_toks                   int64\n",
       "has_media_Photo             bool\n",
       "has_media_Video             bool\n",
       "has_media_GIF               bool\n",
       "num_hshtgs                 int64\n",
       "num_links                  int64\n",
       "num_domns                  int64\n",
       "twt_age                    int64\n",
       "u1_age                     int64\n",
       "u2_age                     int64\n",
       "tm_dayofweek               int64\n",
       "tm_hour                    int64\n",
       "tmdlta_u2u1                int64\n",
       "u1_fllwer_cnt_by_age     float64\n",
       "u1_fllwng_cnt_by_age     float64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chnks_trn = pd.read_csv(f'{p_in}/trn_{trntmstmp}.tsv',sep='\\x01',\n",
    "                    header=None,names=cols, \n",
    "                        chunksize=CHNKSZ)\n",
    "# first chunk as validate data\n",
    "for ichnk,df in enumerate(chnks_trn):\n",
    "    print(dtnow(), 'chunk', ichnk)\n",
    "#     print([datetime.datetime.fromtimestamp(o).strftime('%Y-%m-%d %H:%M:%S') \n",
    "#            for o in (df.tm.min(), df.tm.max())])\n",
    "    dfvalid = prp_df(df)\n",
    "    break\n",
    "print('dfvalid.shape:',dfvalid.shape)\n",
    "\n",
    "cols_feat=[o for o in dfvalid.columns if o not in cols_tgt]\n",
    "\n",
    "display(dfvalid[cols_feat].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trnval data func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdftrvl(tgt):\n",
    "    print(tgt)\n",
    "    tgtcol=tgt2col[tgt]\n",
    "    chnks_trn = pd.read_csv(f'{p_in}/trn_{trntmstmp}.tsv',sep='\\x01',\n",
    "                        header=None,names=cols, \n",
    "                            chunksize=CHNKSZ)\n",
    "    len_df_wanted = int(CHNKSZ)\n",
    "    # retwt          0.113031\n",
    "    # reply          0.027488\n",
    "    # like           0.439499\n",
    "    # retwt_cmmnt    0.007742\n",
    "    pos_rate_wanted = POST_RATE_WANTED\n",
    "    n_pos_wanted = int(len_df_wanted*pos_rate_wanted)\n",
    "    print('n_pos_wanted', n_pos_wanted)\n",
    "    np.random.seed(SEED)\n",
    "    lst_df = []\n",
    "    n_pos_ttl = 0\n",
    "    for ichnk,df in enumerate(chnks_trn):\n",
    "        #skip first chunk (it was validate data)\n",
    "        if ichnk==0: continue\n",
    "        print(dtnow(), 'chunk', ichnk)\n",
    "        df = prp_df(df)\n",
    "        n_pos_ttl+= df[tgtcol].sum()\n",
    "        lst_df.append(df)\n",
    "        if n_pos_ttl>=n_pos_wanted: break\n",
    "\n",
    "    df = pd.concat(lst_df)\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "    # https://stackoverflow.com/questions/28556942/pandas-remove-rows-at-random-without-shuffling-dataset\n",
    "    idx_neg=np.where(df[tgtcol]==0)[0]\n",
    "    n_neg = len(idx_neg)\n",
    "    n_pos = len(df)-len(idx_neg)\n",
    "    n_neg2keep = len_df_wanted-n_pos\n",
    "    n_neg2rmv = n_neg-n_neg2keep\n",
    "    idx_neg2rmv = np.random.choice(idx_neg, n_neg2rmv, replace=False)\n",
    "    dftrvl = df.drop(idx_neg2rmv)\n",
    "    dftrvl = dftrvl.sample(len(dftrvl))\n",
    "    for col in cols_cat:\n",
    "        dftrvl[col]=dftrvl[col].astype('category')\n",
    "    \n",
    "#     display(dftrvl.dtypes)\n",
    "    print('dftrvl.shape:',dftrvl.shape,'dftrvl[tgtcol].mean():',dftrvl[tgtcol].mean())\n",
    "    \n",
    "    pops={\n",
    "        'train_pop':len(df),\n",
    "        'target_pop':n_pos,\n",
    "        'sampled_train_pop':len_df_wanted,\n",
    "        'sampled_target_pop':n_pos,\n",
    "    }\n",
    "    print(pops)\n",
    "    return dftrvl, pops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params,dtr,dvl):\n",
    "    print(params)\n",
    "    evals_result = {}\n",
    "    evallist = [(dtr, 'train'), (dvl, 'eval')]\n",
    "    bst = lgb.train(params=params, \n",
    "                    train_set=dtr, \n",
    "                    num_boost_round=5000,\n",
    "                    valid_sets=[dtr, dvl],\n",
    "                    verbose_eval=100,\n",
    "                    early_stopping_rounds=100,\n",
    "                    evals_result=evals_result,\n",
    "                   )\n",
    "    return bst,evals_result\n",
    "\n",
    "def valid(bst,dftr,dfvl):\n",
    "    prdtr = bst.predict(dftr[cols_feat],num_iteration=bst.best_iteration)\n",
    "    prdvl = bst.predict(dfvl[cols_feat],num_iteration=bst.best_iteration)\n",
    "    return prdtr,prdvl\n",
    "\n",
    "def do_tgt(tgt):\n",
    "    params=tgt2params[tgt]\n",
    "    tgtcol=tgt2col[tgt]\n",
    "    dftrvl, pops=getdftrvl(tgt)\n",
    "    split=int(len(dftrvl)*0.85)\n",
    "    dftr,dfvl=dftrvl[:split],dftrvl[split:]\n",
    "    dtr = lgb.Dataset(dftr[cols_feat], label=dftr[tgtcol])\n",
    "    dvl = lgb.Dataset(dfvl[cols_feat], label=dfvl[tgtcol])\n",
    "    bst,evlres=train(params,dtr,dvl)\n",
    "    prdtr,prdvl=valid(bst,dftr,dfvl)\n",
    "    \n",
    "    tgt2bst[tgt]=bst\n",
    "    tgt2evlres[tgt]=evlres\n",
    "    tgt2ytr[tgt]=dftr[tgtcol]\n",
    "    tgt2yvl[tgt]=dfvl[tgtcol]\n",
    "    tgt2pops[tgt]=pops\n",
    "    tgt2prdtr[tgt]=prdtr\n",
    "    tgt2prdvl[tgt]=prdvl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-25 01:15:51 Retweet ********************************************************************************\n",
      "Retweet\n",
      "n_pos_wanted 1000000\n",
      "2020-03-25 01:18:37 chunk 1\n",
      "dftrvl.shape: (10000000, 28) dftrvl[tgtcol].mean(): 0.1129292\n",
      "{'train_pop': 10000000, 'target_pop': 1129292, 'sampled_train_pop': 10000000, 'sampled_target_pop': 1129292}\n",
      "{'objective': 'binary', 'metric': 'binary_logloss', 'verbosity': 0, 'boosting_type': 'gbdt'}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.309108\tvalid_1's binary_logloss: 0.309489\n",
      "[200]\ttraining's binary_logloss: 0.30676\tvalid_1's binary_logloss: 0.307427\n",
      "[300]\ttraining's binary_logloss: 0.30545\tvalid_1's binary_logloss: 0.306417\n",
      "[400]\ttraining's binary_logloss: 0.304441\tvalid_1's binary_logloss: 0.30571\n",
      "[500]\ttraining's binary_logloss: 0.303537\tvalid_1's binary_logloss: 0.305106\n",
      "[600]\ttraining's binary_logloss: 0.302745\tvalid_1's binary_logloss: 0.3046\n",
      "[700]\ttraining's binary_logloss: 0.302107\tvalid_1's binary_logloss: 0.30426\n",
      "[800]\ttraining's binary_logloss: 0.301543\tvalid_1's binary_logloss: 0.30398\n",
      "[900]\ttraining's binary_logloss: 0.300938\tvalid_1's binary_logloss: 0.303649\n",
      "[1000]\ttraining's binary_logloss: 0.300327\tvalid_1's binary_logloss: 0.303316\n",
      "[1100]\ttraining's binary_logloss: 0.299819\tvalid_1's binary_logloss: 0.303098\n",
      "[1200]\ttraining's binary_logloss: 0.299358\tvalid_1's binary_logloss: 0.302931\n",
      "[1300]\ttraining's binary_logloss: 0.298895\tvalid_1's binary_logloss: 0.30275\n",
      "[1400]\ttraining's binary_logloss: 0.298474\tvalid_1's binary_logloss: 0.302613\n",
      "[1500]\ttraining's binary_logloss: 0.298102\tvalid_1's binary_logloss: 0.302524\n",
      "[1600]\ttraining's binary_logloss: 0.297707\tvalid_1's binary_logloss: 0.302421\n",
      "[1700]\ttraining's binary_logloss: 0.297273\tvalid_1's binary_logloss: 0.302272\n",
      "[1800]\ttraining's binary_logloss: 0.296867\tvalid_1's binary_logloss: 0.302136\n",
      "[1900]\ttraining's binary_logloss: 0.296534\tvalid_1's binary_logloss: 0.302076\n",
      "[2000]\ttraining's binary_logloss: 0.296157\tvalid_1's binary_logloss: 0.301982\n",
      "[2100]\ttraining's binary_logloss: 0.295783\tvalid_1's binary_logloss: 0.301899\n",
      "[2200]\ttraining's binary_logloss: 0.295448\tvalid_1's binary_logloss: 0.30184\n",
      "[2300]\ttraining's binary_logloss: 0.295107\tvalid_1's binary_logloss: 0.301766\n",
      "[2400]\ttraining's binary_logloss: 0.294763\tvalid_1's binary_logloss: 0.301694\n",
      "[2500]\ttraining's binary_logloss: 0.294404\tvalid_1's binary_logloss: 0.301624\n",
      "[2600]\ttraining's binary_logloss: 0.294065\tvalid_1's binary_logloss: 0.301572\n",
      "[2700]\ttraining's binary_logloss: 0.293758\tvalid_1's binary_logloss: 0.301539\n",
      "[2800]\ttraining's binary_logloss: 0.293436\tvalid_1's binary_logloss: 0.301488\n",
      "[2900]\ttraining's binary_logloss: 0.293141\tvalid_1's binary_logloss: 0.301452\n",
      "[3000]\ttraining's binary_logloss: 0.292834\tvalid_1's binary_logloss: 0.301418\n",
      "[3100]\ttraining's binary_logloss: 0.292534\tvalid_1's binary_logloss: 0.301379\n",
      "[3200]\ttraining's binary_logloss: 0.292231\tvalid_1's binary_logloss: 0.301339\n",
      "[3300]\ttraining's binary_logloss: 0.291926\tvalid_1's binary_logloss: 0.301305\n",
      "[3400]\ttraining's binary_logloss: 0.291612\tvalid_1's binary_logloss: 0.301249\n",
      "[3500]\ttraining's binary_logloss: 0.291312\tvalid_1's binary_logloss: 0.301217\n",
      "[3600]\ttraining's binary_logloss: 0.291012\tvalid_1's binary_logloss: 0.301185\n",
      "[3700]\ttraining's binary_logloss: 0.290729\tvalid_1's binary_logloss: 0.301156\n",
      "[3800]\ttraining's binary_logloss: 0.290422\tvalid_1's binary_logloss: 0.301126\n",
      "[3900]\ttraining's binary_logloss: 0.29011\tvalid_1's binary_logloss: 0.301085\n",
      "[4000]\ttraining's binary_logloss: 0.289815\tvalid_1's binary_logloss: 0.301045\n",
      "[4100]\ttraining's binary_logloss: 0.289525\tvalid_1's binary_logloss: 0.30101\n",
      "[4200]\ttraining's binary_logloss: 0.289221\tvalid_1's binary_logloss: 0.300973\n",
      "[4300]\ttraining's binary_logloss: 0.288944\tvalid_1's binary_logloss: 0.300947\n",
      "[4400]\ttraining's binary_logloss: 0.288668\tvalid_1's binary_logloss: 0.300905\n",
      "[4500]\ttraining's binary_logloss: 0.288393\tvalid_1's binary_logloss: 0.300886\n",
      "[4600]\ttraining's binary_logloss: 0.288097\tvalid_1's binary_logloss: 0.300846\n",
      "[4700]\ttraining's binary_logloss: 0.287829\tvalid_1's binary_logloss: 0.300834\n",
      "[4800]\ttraining's binary_logloss: 0.28756\tvalid_1's binary_logloss: 0.300809\n",
      "[4900]\ttraining's binary_logloss: 0.287304\tvalid_1's binary_logloss: 0.300798\n"
     ]
    }
   ],
   "source": [
    "params_shared = {\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": \"binary_logloss\",\n",
    "    \"verbosity\": 0,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "}\n",
    "tgt2params = {k:params_shared for k in tgts}\n",
    "\n",
    "tgt2bst={}\n",
    "tgt2evlres={}\n",
    "tgt2tuning_history={}\n",
    "tgt2ytr={}\n",
    "tgt2yvl={}\n",
    "tgt2prdtr={}\n",
    "tgt2prdvl={}\n",
    "tgt2pops={}\n",
    "for tgt in tgts:\n",
    "    print(dtnow(), tgt, '*'*80)\n",
    "    do_tgt(tgt)\n",
    "    \n",
    "pickle.dump(tgt2bst, open(f\"{p_out}/tgt2bst.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tr vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tgt in tgt2evlres:\n",
    "    evlres=tgt2evlres[tgt]\n",
    "    plt.plot(evlres['training']['binary_logloss'])\n",
    "    plt.plot(evlres['valid_1']['binary_logloss'])\n",
    "    plt.title(f\"{tgt} logloss {len(evlres['valid_1']['binary_logloss'])} rounds\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feat_importance(tgt):\n",
    "    bst=tgt2bst[tgt]\n",
    "    ax = lgb.plot_importance(bst, height=0.8, max_num_features=20)\n",
    "    ax.grid(False, axis=\"y\")\n",
    "    ax.set_title(f'{tgt} Estimated feature importance')\n",
    "    plt.show()\n",
    "#     feat2importance=bst.get_fscore()\n",
    "#     print(tgt)\n",
    "#     display(pd.DataFrame([feat2importance.keys(), \n",
    "#                           feat2importance.values()]).T.sort_values(1, ascending=False))\n",
    "\n",
    "for tgt in tgt2bst:\n",
    "    show_feat_importance(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt2auc_tr={}\n",
    "tgt2rce_tr={}\n",
    "tgt2auc_vl={}\n",
    "tgt2rce_vl={}\n",
    "for tgt in tgt2bst:\n",
    "    print(tgt)\n",
    "    prdtr_i, prdvl_i = tgt2prdtr[tgt], tgt2prdvl[tgt]\n",
    "    ytr_i, yvl_i = tgt2ytr[tgt], tgt2yvl[tgt]\n",
    "    scr_auc_tr=compute_prauc(prdtr_i, ytr_i)\n",
    "    scr_rce_tr=compute_rce(prdtr_i, ytr_i)\n",
    "    scr_auc_vl=compute_prauc(prdvl_i, yvl_i)\n",
    "    scr_rce_vl=compute_rce(prdvl_i, yvl_i)\n",
    "\n",
    "    tgt2auc_tr[tgt]=scr_auc_tr\n",
    "    tgt2rce_tr[tgt]=scr_rce_tr\n",
    "    tgt2auc_vl[tgt]=scr_auc_vl\n",
    "    tgt2rce_vl[tgt]=scr_rce_vl\n",
    "    \n",
    "    print('tr prauc:', f'{scr_auc_tr:.4f}','tr rce:', f'{scr_rce_tr:.4f}', )\n",
    "    print('vl prauc:', f'{scr_auc_vl:.4f}','vl rce:', f'{scr_rce_vl:.4f}', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsttr=[]\n",
    "lstvl=[]\n",
    "for tgt in ['Retweet','Reply','Like','RTwCmnt',]:\n",
    "    if tgt not in tgt2bst: continue\n",
    "    lsttr+=[(f'PRAUC {tgt}',tgt2auc_tr[tgt]),\n",
    "          (f'RCE {tgt}',tgt2rce_tr[tgt])]\n",
    "    lstvl+=[(f'PRAUC {tgt}',tgt2auc_vl[tgt]),\n",
    "          (f'RCE {tgt}',tgt2rce_vl[tgt])]\n",
    "\n",
    "dfscrtr=pd.DataFrame(lsttr)\n",
    "dfscrtr.columns=['metric','scr']\n",
    "dfscrvl=pd.DataFrame(lstvl)\n",
    "dfscrvl.columns=['metric','scr']\n",
    "dfscr = pd.merge(dfscrtr, dfscrvl, on='metric', suffixes=('tr','vl'))\n",
    "dfscr.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tgt=tgts[1]\n",
    "# tgtcol=tgt2col[tgt]\n",
    "# bst=tgt2bst[tgt]\n",
    "\n",
    "# dvalid=xgb.DMatrix(dfvalid[cols_feat], label=dfvalid[tgtcol], feature_names=cols_feat)\n",
    "\n",
    "# prdvalid = bst.predict(dvalid, ntree_limit=bst.best_ntree_limit)\n",
    "\n",
    "# pops=tgt2pops[tgt]\n",
    "\n",
    "# prdvalid[:10]\n",
    "# # array([0.11734424, 0.09971393, 0.05619054, 0.03059793, 0.07979691,\n",
    "# #        0.01358252, 0.05293725, 0.27954698, 0.05738379, 0.01741553],\n",
    "# #       dtype=float32)\n",
    "\n",
    "\n",
    "# pops\n",
    "# # {'train_pop': 4000000,\n",
    "# #  'target_pop': 109752,\n",
    "# #  'sampled_train_pop': 1000000,\n",
    "# #  'sampled_target_pop': 109752}\n",
    "\n",
    "# prdvalid_calib = calibration(prdvalid, **pops)\n",
    "\n",
    "# prdvalid_calib[:10]\n",
    "# # array([0.02952491, 0.02471944, 0.01344113, 0.00717127, 0.01945818,\n",
    "# #        0.00314114, 0.0126298 , 0.08155248, 0.01373977, 0.00403964],\n",
    "# #       dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt2pops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_post_valid(tgt):\n",
    "    tgtcol=tgt2col[tgt]\n",
    "    bst=tgt2bst[tgt]\n",
    "    pops=tgt2pops[tgt]\n",
    "    prdvalid = bst.predict(dfvalid[cols_feat],num_iteration=bst.best_iteration)\n",
    "    prdvalid_calib = calibration(prdvalid, **pops)\n",
    "    return prdvalid,prdvalid_calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt2yvalid={tgt:dfvalid[tgt2col[tgt]] for tgt in tgts}\n",
    "tgt2prdvalid={}\n",
    "tgt2prdvalid_calib={}\n",
    "for tgt in tgts:\n",
    "    print(dtnow(), tgt)\n",
    "    tgt2prdvalid[tgt],tgt2prdvalid_calib[tgt]=do_post_valid(tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt2auc_valid={}\n",
    "tgt2rce_valid={}\n",
    "tgt2auc_valid_calib={}\n",
    "tgt2rce_valid_calib={}\n",
    "for tgt in tgts:\n",
    "    print(dtnow(), tgt)\n",
    "    prdvalid, prdvalid_calib = tgt2prdvalid[tgt], tgt2prdvalid_calib[tgt]\n",
    "    yvalid = tgt2yvalid[tgt]\n",
    "    scr_auc_valid=compute_prauc(prdvalid, yvalid)\n",
    "    scr_rce_valid=compute_rce(prdvalid, yvalid)\n",
    "    scr_auc_valid_calib=compute_prauc(prdvalid_calib, yvalid)\n",
    "    scr_rce_valid_calib=compute_rce(prdvalid_calib, yvalid)\n",
    "\n",
    "    tgt2auc_valid[tgt]=scr_auc_valid\n",
    "    tgt2rce_valid[tgt]=scr_rce_valid\n",
    "    tgt2auc_valid_calib[tgt]=scr_auc_valid_calib\n",
    "    tgt2rce_valid_calib[tgt]=scr_rce_valid_calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tgt in tgts:\n",
    "    print(tgt)\n",
    "    print('tr          prauc:', f'{tgt2auc_tr[tgt]:.4f}','tr rce:', f'{tgt2rce_tr[tgt]:.4f}', )\n",
    "    print('vl          prauc:', f'{tgt2auc_vl[tgt]:.4f}','tr rce:', f'{tgt2rce_vl[tgt]:.4f}', )\n",
    "    print('valid       prauc:', f'{tgt2auc_valid[tgt]:.4f}','tr rce:', f'{tgt2rce_valid[tgt]:.4f}', )\n",
    "    print('valid_calib prauc:', f'{tgt2auc_valid_calib[tgt]:.4f}','tr rce:', f'{tgt2rce_valid_calib[tgt]:.4f}', )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsttr=[]\n",
    "lstvl=[]\n",
    "lstvalid=[]\n",
    "lstvalid_calib=[]\n",
    "for tgt in ['Retweet','Reply','Like','RTwCmnt',]:\n",
    "    if tgt not in tgt2bst: continue\n",
    "    lsttr+=[(f'PRAUC {tgt}',tgt2auc_tr[tgt]),\n",
    "          (f'RCE {tgt}',tgt2rce_tr[tgt])]\n",
    "    lstvl+=[(f'PRAUC {tgt}',tgt2auc_vl[tgt]),\n",
    "          (f'RCE {tgt}',tgt2rce_vl[tgt])]\n",
    "    lstvalid+=[(f'PRAUC {tgt}',tgt2auc_valid[tgt]),\n",
    "          (f'RCE {tgt}',tgt2rce_valid[tgt])]\n",
    "    lstvalid_calib+=[(f'PRAUC {tgt}',tgt2auc_valid_calib[tgt]),\n",
    "          (f'RCE {tgt}',tgt2rce_valid_calib[tgt])]\n",
    "\n",
    "dfscrtr=pd.DataFrame(lsttr)\n",
    "dfscrtr.columns=['metric','scr']\n",
    "dfscrvl=pd.DataFrame(lstvl)\n",
    "dfscrvl.columns=['metric','scr']\n",
    "dfscrvalid=pd.DataFrame(lstvalid)\n",
    "dfscrvalid.columns=['metric','scr']\n",
    "dfscrvalid_calib=pd.DataFrame(lstvalid_calib)\n",
    "dfscrvalid_calib.columns=['metric','scr']\n",
    "\n",
    "dfscr = reduce(lambda df1,df2: pd.merge(df1,df2,on='metric'), \n",
    "            [dfscrtr,dfscrvl,dfscrvalid,dfscrvalid_calib])\n",
    "\n",
    "dfscr.columns=['scr','tr','vl','valid','valid_calib']\n",
    "dfscr.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls -lhS $p_in | grep val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dftst=pd.read_csv(\n",
    "    f'{p_in}/val_{valtmstmp}.tsv',\n",
    "#     f'{p_in}/val_259A6F6DFD672CB1F883CBEC01B99F2D_1584405047.tsv',\n",
    "    sep='\\x01', header=None, names=cols_val,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dftst = prp_df(dftst, istrn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tgt2prdtst={}\n",
    "for tgt in tgts:\n",
    "    print(dtnow(), tgt)\n",
    "    bst = tgt2bst[tgt]\n",
    "    pops=tgt2pops[tgt]\n",
    "    prdtst = bst.predict(dftst[cols_feat], num_iteration=bst.best_iteration)\n",
    "    prdtst_calib = calibration(prdtst, **pops)\n",
    "    tgt2prdtst[tgt] = prdtst_calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsub_ids = dftst[['twtid','u2id',]]\n",
    "\n",
    "tgt2dfsub = {}\n",
    "for tgt,prdtst in tgt2prdtst.items():\n",
    "    dfsub = dfsub_ids.copy()\n",
    "    dfsub['scr'] = prdtst\n",
    "    tgt2dfsub[tgt]=dfsub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i,tgt in enumerate(['Retweet','Reply','RTwCmnt','Like',]):\n",
    "    dfsub = tgt2dfsub[tgt]\n",
    "    print(dtnow(), tgt)\n",
    "    dfsub.to_csv(f'{p_out}/{i}_{tgt}__{valtmstmp}__{PRFX}.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcss20",
   "language": "python",
   "name": "rcss20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
