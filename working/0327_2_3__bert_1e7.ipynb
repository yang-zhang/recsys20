{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRFX='b0327_2_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trntmstmp=1584412344\n",
    "valtmstmp=1585069785\n",
    "\n",
    "import datetime\n",
    "print([datetime.datetime.fromtimestamp(o).strftime('%Y-%m-%d %H:%M:%S') for o in (trntmstmp, valtmstmp)])\n",
    "\n",
    "grand_total=1.5e8\n",
    "MIN_TM_TRN=1580947200\n",
    "MIN_TM_TST=1581552000\n",
    "print([datetime.datetime.fromtimestamp(o).strftime('%Y-%m-%d %H:%M:%S') for o in (MIN_TM_TRN, MIN_TM_TST)])\n",
    "\n",
    "\n",
    "SEED=101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 27 19:44:06 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   41C    P0    38W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import datetime\n",
    "def dtnow(): return datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "SEED=101\n",
    "HOME='/data/git/recsys20'\n",
    "p_in=f'{HOME}/input'\n",
    "p_out = f'{HOME}/output/{PRFX}'\n",
    "from pathlib import Path\n",
    "Path(p_out).mkdir(exist_ok=True)\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "import torch\n",
    "device=torch.device('cuda')\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "pretrained_weights='bert-base-multilingual-cased'\n",
    "bertmodel = BertModel.from_pretrained(pretrained_weights, output_hidden_states=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights, do_lower_case=False)\n",
    "\n",
    "import random\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Retweet': 'retwt',\n",
       " 'Reply': 'reply',\n",
       " 'Like': 'like',\n",
       " 'RTwCmnt': 'retwt_cmmnt'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols=[\n",
    "'toks','hshtgs','twtid','media','links','domns','twttyp','lang','tm',\n",
    "'u1id','u1_fllwer_cnt','u1_fllwng_cnt','u1_vrfed','u1_create_tm',\n",
    "'u2id','u2_fllwer_cnt','u2_fllwng_cnt','u2_vrfed','u2_create_tm',\n",
    "'u1_fllw_u2','reply_tm','retwt_tm','retwt_cmmnt_tm','like_tm',]\n",
    "cols_cat = ['twttyp','lang']\n",
    "cols_val = cols[:-4]\n",
    "cols_tgt_tmstmp=[\n",
    "    'retwt_tm',\n",
    "    'reply_tm',\n",
    "    'like_tm',\n",
    "    'retwt_cmmnt_tm',\n",
    "]\n",
    "cols_tgt=[o.split('_tm')[0] for o in cols_tgt_tmstmp]\n",
    "tgts             = ['Retweet','Reply','Like','RTwCmnt',]\n",
    "assert cols_tgt == ['retwt',  'reply','like','retwt_cmmnt',]\n",
    "ntgts=len(tgts)\n",
    "\n",
    "\n",
    "tgt2col=dict(zip(tgts,cols_tgt))\n",
    "tgt2col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 6.58 s, total: 1min 7s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dftrn=pd.read_csv(f'{p_in}/trn_{trntmstmp}.tsv',sep='\\x01',header=None,encoding='utf-8',names=cols, \n",
    "    nrows=1e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lens = dftrn.toks.apply(lambda x: len(x.split('\\t')))\n",
    "# lens.mean(), np.percentile(lens, 50), np.percentile(lens, 95), np.percentile(lens, 99)\n",
    "# (46.754583, 41.0, 106.0, 135.0)\n",
    "\n",
    "\n",
    "# tokenizer.pad_token, tokenizer.pad_token_id\n",
    "# ('[PAD]', 0)\n",
    "\n",
    "# tokenizer.sep_token, tokenizer.sep_token_id\n",
    "# ('[SEP]', 102)\n",
    "maxlen=128\n",
    "def mkids(x):\n",
    "    tokids=list(map(int, x.split('\\t')))\n",
    "    l=len(tokids) \n",
    "    if l<=maxlen: \n",
    "        return tokids + [0]*(maxlen-len(tokids))\n",
    "    else: \n",
    "        return tokids[:maxlen-1]+[102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_tensors(df, istrn=True):\n",
    "    tokids=dftrn.toks.apply(lambda x: mkids(x))\n",
    "    Xarr=np.array(list(tokids))\n",
    "    X=torch.tensor(Xarr,dtype=torch.long)\n",
    "    if not istrn: return X\n",
    "    ys=dftrn[cols_tgt_tmstmp].notna().values\n",
    "    ys=torch.tensor(ys,dtype=torch.float)\n",
    "    return X,ys\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X,ys = mk_tensors(dftrn)\n",
    "\n",
    "# BS=2\n",
    "# ds = TensorDataset(X,ys)\n",
    "# dl = DataLoader(ds, batch_size=BS, shuffle=True)\n",
    "# for step, batch in enumerate(dl):\n",
    "#     X_b,ys_b = (o.to(device) for o in batch)\n",
    "#     print(X_b.shape,ys_b.shape)\n",
    "#     break\n",
    "# torch.Size([2, 128]) torch.Size([2, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = TensorDataset(X,ys)\n",
    "# dl = DataLoader(ds, batch_size=BS, shuffle=True)\n",
    "# for step, batch in enumerate(dl):\n",
    "#     X_b,ys_b = (o.to(device) for o in batch)\n",
    "#     print(X_b.shape,ys_b.shape)\n",
    "#     break\n",
    "\n",
    "# # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "# bertmodel=bertmodel.eval()\n",
    "# bertmodel.to(device)\n",
    "# with torch.no_grad():\n",
    "#     last_hidden_state, pooler_output, hidden_states = bertmodel(X_b,)\n",
    "#     avg_pool = torch.mean(last_hidden_state,1)\n",
    "#     max_pool,_ = torch.max(last_hidden_state,1)\n",
    "# last_hidden_state.shape,pooler_output.shape, len(hidden_states), avg_pool.shape, max_pool.shape\n",
    "# (torch.Size([2, 128, 768]),\n",
    "#  torch.Size([2, 768]),\n",
    "#  13,\n",
    "#  torch.Size([2, 768]),\n",
    "#  torch.Size([2, 768]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see RobertaClassificationHead in transformers/modeling_roberta.py\n",
    "N_HIDDEN = 768 \n",
    "class TwtModel(nn.Module):\n",
    "    def __init__(self, bertmodel, num_labels=4):\n",
    "        super(TwtModel, self).__init__()\n",
    "        self.bertmodel = bertmodel        \n",
    "        dense_size = 64\n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "        nx = N_HIDDEN*2\n",
    "        self.dense = nn.Linear(nx, nx)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out_proj = nn.Linear(nx, num_labels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_msk = (x!=tokenizer.pad_token_id).float().to(device)\n",
    "        segments = torch.zeros(x.shape, dtype=torch.long).to(device)\n",
    "        bert_output=self.bertmodel(x,attention_mask=attn_msk,token_type_ids=segments)\n",
    "        last_hidden_state,pooler_output,hidden_states = bert_output\n",
    "        avg_pool = torch.mean(last_hidden_state,1)\n",
    "        max_pool,_ = torch.max(last_hidden_state,1)\n",
    "        x = torch.cat([avg_pool,max_pool],1) \n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS=104\n",
    "EPOCHS=4\n",
    "WD=0.01\n",
    "LR=1e-5\n",
    "SCHDLR_FUNC = get_cosine_schedule_with_warmup \n",
    "WARMUP_RATE = 0.05\n",
    "N_CYCLS = .5\n",
    "EPS = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,ys = mk_tensors(dftrn)\n",
    "del dftrn; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "msk_vl = np.random.rand(len(X))<0.15\n",
    "tr = np.where(~msk_vl)[0]\n",
    "vl = np.where( msk_vl)[0]\n",
    "Xtr,Xvl = X[tr],X[vl]\n",
    "ys_tr, ys_vl = ys[tr], ys[vl]\n",
    "ds_tr = TensorDataset(Xtr,ys_tr)\n",
    "ds_vl = TensorDataset(Xvl,ys_vl)\n",
    "dl_tr = DataLoader(ds_tr, batch_size=BS,   pin_memory=True, shuffle=True)\n",
    "dl_vl = DataLoader(ds_vl, batch_size=BS, pin_memory=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwtModel(bertmodel)\n",
    "model = model.to(device)\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': WD},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LR, eps=EPS)\n",
    "t_total = int(EPOCHS*len(dl_tr))\n",
    "scheduler = SCHDLR_FUNC(\n",
    "    optimizer, \n",
    "    num_warmup_steps=int(WARMUP_RATE*t_total), \n",
    "    num_training_steps=t_total,\n",
    "    num_cycles=N_CYCLS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dl):\n",
    "    prd = []\n",
    "    ys = []\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    for step, dat_b in enumerate(dl):\n",
    "        model.eval()\n",
    "        x_b,ys_b = (o.to(device) for o in dat_b)\n",
    "        with torch.no_grad(): \n",
    "            prd_b = model(x_b)\n",
    "            loss_ =  F.binary_cross_entropy_with_logits(prd_b,ys_b)\n",
    "\n",
    "        ys.append(ys_b.cpu().detach().numpy())\n",
    "        prd.append(prd_b.cpu().detach().numpy())\n",
    "        losses.append(loss_.item())\n",
    "\n",
    "    prd = np.concatenate(prd)\n",
    "    ys = np.concatenate(ys)\n",
    "    scrs = [roc_auc_score(ys[:,i], prd[:,i]) for i in range(4)]\n",
    "    scr  = roc_auc_score(ys, prd)\n",
    "    loss = np.mean(losses)\n",
    "    return prd, ys, scrs, scr, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-27 19:50:54 epoch 0 starts\n",
      "2020-03-27 19:50:56 step 0/81744 loss 0.6870\n",
      "2020-03-27 19:52:02 step 100/81744 loss 0.6744\n",
      "2020-03-27 19:53:10 step 200/81744 loss 0.6275\n",
      "2020-03-27 19:54:17 step 300/81744 loss 0.5407\n",
      "2020-03-27 19:55:25 step 400/81744 loss 0.4465\n",
      "2020-03-27 19:56:33 step 500/81744 loss 0.3824\n",
      "2020-03-27 19:57:41 step 600/81744 loss 0.3143\n",
      "2020-03-27 19:58:48 step 700/81744 loss 0.3520\n",
      "2020-03-27 19:59:56 step 800/81744 loss 0.3081\n",
      "2020-03-27 20:01:03 step 900/81744 loss 0.3318\n"
     ]
    }
   ],
   "source": [
    "model.zero_grad()\n",
    "set_seed(SEED)\n",
    "\n",
    "epc2loss_tr = []\n",
    "epc2loss_vl = []\n",
    "epc2scrs_tr = []\n",
    "epc2scrs_vl = []\n",
    "best_scr = float('-inf')\n",
    "save_p = f'{p_out}/mdl_{PRFX}.p'\n",
    "lendl = len(dl_tr)\n",
    "for epoch in range(EPOCHS):\n",
    "    print(dtnow(), f'epoch {epoch} starts')\n",
    "    ys_tr_ep = []\n",
    "    prd_tr_ep = []\n",
    "    for step, dat_b in enumerate(dl_tr):\n",
    "        model.train()\n",
    "        x_b,ys_b = (o.to(device) for o in dat_b)\n",
    "        prd_b = model(x_b)\n",
    "        loss =  F.binary_cross_entropy_with_logits(prd_b,ys_b)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "\n",
    "        ys_tr_ep.append(ys_b.cpu().detach().numpy())\n",
    "        prd_tr_ep.append(prd_b.cpu().detach().numpy())\n",
    "        if step%100==0: print(dtnow(),f'step {step}/{lendl} loss {loss.item():.4f}')\n",
    "\n",
    "    prd_tr_ep = np.concatenate(prd_tr_ep)\n",
    "    ys_tr_ep = np.concatenate(ys_tr_ep)\n",
    "    scrs_tr = [roc_auc_score(ys_tr_ep[:,i], prd_tr_ep[:,i]) for i in range(4)]\n",
    "    scr_tr  = roc_auc_score(ys_tr_ep, prd_tr_ep)\n",
    "    loss_tr = loss.item()\n",
    "    \n",
    "    prd_vl_ep,ys_vl_ep,scrs_vl,scr_vl,loss_vl = evaluate(model, dl_vl)\n",
    "    print(f'lss_tr: {loss_tr: .4f}; scr_tr: {scr_tr: .4f}; {scrs_tr}')\n",
    "    print(f'lss_vl: {loss_vl: .4f}; scr_vl: {scr_vl: .4f}; {scrs_vl}')\n",
    "    if scr_vl>best_scr: \n",
    "        best_scr = scr_vl\n",
    "        best_prd_vl = prd_vl_ep\n",
    "        print(f'found better scr, saving model...')\n",
    "        torch.save(model.state_dict(),save_p)\n",
    "    \n",
    "    epc2loss_tr.append(loss_tr)\n",
    "    epc2loss_vl.append(loss_vl)\n",
    "    epc2scrs_tr.append(scrs_tr)\n",
    "    epc2scrs_vl.append(scrs_vl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(epc2loss_tr)\n",
    "print(epc2loss_vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(epc2scrs_tr))\n",
    "print(np.array(epc2scrs_vl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rcss20",
   "language": "python",
   "name": "rcss20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
